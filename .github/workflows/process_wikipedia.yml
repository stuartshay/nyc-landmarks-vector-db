name: Process Wikipedia Articles for NYC Landmarks

on:
  workflow_dispatch:
    inputs:
      landmark_ids:
        description: "Comma-separated list of landmark IDs to process (leave empty for all landmarks)"
        required: false
        default: ""
      limit:
        description: "Maximum number of landmarks to process (for testing)"
        required: false
        default: "0"
      chunk_size:
        description: "Text chunk size in characters"
        required: false
        default: "1000"
      chunk_overlap:
        description: "Overlap between chunks in characters"
        required: false
        default: "200"
      parallel:
        description: "Run processing in parallel"
        required: false
        default: true
        type: boolean
      workers:
        description: "Number of parallel workers (if parallel is true)"
        required: false
        default: "4"
      batch_size:
        description: "Batch size for each worker in the matrix (ignored if landmark_ids are specified)"
        required: false
        default: "100"
      index_name:
        description: "Pinecone index name (leave empty for default in settings)"
        required: false
        default: ""
      force_reprocess:
        description: "Force reprocessing of articles already in the index"
        required: false
        default: false
        type: boolean
      skip_verification:
        description: "Skip verification step after processing"
        required: false
        default: false
        type: boolean

jobs:
  build_image:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Calculate Docker image metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository_owner }}/nyc-landmarks-vector-db
          tags: |
            type=raw,value=latest
            type=sha,format=short

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ hashFiles('Dockerfile', 'requirements.txt', 'setup.py', 'pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-buildx-${{ hashFiles('Dockerfile') }}
            ${{ runner.os }}-buildx-

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: |
            type=local,src=/tmp/.buildx-cache
            type=registry,ref=ghcr.io/${{ github.repository_owner }}/nyc-landmarks-vector-db:latest
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max
          platforms: linux/amd64
          outputs: type=registry,name=ghcr.io/${{ github.repository_owner }}/nyc-landmarks-vector-db:latest
          build-args: |
            BUILDKIT_INLINE_CACHE=1
            BUILDKIT_CONTEXT_KEEP_GIT_DIR=1

      # This ugly bit is necessary if you don't want your cache to grow forever
      # See https://github.com/docker/build-push-action/issues/252
      # And https://github.com/moby/buildkit/issues/1896
      - name: Move cache
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

  setup:
    needs: build_image
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
      total_records: ${{ steps.get-record-count.outputs.total_records }}
      using_matrix: ${{ steps.create-matrix.outputs.using_matrix }}
    container:
      image: ghcr.io/${{ github.repository_owner }}/nyc-landmarks-vector-db:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure environment
        run: |
          {
            echo "PINECONE_API_KEY=${{ secrets.PINECONE_API_KEY }}"
            echo "PINECONE_ENVIRONMENT=${{ secrets.PINECONE_ENVIRONMENT }}"
            echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
            echo "COREDATASTORE_API_KEY=${{ secrets.COREDATASTORE_API_KEY }}"

            # Add custom index name if provided
            if [ -n "${{ github.event.inputs.index_name }}" ]; then
              echo "PINECONE_INDEX_NAME=${{ github.event.inputs.index_name }}"
            fi
          } >> "$GITHUB_ENV"

      - name: Install package
        run: |
          # Install the nyc_landmarks package in development mode
          pip install -e .
          # Verify installation
          python -c "import nyc_landmarks; print(f'nyc_landmarks package installed successfully: {nyc_landmarks.__file__}')"

      - name: Check API connections
        run: |
          if ! python scripts/ci/test_pinecone_connection.py; then
            echo "Failed to connect to Pinecone API"
            exit 1
          fi

      - name: Get record count and create matrix
        id: get-record-count
        if: ${{ !github.event.inputs.landmark_ids }}
        run: |
          # Get the total record count using the db_client
          TOTAL_RECORDS=$(python -c "from nyc_landmarks.db.db_client import get_db_client; print(get_db_client().get_total_record_count())")
          echo "total_records=$TOTAL_RECORDS" >> $GITHUB_OUTPUT
          echo "Total records: $TOTAL_RECORDS"

      - name: Create processing matrix
        id: create-matrix
        run: |
          if [ -n "${{ github.event.inputs.landmark_ids }}" ]; then
            # If specific landmarks are provided, no matrix is needed
            echo "matrix={\"batch\": [0]}" >> $GITHUB_OUTPUT
            echo "using_matrix=false" >> $GITHUB_OUTPUT
            echo "Using single job for specified landmark IDs"
          else
            # Calculate the number of batches
            BATCH_SIZE="${{ github.event.inputs.batch_size }}"
            TOTAL_RECORDS="${{ steps.get-record-count.outputs.total_records }}"

            # Use limit if provided, otherwise use total records
            if [ "${{ github.event.inputs.limit }}" != "0" ]; then
              TOTAL_RECORDS=$(( ${{ github.event.inputs.limit }} < $TOTAL_RECORDS ? ${{ github.event.inputs.limit }} : $TOTAL_RECORDS ))
            fi

            # Calculate number of batches (ceil division)
            NUM_BATCHES=$(( ($TOTAL_RECORDS + $BATCH_SIZE - 1) / $BATCH_SIZE ))

            # Create matrix with batch indices (0 to NUM_BATCHES-1)
            BATCHES="["
            i=0
            while [ $i -lt $NUM_BATCHES ]; do
              if [ $i -gt 0 ]; then
                BATCHES="$BATCHES,"
              fi
              BATCHES="$BATCHES$i"
              i=$((i + 1))
            done
            BATCHES="$BATCHES]"

            echo "matrix={\"batch\": $BATCHES}" >> $GITHUB_OUTPUT
            echo "using_matrix=true" >> $GITHUB_OUTPUT
            echo "Created matrix with $NUM_BATCHES batches (batch size: $BATCH_SIZE, total records: $TOTAL_RECORDS)"
          fi

  process-wikipedia:
    needs: setup
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      fail-fast: false
    container:
      image: ghcr.io/${{ github.repository_owner }}/nyc-landmarks-vector-db:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure environment
        run: |
          {
            echo "PINECONE_API_KEY=${{ secrets.PINECONE_API_KEY }}"
            echo "PINECONE_ENVIRONMENT=${{ secrets.PINECONE_ENVIRONMENT }}"
            echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
            echo "COREDATASTORE_API_KEY=${{ secrets.COREDATASTORE_API_KEY }}"

            # Add custom index name if provided
            if [ -n "${{ github.event.inputs.index_name }}" ]; then
              echo "PINECONE_INDEX_NAME=${{ github.event.inputs.index_name }}"
            fi
          } >> "$GITHUB_ENV"

      - name: Install package
        run: |
          # Install the nyc_landmarks package in development mode
          pip install -e .
          # Verify installation
          python -c "import nyc_landmarks; print(f'nyc_landmarks package installed successfully: {nyc_landmarks.__file__}')"

      - name: Process Wikipedia articles
        id: process_articles
        shell: bash
        run: |
          # Build command arguments - using POSIX-compatible command building
          ARGS=""

          if [ "${{ needs.setup.outputs.using_matrix }}" = "true" ]; then
            # Matrix mode - calculate batch range
            BATCH_INDEX=${{ matrix.batch }}
            BATCH_SIZE="${{ github.event.inputs.batch_size }}"
            # Use page numbers (1-indexed) not record indices
            PAGE_NUMBER=$((BATCH_INDEX + 1))

            # Use pagination without --all flag (they're mutually exclusive)
            ARGS="$ARGS --page $PAGE_NUMBER --page-size $BATCH_SIZE"
            echo "Processing batch $BATCH_INDEX: Page $PAGE_NUMBER with batch size $BATCH_SIZE"
          elif [ -n "${{ github.event.inputs.landmark_ids }}" ]; then
            # Specific landmark IDs mode
            ARGS="$ARGS --landmark-ids ${{ github.event.inputs.landmark_ids }}"
          else
            # Fallback to simple --all mode (should not happen with current logic)
            ARGS="$ARGS --all"
          fi

          # Add limit if specified (only for non-matrix mode)
          if [ "${{ needs.setup.outputs.using_matrix }}" != "true" ] && [ "${{ github.event.inputs.limit }}" != "0" ]; then
            ARGS="$ARGS --limit ${{ github.event.inputs.limit }}"
          fi

          # Add chunk configuration
          ARGS="$ARGS --chunk-size ${{ github.event.inputs.chunk_size }} --chunk-overlap ${{ github.event.inputs.chunk_overlap }}"

          # Add parallel processing if enabled
          if [ "${{ github.event.inputs.parallel }}" = "true" ]; then
            # Use the specified number of workers
            WORKERS="${{ github.event.inputs.workers }}"
            ARGS="$ARGS --parallel --workers $WORKERS"
          fi

          # Add force reprocess if enabled
          if [ "${{ github.event.inputs.force_reprocess }}" = "true" ]; then
            ARGS="$ARGS --delete-existing"
          fi

          # Always enable verbose logging
          ARGS="$ARGS --verbose"

          # Run the processing script
          echo "Running: python scripts/ci/process_wikipedia_articles.py $ARGS"
          python scripts/ci/process_wikipedia_articles.py $ARGS

          # Create batch-specific summary file
          BATCH_SUFFIX=""
          if [ "${{ needs.setup.outputs.using_matrix }}" = "true" ]; then
            BATCH_SUFFIX="-batch-${{ matrix.batch }}"
          fi

          # Create a summary file for artifacts
          {
            echo "# Wikipedia Processing Results$BATCH_SUFFIX"
            echo "## Configuration"
            if [ "${{ needs.setup.outputs.using_matrix }}" = "true" ]; then
              echo "- Batch: ${{ matrix.batch }}"
              echo "- Page: $PAGE_NUMBER"
              echo "- Batch Size: $BATCH_SIZE"
            else
              echo "- Landmark IDs: ${{ github.event.inputs.landmark_ids || 'All landmarks' }}"
            fi
            echo "- Chunk size: ${{ github.event.inputs.chunk_size }}"
            echo "- Chunk overlap: ${{ github.event.inputs.chunk_overlap }}"
            echo "- Parallel processing: ${{ github.event.inputs.parallel }}"
            if [ -n "${{ github.event.inputs.index_name }}" ]; then
              echo "- Pinecone index: ${{ github.event.inputs.index_name }}"
            fi

            echo "Processing completed successfully."
          } > "wiki_processing_summary${BATCH_SUFFIX}.md"

      - name: Upload processing results for batch
        uses: actions/upload-artifact@v4
        with:
          name: wikipedia-processing-results${{ needs.setup.outputs.using_matrix == 'true' && format('-batch-{0}', matrix.batch) || '' }}
          path: |
            wiki_processing_summary*.md
            logs/
          if-no-files-found: warn

  verify-wikipedia:
    needs: process-wikipedia
    if: ${{ github.event.inputs.skip_verification != 'true' }}
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/nyc-landmarks-vector-db:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure environment
        run: |
          {
            echo "PINECONE_API_KEY=${{ secrets.PINECONE_API_KEY }}"
            echo "PINECONE_ENVIRONMENT=${{ secrets.PINECONE_ENVIRONMENT }}"
            echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
            echo "COREDATASTORE_API_KEY=${{ secrets.COREDATASTORE_API_KEY }}"

            # Add custom index name if provided
            if [ -n "${{ github.event.inputs.index_name }}" ]; then
              echo "PINECONE_INDEX_NAME=${{ github.event.inputs.index_name }}"
            fi
          } >> "$GITHUB_ENV"

      - name: Install package
        run: |
          # Install the nyc_landmarks package in development mode
          pip install -e .
          # Verify installation
          python -c "import nyc_landmarks; print(f'nyc_landmarks package installed successfully: {nyc_landmarks.__file__}')"

      - name: Verify Wikipedia integration
        run: |
          echo "Verifying Wikipedia integration..."
          # Create verification directory
          mkdir -p verification_results

          # Run verification script
          python scripts/ci/verify_wikipedia_imports.py --verbose --coverage-report --output-dir verification_results

          # Create summary file
          {
            echo "# Wikipedia Verification Results"
            echo "## Verification Summary"
            echo "Verification performed after processing all batches."
            echo "See attached verification report for details."
          } > verification_summary.md

      - name: Upload verification results
        uses: actions/upload-artifact@v4
        with:
          name: wikipedia-verification-results
          path: |
            verification_summary.md
            verification_results/
          if-no-files-found: warn
