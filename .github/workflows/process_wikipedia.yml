name: Process Wikipedia Articles for NYC Landmarks

on:
  workflow_dispatch:
    inputs:
      landmark_ids:
        description: 'Comma-separated list of landmark IDs to process (leave empty for all landmarks)'
        required: false
        default: ''
      limit:
        description: 'Maximum number of landmarks to process (for testing)'
        required: false
        default: '0'
      chunk_size:
        description: 'Text chunk size in characters'
        required: false
        default: '1000'
      chunk_overlap:
        description: 'Overlap between chunks in characters'
        required: false
        default: '200'
      parallel:
        description: 'Run processing in parallel'
        required: false
        default: true
        type: boolean
      workers:
        description: 'Number of parallel workers (if parallel is true)'
        required: false
        default: '4'
      index_name:
        description: 'Pinecone index name (leave empty for default in settings)'
        required: false
        default: ''
      force_reprocess:
        description: 'Force reprocessing of articles already in the index'
        required: false
        default: false
        type: boolean
      skip_verification:
        description: 'Skip verification step after processing'
        required: false
        default: false
        type: boolean
      verbose:
        description: 'Enable verbose logging'
        required: false
        default: true
        type: boolean

jobs:
  process-wikipedia:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install -r requirements.txt

      - name: Configure environment
        run: |
          echo "PINECONE_API_KEY=${{ secrets.PINECONE_API_KEY }}" >> $GITHUB_ENV
          echo "PINECONE_ENVIRONMENT=${{ secrets.PINECONE_ENVIRONMENT }}" >> $GITHUB_ENV
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> $GITHUB_ENV
          echo "COREDATASTORE_API_KEY=${{ secrets.COREDATASTORE_API_KEY }}" >> $GITHUB_ENV

          # Add custom index name if provided
          if [[ -n "${{ github.event.inputs.index_name }}" ]]; then
            echo "PINECONE_INDEX_NAME=${{ github.event.inputs.index_name }}" >> $GITHUB_ENV
          fi

      - name: Check API connections
        run: |
          python scripts/test_pinecone_connection.py

      - name: Process Wikipedia articles
        id: process_articles
        run: |
          # Build command arguments
          ARGS=""

          # Add landmark IDs if specified
          if [[ -n "${{ github.event.inputs.landmark_ids }}" ]]; then
            ARGS="$ARGS --landmark-ids ${{ github.event.inputs.landmark_ids }}"
          fi

          # Add limit if specified
          if [[ "${{ github.event.inputs.limit }}" != "0" ]]; then
            ARGS="$ARGS --limit ${{ github.event.inputs.limit }}"
          fi

          # Add chunk configuration
          ARGS="$ARGS --chunk-size ${{ github.event.inputs.chunk_size }} --chunk-overlap ${{ github.event.inputs.chunk_overlap }}"

          # Add parallel processing if enabled
          if [[ "${{ github.event.inputs.parallel }}" == "true" ]]; then
            ARGS="$ARGS --parallel --workers ${{ github.event.inputs.workers }}"
          fi

          # Add force reprocess if enabled
          if [[ "${{ github.event.inputs.force_reprocess }}" == "true" ]]; then
            ARGS="$ARGS --force-reprocess"
          fi

          # Add verbose flag if enabled
          if [[ "${{ github.event.inputs.verbose }}" == "true" ]]; then
            ARGS="$ARGS --verbose"
          fi

          # Run the processing script
          echo "Running: python scripts/process_wikipedia_articles.py $ARGS"
          python scripts/process_wikipedia_articles.py $ARGS

          # Create a summary file for artifacts
          echo "# Wikipedia Processing Results" > wiki_processing_summary.md
          echo "## Configuration" >> wiki_processing_summary.md
          echo "- Landmark IDs: ${{ github.event.inputs.landmark_ids || 'All landmarks' }}" >> wiki_processing_summary.md
          echo "- Chunk size: ${{ github.event.inputs.chunk_size }}" >> wiki_processing_summary.md
          echo "- Chunk overlap: ${{ github.event.inputs.chunk_overlap }}" >> wiki_processing_summary.md
          echo "- Parallel processing: ${{ github.event.inputs.parallel }}" >> wiki_processing_summary.md
          echo "- Workers: ${{ github.event.inputs.workers }}" >> wiki_processing_summary.md
          echo "- Force reprocess: ${{ github.event.inputs.force_reprocess }}" >> wiki_processing_summary.md
          if [[ -n "${{ github.event.inputs.index_name }}" ]]; then
            echo "- Pinecone index: ${{ github.event.inputs.index_name }}" >> wiki_processing_summary.md
          fi

          echo "Processing completed successfully." >> wiki_processing_summary.md

      - name: Verify Wikipedia integration
        if: ${{ github.event.inputs.skip_verification != 'true' }}
        run: |
          echo "Verifying Wikipedia integration..."
          # Create verification directory
          mkdir -p verification_results

          # Run verification script
          python scripts/verify_wikipedia_imports.py --verbose --coverage-report --output-dir verification_results

          # Add verification results to summary
          echo "## Verification Results" >> wiki_processing_summary.md
          echo "See attached verification report for details." >> wiki_processing_summary.md

      - name: Upload processing results
        uses: actions/upload-artifact@v3
        with:
          name: wikipedia-processing-results
          path: |
            wiki_processing_summary.md
            verification_results/
            logs/
          if-no-files-found: warn
