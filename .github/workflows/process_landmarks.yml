name: Process NYC Landmarks Data

on:
  workflow_dispatch: # Allows manual triggering
    inputs:
      api_page_size:
        description: 'Records per API page fetch'
        required: true
        default: '100'
      job_batch_size:
        description: 'Number of API pages per parallel job'
        required: true
        default: '5'
      parallel_workers:
        description: 'Number of parallel workers per job (0 for sequential)'
        required: true
        default: '2'
      recreate_index:
        description: 'Recreate Pinecone index before processing? (true/false)'
        required: false
        default: 'false'
      total_records:
        description: 'Total number of landmark records to process (used for batching)'
        required: true
        default: '1765' # Update if the total changes significantly

jobs:
  generate_batches:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Generate processing matrix
        id: set_matrix
        run: |
          echo "Generating matrix..."
          MATRIX_JSON=$(python scripts/generate_matrix.py \
            --total-records ${{ github.event.inputs.total_records }} \
            --api-page-size ${{ github.event.inputs.api_page_size }} \
            --job-batch-size ${{ github.event.inputs.job_batch_size }})
          echo "Matrix JSON: $MATRIX_JSON"
          # Check if JSON is valid (basic check)
          if ! echo "$MATRIX_JSON" | python -c "import json, sys; json.load(sys.stdin)"; then
            echo "::error::Generated matrix JSON is invalid: $MATRIX_JSON"
            exit 1
          fi
          echo "matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT

  process:
    needs: generate_batches
    runs-on: ubuntu-latest
    timeout-minutes: 120 # Increased timeout for potentially longer batch jobs
    strategy:
      fail-fast: false # Allow other jobs to continue if one fails
      matrix: ${{ fromJson(needs.generate_batches.outputs.matrix) }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # Match project version

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential libpq-dev python3-dev poppler-utils libssl-dev tesseract-ocr libtesseract-dev
          python -m pip install --upgrade pip setuptools wheel

          # Debug information
          echo "Python version:"
          python --version
          echo "Pip version:"
          pip --version

          # Install packages in smaller groups with verbose output
          echo "Installing core dependencies..."
          pip install --no-cache-dir -v numpy tqdm requests

          echo "Installing PDF processing libraries..."
          pip install --no-cache-dir -v pdfminer.six pdfplumber PyPDF2 pypdfium2

          echo "Installing database and vector libraries..."
          pip install --no-cache-dir -v psycopg2-binary SQLAlchemy pinecone-client

          echo "Installing AI libraries..."
          pip install --no-cache-dir -v openai tiktoken

          echo "Installing remaining requirements..."
          pip install --no-cache-dir -r requirements.txt || {
            echo "::error::Requirements installation failed. Detailed error above."
            exit 1
          }

      - name: Run Landmark Processing Script for Batch (Pages ${{ matrix.start_page }}-${{ matrix.end_page }})
        env:
          PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
          COREDATASTORE_API_KEY: ${{ secrets.COREDATASTORE_API_KEY }} # Provided as env var
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          PINECONE_ENVIRONMENT: ${{ secrets.PINECONE_ENVIRONMENT }}
        run: |
          echo "Starting landmark processing for pages ${{ matrix.start_page }} to ${{ matrix.end_page }}..."
          ARGS="--start-page ${{ matrix.start_page }} --end-page ${{ matrix.end_page }} --page-size ${{ github.event.inputs.api_page_size }}"

          # Determine if parallel processing should be used within this job
          WORKERS=${{ github.event.inputs.parallel_workers }}
          if [[ "$WORKERS" -gt 0 ]]; then
            ARGS="$ARGS --parallel --workers $WORKERS"
            echo "Using parallel mode with $WORKERS workers for this batch."
          else
            echo "Using sequential mode for this batch."
          fi

          # Handle index recreation - Note: This will run for the FIRST job only if needed.
          # Consider if index recreation should be a separate, initial job.
          # For simplicity now, it runs within the first triggered job if set.
          # A more robust approach might be a dedicated setup job.
          if [[ "${{ github.event.inputs.recreate_index }}" == "true" ]]; then
             # Only add --recreate-index for the very first batch (start_page == 1)
             if [[ "${{ matrix.start_page }}" == "1" ]]; then
               echo "Adding --recreate-index for the first batch."
               ARGS="$ARGS --recreate-index"
             else
               echo "Skipping --recreate-index for subsequent batches."
             fi
          fi

          # The script uses COREDATASTORE_API_KEY from env var if --api-key is not provided.
          python scripts/process_landmarks.py $ARGS
          echo "Landmark processing finished for pages ${{ matrix.start_page }}-${{ matrix.end_page }}."
