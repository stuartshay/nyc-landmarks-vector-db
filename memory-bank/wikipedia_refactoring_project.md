# Wikipedia Processing Script Refactoring & Metadata Enhancement Project

## Project Overview

**Objective**: Refactor `scripts/ci/process_wikipedia_articles.py` (757 lines) to improve maintainability and conduct comprehensive analysis of Wikipedia articles and underutilized CoreDataStore APIs for metadata enhancement opportunities.

**Date Started**: 2025-05-31

## Phase 1: Script Refactoring (File Size Reduction)

### Current State Analysis

- **Current File Size**: 757 lines
- **Target File Size**: ~200 lines (60%+ reduction)
- **Main Issue**: Multiple responsibilities in single file

### Refactoring Plan

#### 1. Create `nyc_landmarks/wikipedia/processor.py` (~150 lines reduction)

**Components to Extract:**

- `_fetch_wikipedia_articles()` - Wikipedia content retrieval
- `_process_articles_into_chunks()` - Article chunking logic
- `_generate_embeddings_and_store()` - Embedding generation and storage
- `process_landmark_wikipedia()` - Main processing workflow
- `_split_into_token_chunks()` - Token-based text splitting
- `_enrich_chunks_with_article_metadata()` - Metadata enhancement

**New Class**: `WikipediaProcessor`

- Clean separation of concerns
- Reusable across different scripts
- Better testability

#### 2. Create `nyc_landmarks/wikipedia/utils.py` (~80 lines reduction)

**Components to Extract:**

- `get_all_landmark_ids()` - Landmark ID fetching
- `get_landmarks_to_process()` - Landmark selection logic
- `_add_metadata_to_chunks()` - Metadata helper
- Token chunking utilities

#### 3. Create `nyc_landmarks/utils/results_reporter.py` (~100 lines reduction)

**Components to Extract:**

- `calculate_statistics()` - Statistics calculation
- `print_results()` - Result reporting
- `_print_statistics()` - Statistics formatting
- `_print_list_summary()` - List summarization
- `_determine_exit_status()` - Exit code determination

#### 4. Streamlined Main Script (~200 lines)

**Remaining Responsibilities:**

- Argument parsing
- Component initialization
- Process coordination
- High-level workflow management

### Benefits of Refactoring

- **Maintainability**: Easier to modify and extend
- **Reusability**: Components can be used in other scripts
- **Testability**: Smaller, focused modules are easier to test
- **Readability**: Clear separation of concerns
- **Future Enhancement**: Foundation for metadata enhancement features

## Phase 2: Underutilized API Analysis

### CoreDataStore APIs Currently Underutilized

#### 1. Photo Archive API

**Endpoint**: `get_landmark_photos(landmark_id, limit=10)`
**Returns**: Photo information with titles, descriptions, collections, date periods
**Metadata Enhancement Potential**:

- Historical imagery context
- Photo titles for additional search terms
- Date periods for temporal context
- Collection classifications

#### 2. PLUTO Data API

**Endpoint**: `get_landmark_pluto_data(landmark_id)`
**Returns**: Building data (year built, land use, historic district, zoning)
**Metadata Enhancement Potential**:

- Building construction year
- Land use classifications
- Historic district associations
- Zoning district information

#### 3. Building Details API

**Endpoint**: `get_landmark_buildings(landmark_id, limit=50)`
**Returns**: Detailed building information (BBL, BIN, addresses, coordinates)
**Metadata Enhancement Potential**:

- Precise geographic coordinates
- Building identification numbers
- Address standardization
- Lot and block information

#### 4. Reference Data APIs

**Endpoints**:

- `get_neighborhoods(borough=None)`
- `get_object_types()`
- `get_boroughs()`
  **Metadata Enhancement Potential**:
- Standardized neighborhood classifications
- Object type hierarchies
- Borough-specific context

### API Integration Strategy

1. **Non-Intrusive**: Enhance existing metadata without breaking current functionality
1. **Performance Conscious**: Batch API calls and implement caching
1. **Optional**: Allow processing with or without enhanced metadata
1. **Backward Compatible**: Maintain existing metadata structure

## Phase 3: Wikipedia Processing & Analysis

### Processing Execution

**Command**: `python scripts/ci/process_wikipedia_articles.py --page 1 --limit 25 --verbose`

### Analysis Components

#### 1. Wikipedia Article Analysis

- Article content quality assessment
- Chunk distribution analysis
- Processing performance metrics
- Error categorization

#### 2. API Enhancement Testing

For each processed landmark:

- Test photo archive API availability
- Test PLUTO data completeness
- Test building details accuracy
- Measure API response times

#### 3. Metadata Comparison

- **Baseline**: Current Wikipedia metadata
- **Enhanced**: Metadata with API integration
- **Performance**: Processing time impact
- **Quality**: Information enrichment value

### Deliverables

#### 1. Analysis Dump File (logs directory)

**Filename**: `wikipedia_analysis_YYYYMMDD_HHMMSS.json`
**Contents**:

- Processed landmark IDs and article information
- API enhancement test results
- Performance metrics
- Metadata comparison examples
- Implementation recommendations

#### 2. Implementation Recommendations

- Priority ranking of API enhancements
- Performance impact assessment
- Implementation complexity analysis
- Resource requirements

## Phase 4: Implementation Roadmap

### Immediate (Phase 1)

1. âœ… Document project plan in Memory Bank
1. ðŸ”„ Refactor Wikipedia processor into modular components
1. ðŸ”„ Create utilities and results reporting modules
1. ðŸ”„ Streamline main script

### Short-term (Phase 2)

5. Execute Wikipedia processing command
1. Test underutilized API endpoints
1. Analyze metadata enhancement opportunities
1. Generate comprehensive analysis dump file

### Medium-term (Phase 3)

9. Implement highest-value API integrations
1. Create enhanced metadata schema
1. Update processing pipeline
1. Performance optimization

### Long-term (Phase 4)

13. Advanced metadata analytics
01. Quality metrics and monitoring
01. Automated enhancement workflows
01. Documentation and training materials

## Success Metrics

### Refactoring Success

- Main script reduced from 757 to ~200 lines
- All functionality preserved
- Improved code organization and testability
- No breaking changes to existing interfaces

### Analysis Success

- Comprehensive analysis of 25 landmark Wikipedia articles
- Complete testing of underutilized APIs
- Detailed performance impact assessment
- Clear implementation recommendations

### Metadata Enhancement Success

- Identified valuable metadata enhancement opportunities
- Quantified processing performance impact
- Established implementation priority framework
- Created foundation for future enhancements

## Technical Considerations

### Architecture Principles

- **Modularity**: Clear separation of concerns
- **Reusability**: Components usable across multiple scripts
- **Maintainability**: Easy to modify and extend
- **Performance**: Efficient processing and API usage
- **Compatibility**: Backward compatible with existing systems

### Quality Standards

- **Type Safety**: Comprehensive type hints
- **Error Handling**: Robust error management
- **Documentation**: Comprehensive docstrings and examples
- **Testing**: Unit and integration test coverage
- **Logging**: Detailed logging for debugging and monitoring

## Project Dependencies

### Internal Dependencies

- `nyc_landmarks.db.db_client` - Database access
- `nyc_landmarks.embeddings.generator` - Embedding generation
- `nyc_landmarks.vectordb.pinecone_db` - Vector storage
- `nyc_landmarks.models.*` - Data models

### External Dependencies

- No new external dependencies required
- Leverages existing CoreDataStore API infrastructure
- Uses established Wikipedia processing pipeline

## Risk Mitigation

### Refactoring Risks

- **Breaking Changes**: Maintain identical interfaces
- **Performance Regression**: Monitor processing performance
- **Test Coverage**: Ensure comprehensive testing

### API Integration Risks

- **Rate Limiting**: Implement appropriate delays and batching
- **Data Quality**: Validate API responses before integration
- **Performance Impact**: Measure and optimize API call patterns

### Timeline Risks

- **Scope Creep**: Focus on defined deliverables
- **Technical Complexity**: Break work into manageable phases
- **Resource Availability**: Plan for iterative development
