{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279b235d",
   "metadata": {},
   "source": [
    "# NYC Landmarks Vector Database Statistics\n",
    "\n",
    "This notebook provides a comprehensive analysis of the NYC Landmarks data stored in the Pinecone vector database. It examines the vectors, metadata distribution, and overall statistics of the embeddings to give insights about the landmarks collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11db6d",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll import the necessary libraries and set up connections to the Pinecone database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33c4a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Data analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# For map visualizations\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Vector analysis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "983dac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "from nyc_landmarks.db.db_client import DbClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299ee5d",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "\n",
    "Connect to the Pinecone database and verify the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4444d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nyc_landmarks.db.db_client:Using CoreDataStore API client\n",
      "INFO:nyc_landmarks.db.coredatastore_api:Initialized CoreDataStore API client\n",
      "INFO:nyc_landmarks.vectordb.pinecone_db:Initialized Pinecone client\n",
      "INFO:nyc_landmarks.db.coredatastore_api:Initialized CoreDataStore API client\n",
      "INFO:nyc_landmarks.vectordb.pinecone_db:Initialized Pinecone client\n",
      "INFO:nyc_landmarks.vectordb.pinecone_db:Connected to Pinecone index: nyc-landmarks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Pinecone index: nyc-landmarks\n",
      "Namespace: landmarks\n",
      "Dimensions: 1536\n",
      "Metric: cosine\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Pinecone database client\n",
    "pinecone_db = PineconeDB()\n",
    "\n",
    "# Check if the connection was successful\n",
    "if pinecone_db.index:\n",
    "    print(f\"‚úÖ Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "    print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "    print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "    print(f\"Metric: {pinecone_db.metric}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to Pinecone. Check your credentials and network connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5f6bb",
   "metadata": {},
   "source": [
    "## Index Statistics\n",
    "\n",
    "Retrieve basic statistics about the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73d0cbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking Pinecone index status:\n",
      "Index object: Available\n",
      "Index name: nyc-landmarks\n",
      "API key set: Yes\n",
      "\n",
      "üìä Index Statistics:\n",
      "Dimension: 1536\n",
      "Index Fullness: 0.0\n",
      "Namespace object type: <class 'dict'>\n",
      "Namespace landmarks data type: <class 'pinecone.core.openapi.db_data.model.namespace_summary.NamespaceSummary'>\n",
      "\n",
      "üî¢ Total Vector Count: 516\n",
      "\n",
      "üìÅ Namespace Statistics:\n",
      "  - landmarks: {'vector_count': 516}\n"
     ]
    }
   ],
   "source": [
    "# Debug Pinecone connection and work with the index directly\n",
    "print(f\"\\nüîç Checking Pinecone index status:\")\n",
    "print(f\"Index object: {'Available' if pinecone_db.index else 'Not initialized'}\")\n",
    "print(f\"Index name: {pinecone_db.index_name}\")\n",
    "print(f\"API key set: {'Yes' if pinecone_db.api_key else 'No'}\")\n",
    "\n",
    "# Try direct approach with the index object instead of get_index_stats\n",
    "try:\n",
    "    if not pinecone_db.index:\n",
    "        print(\"‚ùå Pinecone index not initialized - attempting to reconnect...\")\n",
    "        # Try to re-initialize the connection (with error handling)\n",
    "        try:\n",
    "            from pinecone import Pinecone  # Use updated Pinecone SDK\n",
    "            # Initialize Pinecone again\n",
    "            pc = Pinecone(api_key=pinecone_db.api_key)\n",
    "            # Connect to existing index\n",
    "            pinecone_db.index = pc.Index(pinecone_db.index_name)\n",
    "            print(f\"‚úÖ Successfully reconnected to index: {pinecone_db.index_name}\")\n",
    "        except Exception as reconnect_error:\n",
    "            print(f\"‚ùå Reconnection failed: {reconnect_error}\")\n",
    "    \n",
    "    # Use direct index call with manual error handling\n",
    "    if pinecone_db.index:\n",
    "        try:\n",
    "            # Try to get stats directly from the index object\n",
    "            stats = pinecone_db.index.describe_index_stats()\n",
    "            \n",
    "            # If we get here, the call was successful\n",
    "            print(\"\\nüìä Index Statistics:\")\n",
    "            \n",
    "            # Extract index information - handling the new structure\n",
    "            dimension = None\n",
    "            index_fullness = None\n",
    "            \n",
    "            if hasattr(stats, \"dimension\"):\n",
    "                dimension = stats.dimension\n",
    "            \n",
    "            if hasattr(stats, \"index_fullness\"):\n",
    "                index_fullness = stats.index_fullness\n",
    "                \n",
    "            print(f\"Dimension: {dimension}\")\n",
    "            print(f\"Index Fullness: {index_fullness}\")\n",
    "            \n",
    "            # Extract namespace information safely - updated to handle new SDK structure\n",
    "            namespaces = {}\n",
    "            total_vector_count = 0\n",
    "            \n",
    "            # Debug information\n",
    "            if hasattr(stats, \"namespaces\"):\n",
    "                # Access the namespaces attribute safely\n",
    "                namespaces_obj = stats.namespaces\n",
    "                print(f\"Namespace object type: {type(namespaces_obj)}\")\n",
    "                \n",
    "                # Handle various namespace data structures\n",
    "                try:\n",
    "                    if hasattr(namespaces_obj, \"items\"):\n",
    "                        # If namespaces is dict-like\n",
    "                        for ns_name, ns_data in namespaces_obj.items():\n",
    "                            # Debug the namespace data structure\n",
    "                            print(f\"Namespace {ns_name} data type: {type(ns_data)}\")\n",
    "                            \n",
    "                            # Create a safe dictionary for the namespace\n",
    "                            namespace_dict = {}\n",
    "                            \n",
    "                            # For objects with vector_count attribute\n",
    "                            if hasattr(ns_data, \"vector_count\"):\n",
    "                                vector_count = ns_data.vector_count\n",
    "                                namespace_dict[\"vector_count\"] = vector_count\n",
    "                                total_vector_count += vector_count\n",
    "                            \n",
    "                            # For dictionary-like objects\n",
    "                            elif isinstance(ns_data, dict) and \"vector_count\" in ns_data:\n",
    "                                vector_count = ns_data[\"vector_count\"]\n",
    "                                namespace_dict[\"vector_count\"] = vector_count\n",
    "                                total_vector_count += vector_count\n",
    "                                \n",
    "                            # For other types, just get the direct value if possible\n",
    "                            elif ns_data is not None:\n",
    "                                namespace_dict[\"vector_count\"] = 0  # Default\n",
    "                                try:\n",
    "                                    # Try to get a value directly or convert to int if possible\n",
    "                                    if hasattr(ns_data, \"__int__\"):\n",
    "                                        vector_count = int(ns_data)\n",
    "                                        namespace_dict[\"vector_count\"] = vector_count\n",
    "                                        total_vector_count += vector_count\n",
    "                                except (TypeError, ValueError):\n",
    "                                    pass\n",
    "                                    \n",
    "                            namespaces[ns_name] = namespace_dict\n",
    "                    else:\n",
    "                        # If it's some other structure, try to iterate\n",
    "                        for ns_name in namespaces_obj:\n",
    "                            namespaces[ns_name] = {\"vector_count\": 0}\n",
    "                except Exception as ns_error:\n",
    "                    print(f\"Error processing namespace data: {ns_error}\")\n",
    "                    namespaces[\"error\"] = {\"vector_count\": 0}\n",
    "            \n",
    "            # Use total_vector_count from stats if available and override calculated value\n",
    "            if hasattr(stats, \"total_vector_count\"):\n",
    "                total_vector_count = stats.total_vector_count\n",
    "                \n",
    "            print(f\"\\nüî¢ Total Vector Count: {total_vector_count:,}\")\n",
    "            print(\"\\nüìÅ Namespace Statistics:\")\n",
    "            for ns_name, ns_data in namespaces.items():\n",
    "                print(f\"  - {ns_name}: {ns_data}\")\n",
    "            \n",
    "            # Store in a dictionary for later use\n",
    "            index_stats = {\n",
    "                \"dimension\": dimension,\n",
    "                \"index_fullness\": index_fullness,\n",
    "                \"namespaces\": namespaces, \n",
    "                \"total_vector_count\": total_vector_count\n",
    "            }\n",
    "                \n",
    "        except Exception as stats_error:\n",
    "            print(f\"‚ùå Error getting index stats: {stats_error}\")\n",
    "            print(\"This might be due to an API version mismatch.\")\n",
    "            # Create some mock data for demonstration\n",
    "            print(\"\\nüìä Creating mock data for demonstration purposes:\")\n",
    "            namespaces = {\"default\": {\"vector_count\": 1000}, \"landmarks\": {\"vector_count\": 500}}\n",
    "            total_vector_count = 1500\n",
    "            index_stats = {\"namespaces\": namespaces, \"dimension\": 1536, \"index_fullness\": 0.01}\n",
    "            print(\"‚úÖ Mock data created for demonstration\")\n",
    "            \n",
    "            # Print mock stats\n",
    "            print(\"\\nüìä Mock Index Statistics:\")\n",
    "            print(f\"Dimension: {index_stats.get('dimension')}\")\n",
    "            print(f\"Index Fullness: {index_stats.get('index_fullness')}\")\n",
    "            print(f\"\\nüî¢ Mock Total Vector Count: {total_vector_count:,}\")\n",
    "            print(\"\\nüìÅ Mock Namespace Statistics:\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to initialize Pinecone index.\")\n",
    "        # Create mock data for demonstration\n",
    "        print(\"\\nüìä Creating mock data for demonstration purposes:\")\n",
    "        namespaces = {\"default\": {\"vector_count\": 1000}, \"landmarks\": {\"vector_count\": 500}}\n",
    "        total_vector_count = 1500\n",
    "        index_stats = {\"namespaces\": namespaces, \"dimension\": 1536, \"index_fullness\": 0.01}\n",
    "        print(\"‚úÖ Mock data created for demonstration\")\n",
    "        \n",
    "        # Print mock stats\n",
    "        print(\"\\nüìä Mock Index Statistics:\")\n",
    "        print(f\"Dimension: {index_stats.get('dimension')}\")\n",
    "        print(f\"Index Fullness: {index_stats.get('index_fullness')}\")\n",
    "        print(f\"\\nüî¢ Mock Total Vector Count: {total_vector_count:,}\")\n",
    "        print(\"\\nüìÅ Mock Namespace Statistics:\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error working with Pinecone: {e}\")\n",
    "    print(\"Falling back to mock data for demonstration purposes.\")\n",
    "    \n",
    "    # Create mock data for demonstration\n",
    "    namespaces = {\"default\": {\"vector_count\": 1000}, \"landmarks\": {\"vector_count\": 500}}\n",
    "    total_vector_count = 1500\n",
    "    index_stats = {\"namespaces\": namespaces, \"dimension\": 1536, \"index_fullness\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e52baea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m     percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(vector_count) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(total_vector_count) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_vector_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m     namespace_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m: ns_name \u001b[38;5;28;01mif\u001b[39;00m ns_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVector Count\u001b[39m\u001b[38;5;124m'\u001b[39m: vector_count,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPercentage\u001b[39m\u001b[38;5;124m'\u001b[39m: percentage\n\u001b[1;32m     11\u001b[0m     })\n\u001b[0;32m---> 13\u001b[0m namespace_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(namespace_data)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m namespace_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     15\u001b[0m     namespace_df \u001b[38;5;241m=\u001b[39m namespace_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVector Count\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    854\u001b[0m         data,\n\u001b[1;32m    855\u001b[0m         columns,\n\u001b[1;32m    856\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    857\u001b[0m         dtype,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/internals/construction.py:837\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m--> 837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m    839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/internals/construction.py:918\u001b[0m, in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    916\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    917\u001b[0m     pre_cols \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_unique_multiple_list_gen(gen, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m--> 918\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(pre_cols)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# classes\u001b[39;00m\n\u001b[1;32m    922\u001b[0m data \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(d) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/indexes/base.py:7647\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7645\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MultiIndex\u001b[38;5;241m.\u001b[39mfrom_arrays(index_like)\n\u001b[1;32m   7646\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/indexes/base.py:565\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    562\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     arr \u001b[38;5;241m=\u001b[39m sanitize_array(data, \u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/construction.py:654\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_convert_platform(data)\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    656\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:138\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[1;32m    137\u001b[0m     arr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, arr)\n\u001b[0;32m--> 138\u001b[0m     arr \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(arr)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32mlib.pyx:2538\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for namespace stats\n",
    "namespace_data = []\n",
    "\n",
    "for ns_name, ns_stats in namespaces.items():\n",
    "    vector_count = ns_stats.get('vector_count', 0)\n",
    "    percentage = float(vector_count) / float(total_vector_count) * 100 if total_vector_count > 0 else 0\n",
    "    namespace_data.append({\n",
    "        'Namespace': ns_name if ns_name else 'default',\n",
    "        'Vector Count': vector_count,\n",
    "        'Percentage': percentage\n",
    "    })\n",
    "\n",
    "namespace_df = pd.DataFrame(namespace_data)\n",
    "if not namespace_df.empty:\n",
    "    namespace_df = namespace_df.sort_values('Vector Count', ascending=False).reset_index(drop=True)\n",
    "    display(namespace_df)\n",
    "else:\n",
    "    print(\"No namespace data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d802d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize namespace distribution\n",
    "if not namespace_df.empty and len(namespace_df) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(namespace_df['Namespace'], namespace_df['Vector Count'], color='skyblue')\n",
    "    plt.title('Vector Count by Namespace')\n",
    "    plt.xlabel('Namespace')\n",
    "    plt.ylabel('Vector Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed1c7a",
   "metadata": {},
   "source": [
    "## Vector Metadata Analysis\n",
    "\n",
    "Let's analyze the metadata associated with the vectors to understand the distribution of landmark properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15526ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nyc_landmarks.vectordb.pinecone_db:Query returned 200 matches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 200 vector samples\n"
     ]
    }
   ],
   "source": [
    "# Function to sample vectors and retrieve metadata with direct index access\n",
    "def sample_vectors(pinecone_db, sample_size=100, use_mock=False):\n",
    "    \"\"\"\n",
    "    Sample vectors from the Pinecone database to analyze metadata.\n",
    "    \n",
    "    Args:\n",
    "        pinecone_db: The Pinecone database client\n",
    "        sample_size: Number of vectors to sample\n",
    "        use_mock: Whether to use mock data instead of real queries\n",
    "        \n",
    "    Returns:\n",
    "        List of vector samples with metadata\n",
    "    \"\"\"\n",
    "    # If mock data is requested or we know there's an issue, return mock data\n",
    "    if use_mock:\n",
    "        print(\"Generating mock vector samples for demonstration...\")\n",
    "        # Create realistic mock data that resembles NYC landmark data\n",
    "        mock_samples = []\n",
    "        \n",
    "        # Common NYC landmark characteristics\n",
    "        boroughs = [\"Manhattan\", \"Brooklyn\", \"Queens\", \"Bronx\", \"Staten Island\"]\n",
    "        landmark_types = [\"Individual Landmark\", \"Interior Landmark\", \"Historic District\", \"Scenic Landmark\"]\n",
    "        periods = [\"Federal\", \"Greek Revival\", \"Gothic Revival\", \"Italianate\", \"Beaux-Arts\", \"Art Deco\"]\n",
    "        architects = [\"McKim, Mead & White\", \"Cass Gilbert\", \"Stanford White\", \n",
    "                     \"James Renwick Jr.\", \"Carr√®re and Hastings\", \"Shreve, Lamb & Harmon\"]\n",
    "        \n",
    "        # Generate realistic mock vector samples\n",
    "        for i in range(sample_size):\n",
    "            borough = np.random.choice(boroughs)\n",
    "            landmark_type = np.random.choice(landmark_types)\n",
    "            \n",
    "            # Create coordinates within NYC bounds\n",
    "            if borough == \"Manhattan\":\n",
    "                lat = 40.7831 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.9712 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Brooklyn\":\n",
    "                lat = 40.6782 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.9442 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Queens\":\n",
    "                lat = 40.7282 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.7949 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Bronx\":\n",
    "                lat = 40.8448 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.8648 + (np.random.random() - 0.5) * 0.1\n",
    "            else:  # Staten Island\n",
    "                lat = 40.5795 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -74.1502 + (np.random.random() - 0.5) * 0.1\n",
    "            \n",
    "            # Generate sample metadata\n",
    "            metadata = {\n",
    "                \"id\": f\"landmark-{i+1000}\",\n",
    "                \"name\": f\"NYC Landmark #{i+1}\",\n",
    "                \"title\": f\"Example {landmark_type} in {borough}\",\n",
    "                \"borough\": borough,\n",
    "                \"landmark_type\": landmark_type,\n",
    "                \"architectural_style\": np.random.choice(periods),\n",
    "                \"designated_date\": f\"{1965 + np.random.randint(0, 56)}-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}\",\n",
    "                \"architect\": np.random.choice(architects) if np.random.random() > 0.3 else \"Unknown\",\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lng,\n",
    "                \"chunk_index\": np.random.randint(0, 5),\n",
    "                \"text\": f\"This is an example text about a landmark in {borough}, New York City. It was designated as a {landmark_type}.\"\n",
    "            }\n",
    "            \n",
    "            # Create a sample with metadata and a score\n",
    "            sample = {\n",
    "                \"id\": f\"vec-{i}\",\n",
    "                \"metadata\": metadata,\n",
    "                \"score\": 0.85 - (np.random.random() * 0.3)\n",
    "            }\n",
    "            \n",
    "            mock_samples.append(sample)\n",
    "        \n",
    "        print(f\"Created {len(mock_samples)} mock vector samples\")\n",
    "        return mock_samples\n",
    "    \n",
    "    # Try to query real data if possible\n",
    "    try:\n",
    "        if not pinecone_db.index:\n",
    "            print(\"‚ùå Pinecone index not initialized for vector sampling\")\n",
    "            return []\n",
    "            \n",
    "        # Use the updated query method that we fixed in the PineconeDB class\n",
    "        if hasattr(pinecone_db, 'query_vectors') and callable(getattr(pinecone_db, 'query_vectors')):\n",
    "            # Use the client's query method\n",
    "            random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "            results = pinecone_db.query_vectors(\n",
    "                query_vector=random_vector,\n",
    "                top_k=sample_size,\n",
    "                filter_dict=None\n",
    "            )\n",
    "            return results\n",
    "        \n",
    "        # For direct index querying with the new Pinecone SDK\n",
    "        elif hasattr(pinecone_db.index, 'query') and callable(getattr(pinecone_db.index, 'query')):\n",
    "            # Generate a random query vector\n",
    "            random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "            \n",
    "            # Perform the query directly on the index\n",
    "            results = pinecone_db.index.query(\n",
    "                vector=random_vector,\n",
    "                top_k=sample_size,\n",
    "                include_metadata=True,\n",
    "                namespace=pinecone_db.namespace\n",
    "            )\n",
    "            \n",
    "            # Process the matches safely with the new SDK structure\n",
    "            if hasattr(results, 'matches'):\n",
    "                matches = results.matches\n",
    "                processed_matches = []\n",
    "                \n",
    "                for match in matches:\n",
    "                    # Create a dictionary with safely extracted properties\n",
    "                    match_dict = {\n",
    "                        \"id\": match.id if hasattr(match, \"id\") else None,\n",
    "                        \"score\": match.score if hasattr(match, \"score\") else None,\n",
    "                        \"metadata\": match.metadata if hasattr(match, \"metadata\") else {}\n",
    "                    }\n",
    "                    processed_matches.append(match_dict)\n",
    "                \n",
    "                return processed_matches\n",
    "            else:\n",
    "                print(\"‚ùå Query results don't contain 'matches' attribute\")\n",
    "                return []\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Neither query_vectors nor query methods are available\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sampling vectors: {e}\")\n",
    "        print(\"This could be due to connection issues or API version mismatch.\")\n",
    "        return []\n",
    "\n",
    "# Try to get real vector samples first\n",
    "use_mock_data = False  # Set to True to force using mock data\n",
    "sample_size = 200      # Adjust based on your database size\n",
    "\n",
    "vector_samples = sample_vectors(pinecone_db, sample_size)\n",
    "\n",
    "# If we didn't get any samples, use mock data\n",
    "if not vector_samples:\n",
    "    print(\"No real vector samples retrieved. Using mock data for demonstration...\")\n",
    "    vector_samples = sample_vectors(pinecone_db, sample_size, use_mock=True)\n",
    "\n",
    "print(f\"Working with {len(vector_samples)} vector samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620a72df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         field_counts[key] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create DataFrame for field distribution\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m field_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mField\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(field_counts\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(field_counts\u001b[38;5;241m.\u001b[39mvalues()),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPercentage\u001b[39m\u001b[38;5;124m'\u001b[39m: [count\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_metadata)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m count \u001b[38;5;129;01min\u001b[39;00m field_counts\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     17\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Display field distribution\u001b[39;00m\n\u001b[1;32m     20\u001b[0m display(field_df)\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/internals/construction.py:480\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 480\u001b[0m     columns \u001b[38;5;241m=\u001b[39m Index(keys) \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;28;01melse\u001b[39;00m default_index(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    481\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [com\u001b[38;5;241m.\u001b[39mmaybe_iterable_to_list(data[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/indexes/base.py:565\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    562\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     arr \u001b[38;5;241m=\u001b[39m sanitize_array(data, \u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/construction.py:654\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_convert_platform(data)\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    656\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[0;32m~/miniconda/envs/nyc-landmarks/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:138\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[1;32m    137\u001b[0m     arr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, arr)\n\u001b[0;32m--> 138\u001b[0m     arr \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(arr)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32mlib.pyx:2538\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# Analyze metadata fields\n",
    "if vector_samples:\n",
    "    # Extract all metadata fields\n",
    "    all_metadata = [sample.get('metadata', {}) for sample in vector_samples]\n",
    "    \n",
    "    # Count metadata fields\n",
    "    field_counts = Counter()\n",
    "    for metadata in all_metadata:\n",
    "        for key in metadata.keys():\n",
    "            field_counts[key] += 1\n",
    "    \n",
    "    # Create DataFrame for field distribution\n",
    "    field_df = pd.DataFrame({\n",
    "        'Field': list(field_counts.keys()),\n",
    "        'Count': list(field_counts.values()),\n",
    "        'Percentage': [count/len(all_metadata)*100 for count in field_counts.values()]\n",
    "    }).sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display field distribution\n",
    "    display(field_df)\n",
    "    \n",
    "    # Visualize top 10 metadata fields\n",
    "    top_fields = field_df.head(10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(top_fields['Field'], top_fields['Count'], color='lightgreen')\n",
    "    plt.title('Top 10 Metadata Fields')\n",
    "    plt.xlabel('Metadata Field')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No vector samples available for metadata analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab734c",
   "metadata": {},
   "source": [
    "## Geographical Distribution\n",
    "\n",
    "If the vectors contain location information, let's visualize the geographical distribution of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geographical information and create a map\n",
    "def create_landmark_map(vector_samples):\n",
    "    \"\"\"\n",
    "    Create a map visualization of landmarks with geographical information.\n",
    "    \"\"\"\n",
    "    # Extract latitude and longitude information if available\n",
    "    geo_data = []\n",
    "    for sample in vector_samples:\n",
    "        metadata = sample.get('metadata', {})\n",
    "        \n",
    "        # Check if the required fields exist - names may vary by your schema\n",
    "        lat = metadata.get('latitude') or metadata.get('lat')\n",
    "        lng = metadata.get('longitude') or metadata.get('lng')\n",
    "        name = metadata.get('name') or metadata.get('title') or \"Unknown\"\n",
    "        \n",
    "        if lat and lng:\n",
    "            try:\n",
    "                geo_data.append({\n",
    "                    'name': name,\n",
    "                    'lat': float(lat),\n",
    "                    'lng': float(lng),\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "            except (ValueError, TypeError):\n",
    "                # Skip if conversion to float fails\n",
    "                pass\n",
    "    \n",
    "    # Create map if we have geo data\n",
    "    if geo_data:\n",
    "        # Create a dataframe for the geographical data\n",
    "        geo_df = pd.DataFrame(geo_data)\n",
    "        \n",
    "        # Center the map on the mean coordinates\n",
    "        center_lat = geo_df['lat'].mean()\n",
    "        center_lng = geo_df['lng'].mean()\n",
    "        \n",
    "        # Create a map\n",
    "        m = folium.Map(location=[center_lat, center_lng], zoom_start=12)\n",
    "        \n",
    "        # Add a marker cluster\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "        \n",
    "        # Add markers for each landmark\n",
    "        for _, row in geo_df.iterrows():\n",
    "            popup_html = f\"<b>{row['name']}</b>\"\n",
    "            folium.Marker(\n",
    "                location=[row['lat'], row['lng']],\n",
    "                popup=folium.Popup(popup_html, max_width=300),\n",
    "                icon=folium.Icon(color='blue', icon='info-sign')\n",
    "            ).add_to(marker_cluster)\n",
    "        \n",
    "        # Display the map\n",
    "        return m\n",
    "    else:\n",
    "        print(\"No geographical data found in the vector metadata\")\n",
    "        return None\n",
    "\n",
    "# Create and display the map\n",
    "landmark_map = create_landmark_map(vector_samples)\n",
    "if landmark_map:\n",
    "    display(landmark_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b645017",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive analysis of the NYC Landmarks vector database. The analysis includes:\n",
    "\n",
    "1. Database size and vector counts by namespace\n",
    "2. Metadata distribution analysis\n",
    "3. Vector distribution across landmarks\n",
    "4. Vector clustering and dimensionality reduction visualization\n",
    "\n",
    "These insights help us understand the structure and content of the vector database, enabling better optimization and usage of the data for landmark information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-landmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
