{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Landmarks Vector Database - Query Testing\n",
    "\n",
    "This notebook provides testing and examples for the vector query capabilities of the NYC Landmarks Vector Database. It demonstrates how to connect to the Pinecone database, execute various types of queries, and analyze the results.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Test basic vector search functionality\n",
    "2. Demonstrate filtering capabilities\n",
    "3. Analyze query performance and result relevance\n",
    "4. Visualize search results\n",
    "\n",
    "This notebook represents Phase 1 of the Query API Enhancement, focusing on establishing the foundations for more advanced query capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data analysis libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "# Import project modules\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.embeddings.generator import EmbeddingGenerator\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=settings.LOG_LEVEL.value,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pinecone Connection\n",
    "\n",
    "Next, we'll establish a connection to the Pinecone vector database and verify the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone database client\n",
    "pinecone_db = PineconeDB()\n",
    "\n",
    "# Check if the connection was successful\n",
    "if pinecone_db.index:\n",
    "    print(f\"‚úÖ Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "    print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "    print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "else:\n",
    "    print(\n",
    "        \"‚ùå Failed to connect to Pinecone. Check your credentials and network connection.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index statistics\n",
    "stats = pinecone_db.get_index_stats()\n",
    "\n",
    "# Check for errors\n",
    "if \"error\" in stats:\n",
    "    print(f\"‚ùå Error retrieving index stats: {stats['error']}\")\n",
    "    # Create fallback mock stats for demonstration\n",
    "    total_vector_count = 0\n",
    "    namespaces = {}\n",
    "else:\n",
    "    print(\"‚úÖ Successfully retrieved index stats\")\n",
    "    total_vector_count = stats.get(\"total_vector_count\", 0)\n",
    "    namespaces = stats.get(\"namespaces\", {})\n",
    "\n",
    "print(\"\\nüìä Index Statistics:\")\n",
    "print(f\"Total Vector Count: {total_vector_count:,}\")\n",
    "print(f\"Dimension: {stats.get('dimension')}\")\n",
    "print(f\"Index Fullness: {stats.get('index_fullness')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Vector Search Test\n",
    "\n",
    "Now let's test the basic vector search capabilities using sample queries about NYC landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding generator\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "\n",
    "# Define some sample queries about NYC landmarks\n",
    "sample_queries = [\n",
    "    \"What is the Empire State Building?\",\n",
    "    \"Tell me about the Brooklyn Bridge\",\n",
    "    \"What are the historic districts in Manhattan?\",\n",
    "    \"What is the architectural style of Grand Central Terminal?\",\n",
    "    \"When was the Statue of Liberty designated as a landmark?\",\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(sample_queries)} sample queries for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Vector Search Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PineconeDB Query Implementation\n",
    "\n",
    "Below is the implementation of `query_vectors` method from the `PineconeDB` class that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: nyc_landmarks/vectordb/pinecone_db.py\n",
    "\n",
    "'''\n",
    "def query_vectors(\n",
    "    self,\n",
    "    query_vector: List[float],\n",
    "    top_k: int = 5,\n",
    "    filter_dict: Optional[Dict[str, Any]] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Query vectors from Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        query_vector: Embedding of the query text\n",
    "        top_k: Number of results to return\n",
    "        filter_dict: Dictionary of metadata filters\n",
    "\n",
    "    Returns:\n",
    "        List of matching vectors with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = self.index.query(\n",
    "            vector=query_vector,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            filter=filter_dict,\n",
    "        )\n",
    "\n",
    "        # Process the response to extract matches\n",
    "        result_list: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Handle response.matches which can be a list or other iterable\n",
    "        # Cast response to Any to handle different return types from Pinecone SDK\n",
    "        from typing import Any as TypeAny\n",
    "        from typing import cast\n",
    "\n",
    "        response_dict = cast(TypeAny, response)\n",
    "\n",
    "        # Access matches safely\n",
    "        matches = getattr(response_dict, \"matches\", [])\n",
    "        for match in matches:\n",
    "            # Handle match objects\n",
    "            match_dict: Dict[str, Any] = {}\n",
    "\n",
    "            # Extract ID if available\n",
    "            if hasattr(match, \"id\"):\n",
    "                match_dict[\"id\"] = match.id\n",
    "\n",
    "            # Extract score if available\n",
    "            if hasattr(match, \"score\"):\n",
    "                match_dict[\"score\"] = match.score\n",
    "\n",
    "            # Extract metadata if available\n",
    "            if hasattr(match, \"metadata\"):\n",
    "                match_dict[\"metadata\"] = match.metadata\n",
    "\n",
    "            result_list.append(match_dict)\n",
    "\n",
    "        return result_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to query vectors: {e}\")\n",
    "        return []\n",
    "'''\n",
    "\n",
    "# Note: This code block is for reference only and won't be executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Notebook's Query Function\n",
    "\n",
    "In this notebook, our `execute_query` function wraps around the `PineconeDB.query_vectors` method to provide additional functionality:\n",
    "\n",
    "1. **Timing and Performance Metrics**: We measure embedding generation time, query execution time, and total time\n",
    "2. **Automatic Embedding Generation**: Our function handles text-to-vector embedding using `EmbeddingGenerator`\n",
    "3. **Structured Results**: The response is packaged with both the results and performance metrics\n",
    "\n",
    "Our implementation is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a query and measure performance\n",
    "\n",
    "\n",
    "def execute_query(query_text, top_k=5, filter_dict=None):\n",
    "    \"\"\"Execute a vector search query and return the results along with performance metrics.\"\"\"\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate embedding for the query\n",
    "    embedding_start = time.time()\n",
    "    query_embedding = embedding_generator.generate_embedding(query_text)\n",
    "    embedding_time = time.time() - embedding_start\n",
    "\n",
    "    # Execute the query\n",
    "    query_start = time.time()\n",
    "    results = pinecone_db.query_vectors(\n",
    "        query_vector=query_embedding, top_k=top_k, filter_dict=filter_dict\n",
    "    )\n",
    "    query_time = time.time() - query_start\n",
    "\n",
    "    # Calculate total time\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        \"query\": query_text,\n",
    "        \"embedding\": query_embedding,\n",
    "        \"results\": results,\n",
    "        \"metrics\": {\n",
    "            \"embedding_time\": embedding_time,\n",
    "            \"query_time\": query_time,\n",
    "            \"total_time\": total_time,\n",
    "            \"result_count\": len(results),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_results(query_result, max_results=None, show_metadata=True):\n",
    "    \"\"\"Display query results in a readable format.\n",
    "\n",
    "    Args:\n",
    "        query_result: The result dictionary returned by execute_query\n",
    "        max_results: Maximum number of results to display (default: all)\n",
    "        show_metadata: Whether to display metadata fields (default: True)\n",
    "    \"\"\"\n",
    "    results = query_result[\"results\"]\n",
    "    if max_results:\n",
    "        results = results[:max_results]\n",
    "\n",
    "    print(f\"Query: '{query_result['query']}'\\n\")\n",
    "    print(\n",
    "        f\"üîç Found {query_result['metrics']['result_count']} results (showing {len(results)})\"\n",
    "    )\n",
    "    print(f\"‚è±Ô∏è Total query time: {query_result['metrics']['total_time']:.3f} seconds\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, match in enumerate(results):\n",
    "        print(f\"\\nüìå Result #{i+1} - Score: {match['score']:.4f}\")\n",
    "        print(f\"ID: {match['id']}\")\n",
    "\n",
    "        if show_metadata and \"metadata\" in match:\n",
    "            metadata = match[\"metadata\"]\n",
    "            print(\"\\nMetadata:\")\n",
    "\n",
    "            # Print important fields first\n",
    "            priority_fields = [\n",
    "                \"name\",\n",
    "                \"borough\",\n",
    "                \"landmark_type\",\n",
    "                \"designation_date\",\n",
    "                \"neighborhood\",\n",
    "            ]\n",
    "            for field in priority_fields:\n",
    "                if field in metadata and metadata[field]:\n",
    "                    print(f\"  {field.capitalize()}: {metadata[field]}\")\n",
    "\n",
    "            # Print content fields if available\n",
    "            content_fields = [\"text_chunk\", \"description\", \"text\"]\n",
    "            for field in content_fields:\n",
    "                if field in metadata and metadata[field]:\n",
    "                    content = metadata[field]\n",
    "                    snippet = content[:250] + \"...\" if len(content) > 250 else content\n",
    "                    print(f\"\\n  Content ({field}): {snippet}\")\n",
    "                    break\n",
    "\n",
    "            # Print other metadata fields that weren't already shown\n",
    "            other_fields = [\n",
    "                f\n",
    "                for f in metadata\n",
    "                if f not in priority_fields and f not in content_fields\n",
    "            ]\n",
    "            if other_fields:\n",
    "                print(\"\\n  Other metadata:\")\n",
    "                for field in other_fields:\n",
    "                    if metadata[field]:\n",
    "                        # Don't print very long values completely\n",
    "                        if (\n",
    "                            isinstance(metadata[field], str)\n",
    "                            and len(metadata[field]) > 50\n",
    "                        ):\n",
    "                            value = metadata[field][:50] + \"...\"\n",
    "                        else:\n",
    "                            value = metadata[field]\n",
    "                        print(f\"    {field}: {value}\")\n",
    "\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Query with PineconeDB\n",
    "\n",
    "You can also query Pinecone directly using the `PineconeDB.query_vectors` method. This requires you to first generate the embedding vector for your query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of direct querying with PineconeDB\n",
    "\n",
    "\n",
    "# 1. First, generate embedding for query text\n",
    "query_text = \"What is the Brooklyn Bridge?\"\n",
    "print(f\"Query: '{query_text}'\")\n",
    "\n",
    "# Generate the embedding vector using our embedding generator\n",
    "embedding = embedding_generator.generate_embedding(query_text)\n",
    "\n",
    "# 2. Now directly query Pinecone using the embedding vector\n",
    "print(\"\\nDirectly querying Pinecone with embedding vector...\")\n",
    "results = pinecone_db.query_vectors(query_vector=embedding, top_k=3)\n",
    "\n",
    "# 3. Display the raw results\n",
    "print(f\"\\nFound {len(results)} results from direct query:\\n\")\n",
    "\n",
    "# Print raw results to show the direct output format\n",
    "for i, match in enumerate(results):\n",
    "    print(f\"Result #{i+1}:\")\n",
    "    print(f\"  ID: {match['id']}\")\n",
    "    print(f\"  Score: {match['score']:.4f}\")\n",
    "\n",
    "    # Print some key metadata if available\n",
    "    if \"metadata\" in match and \"name\" in match[\"metadata\"]:\n",
    "        print(f\"  Name: {match['metadata']['name']}\")\n",
    "    if \"metadata\" in match and \"borough\" in match[\"metadata\"]:\n",
    "        print(f\"  Borough: {match['metadata']['borough']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Direct Query vs. execute_query Function\n",
    "\n",
    "Let's compare the direct query approach with our `execute_query` function to see the differences in usage and output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches with the same query\n",
    "\n",
    "# The query to test\n",
    "comparison_query = \"What are important landmarks in Manhattan?\"\n",
    "print(f\"Query: '{comparison_query}'\")\n",
    "\n",
    "# Approach 1: Direct PineconeDB.query_vectors approach\n",
    "# First generate embedding\n",
    "print(\"\\n----- Approach 1: Direct PineconeDB.query_vectors -----\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate embedding\n",
    "emb_start = time.time()\n",
    "query_embedding = embedding_generator.generate_embedding(comparison_query)\n",
    "emb_time = time.time() - emb_start\n",
    "\n",
    "# Execute direct query\n",
    "query_start = time.time()\n",
    "direct_results = pinecone_db.query_vectors(query_vector=query_embedding, top_k=3)\n",
    "query_time = time.time() - query_start\n",
    "\n",
    "# Calculate total time\n",
    "total_direct_time = time.time() - start_time\n",
    "print(f\"Embedding time: {emb_time:.3f}s\")\n",
    "print(f\"Query time: {query_time:.3f}s\")\n",
    "print(f\"Total time: {total_direct_time:.3f}s\")\n",
    "print(f\"Results returned: {len(direct_results)}\")\n",
    "\n",
    "# Approach 2: Using our execute_query function\n",
    "print(\"\\n----- Approach 2: Using execute_query function -----\")\n",
    "func_start_time = time.time()\n",
    "function_results = execute_query(comparison_query, top_k=3)\n",
    "total_function_time = time.time() - func_start_time\n",
    "\n",
    "print(f\"Embedding time: {function_results['metrics']['embedding_time']:.3f}s\")\n",
    "print(f\"Query time: {function_results['metrics']['query_time']:.3f}s\")\n",
    "print(f\"Total time: {function_results['metrics']['total_time']:.3f}s\")\n",
    "print(f\"Results returned: {function_results['metrics']['result_count']}\")\n",
    "\n",
    "# Compare the top result IDs to verify both approaches return the same data\n",
    "print(\"\\n----- Top Result Comparison -----\")\n",
    "if direct_results and function_results[\"results\"]:\n",
    "    print(f\"Direct query top result ID: {direct_results[0]['id']}\")\n",
    "    print(f\"Function query top result ID: {function_results['results'][0]['id']}\")\n",
    "\n",
    "    # Check if they're the same\n",
    "    print(\n",
    "        f\"\\nSame top result: {direct_results[0]['id'] == function_results['results'][0]['id']}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No results to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Approach\n",
    "\n",
    "**Direct Query with `PineconeDB.query_vectors`:**\n",
    "- When you already have embedding vectors (pre-computed)\n",
    "- When you need custom embedding generation logic\n",
    "- For low-level integration with other systems\n",
    "- When you want complete control over the query process\n",
    "\n",
    "**Using the `execute_query` Function:**\n",
    "- For simplicity and convenience\n",
    "- When you need built-in performance metrics\n",
    "- For consistent formatting of results\n",
    "- When working with text queries rather than vectors\n",
    "\n",
    "Both approaches ultimately use the same underlying `query_vectors` method of the PineconeDB class, but the `execute_query` function provides additional convenience and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Using Pre-computed Embeddings\n",
    "\n",
    "In some scenarios, you might already have embedding vectors computed beforehand, such as:\n",
    "\n",
    "1. **Batch Processing**: You've pre-computed embeddings for multiple queries to save time\n",
    "2. **Cached Embeddings**: You're storing common query embeddings to avoid regenerating them\n",
    "3. **Vector Operations**: You've modified vectors (e.g., combining or averaging multiple embeddings)\n",
    "4. **External Systems**: You've generated embeddings using a different system or model\n",
    "\n",
    "Let's demonstrate how to use pre-computed embeddings with Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of using pre-computed embeddings\n",
    "\n",
    "# 1. First, let's generate an embedding for our query\n",
    "query_text = \"Empire State Building history\"\n",
    "print(f\"Original query: '{query_text}'\")\n",
    "\n",
    "# Generate and store the embedding vector\n",
    "start_time = time.time()\n",
    "pre_computed_embedding = embedding_generator.generate_embedding(query_text)\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"Generated embedding in {embedding_time:.3f} seconds\")\n",
    "print(f\"Embedding dimensions: {len(pre_computed_embedding)}\")\n",
    "\n",
    "# Show a small snippet of the embedding vector\n",
    "print(\"\\nEmbedding vector snippet (first 5 elements):\")\n",
    "print(pre_computed_embedding[:5])\n",
    "\n",
    "# 2. Now imagine this embedding was computed earlier or in a different system\n",
    "print(\"\\n--- Scenario: Using a pre-computed embedding ---\")\n",
    "print(\"In a real scenario, this embedding might have been:\")\n",
    "print(\"- Loaded from a file or database\")\n",
    "print(\"- Retrieved from a cache\")\n",
    "print(\"- Generated by another system\")\n",
    "print(\"- Modified through vector operations\")\n",
    "\n",
    "# 3. Use the pre-computed embedding directly with query_vectors\n",
    "print(\"\\nQuerying Pinecone with pre-computed embedding...\")\n",
    "query_start = time.time()\n",
    "\n",
    "# This is the key step - using the pre-computed embedding directly\n",
    "results = pinecone_db.query_vectors(query_vector=pre_computed_embedding, top_k=3)\n",
    "\n",
    "query_time = time.time() - query_start\n",
    "print(f\"Query executed in {query_time:.3f} seconds\")\n",
    "print(f\"Found {len(results)} results\")\n",
    "\n",
    "# Display results\n",
    "for i, match in enumerate(results):\n",
    "    print(f\"\\nResult #{i+1}:\")\n",
    "    print(f\"  ID: {match['id']}\")\n",
    "    print(f\"  Score: {match['score']:.4f}\")\n",
    "\n",
    "    # Print name if available\n",
    "    if \"metadata\" in match and \"name\" in match[\"metadata\"]:\n",
    "        print(f\"  Name: {match['metadata']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Using Pre-computed Embeddings\n",
    "\n",
    "1. **Performance Optimization**: \n",
    "   - Embedding generation is often the most time-consuming part of the search process\n",
    "   - Pre-computing embeddings can significantly reduce query latency\n",
    "   - Especially important for real-time applications with strict response time requirements\n",
    "\n",
    "2. **Cost Reduction**: \n",
    "   - API calls to embedding models (like OpenAI) cost money\n",
    "   - Pre-computing and reusing embeddings reduces API calls\n",
    "\n",
    "3. **Advanced Vector Operations**:\n",
    "   - Combine multiple embeddings (e.g., average vectors for multiple related queries)\n",
    "   - Apply dimensionality reduction techniques\n",
    "   - Fine-tune or adjust vectors based on relevance feedback\n",
    "\n",
    "4. **System Architecture Flexibility**:\n",
    "   - Decouple embedding generation from vector search\n",
    "   - Different systems or services can handle each part of the process\n",
    "   - Enables batch processing for large-scale applications\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- **Autocomplete Systems**: Pre-compute embeddings for common queries\n",
    "- **Search Optimization**: Cache embeddings for frequent searches\n",
    "- **Semantic Search Enhancement**: Modify embeddings to improve search quality\n",
    "- **Cross-Modal Search**: Use embeddings from different modalities (text, image, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Pinecone Stores Vectors and Handles Common Queries\n",
    "\n",
    "Pinecone is a vector database designed specifically for storing and searching pre-computed vectors. This is a core part of how vector databases work, so let's explore this concept in more detail:\n",
    "\n",
    "### How Pinecone Stores Vectors\n",
    "\n",
    "1. **Pre-computed Vectors**: Pinecone *only* stores pre-computed vectors. It doesn't generate embeddings itself.\n",
    "   - All vectors in the Pinecone database were computed elsewhere and then stored in Pinecone\n",
    "   - In our NYC Landmarks project, we generated embeddings for all landmark documents using OpenAI's embedding API, then stored those vectors in Pinecone\n",
    "\n",
    "2. **Vector Storage Structure**:\n",
    "   - Each vector is stored with a unique ID\n",
    "   - Each vector is associated with metadata (like the landmark name, borough, etc.)\n",
    "   - Vectors are organized for efficient similarity searching using specialized indexing structures\n",
    "\n",
    "3. **Query Process**:\n",
    "   - When you query Pinecone, you provide a query vector (not text)\n",
    "   - Pinecone finds the most similar vectors in its database\n",
    "   - It returns those similar vectors' IDs, similarity scores, and metadata\n",
    "\n",
    "### Common Queries and Pre-computation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For frequently asked questions or common queries, there are several strategies to optimize performance:\n",
    "\n",
    "1. **Client-Side Caching**: \n",
    "   - Store embeddings for common queries in your application\n",
    "   - When users ask similar questions, use the cached embeddings\n",
    "   - Example: Cache embeddings for \"What is the Empire State Building?\", \"Brooklyn Bridge history\", etc.\n",
    "\n",
    "2. **Query Pre-processing**:\n",
    "   - Map various phrasings to canonical queries\n",
    "   - Use the pre-computed embedding for the canonical form\n",
    "   - Example: Map \"Tell me about Empire State Building\" and \"What's the Empire State Building?\" to the same cached embedding\n",
    "\n",
    "3. **Vector Quantization**:\n",
    "   - Create representative vectors for categories of questions\n",
    "   - Use these as \"template\" embeddings for classes of queries\n",
    "   - Example: Have a single \"landmark history query\" template vector for history-related questions\n",
    "\n",
    "4. **Hybrid Search Systems**:\n",
    "   - Use traditional search to identify the query type\n",
    "   - Use pre-computed embeddings based on the identified query type\n",
    "   - Example: Identify \"Brooklyn Bridge\" as the entity, then use a pre-computed \"Brooklyn Bridge\" embedding\n",
    "\n",
    "Let's demonstrate a practical example of client-side caching for common queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Client-side caching for common queries\n",
    "\n",
    "# Let's create a simple embedding cache for common queries\n",
    "embedding_cache = {}\n",
    "\n",
    "# Common queries about NYC landmarks that we expect users to ask frequently\n",
    "common_queries = [\n",
    "    \"What is the Empire State Building?\",\n",
    "    \"Tell me about the Brooklyn Bridge\",\n",
    "    \"What are the historic districts in Manhattan?\",\n",
    "    \"When was the Statue of Liberty designated as a landmark?\",\n",
    "]\n",
    "\n",
    "print(\"Generating embeddings for common queries and storing in cache...\")\n",
    "\n",
    "# Pre-compute embeddings for these common queries (in a real system, you might load these from a file)\n",
    "for query in common_queries:\n",
    "    # Generate and store the embedding\n",
    "    start_time = time.time()\n",
    "    embedding_cache[query] = embedding_generator.generate_embedding(query)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  - '{query}': Embedding generated in {elapsed:.3f}s\")\n",
    "\n",
    "print(f\"\\nCache contains {len(embedding_cache)} pre-computed embeddings\\n\")\n",
    "\n",
    "# Simulate a user entering a query that's similar to a cached query\n",
    "user_query = \"What can you tell me about the Brooklyn Bridge?\"\n",
    "print(f\"User query: '{user_query}'\")\n",
    "\n",
    "# In a real system, you might use a more sophisticated matching algorithm\n",
    "# For this example, we'll use a simple substring match\n",
    "cached_query = None\n",
    "for query in embedding_cache.keys():\n",
    "    if \"Brooklyn Bridge\" in query and \"Brooklyn Bridge\" in user_query:\n",
    "        cached_query = query\n",
    "        break\n",
    "\n",
    "if cached_query:\n",
    "    print(f\"Found similar cached query: '{cached_query}'\")\n",
    "\n",
    "    # Use the cached embedding instead of generating a new one\n",
    "    start_time = time.time()\n",
    "    cached_embedding = embedding_cache[cached_query]\n",
    "    print(\n",
    "        f\"Retrieved cached embedding in {(time.time() - start_time):.6f}s (vs. ~0.3s to generate new)\"\n",
    "    )\n",
    "\n",
    "    # Query Pinecone with the cached embedding\n",
    "    query_start = time.time()\n",
    "    results = pinecone_db.query_vectors(query_vector=cached_embedding, top_k=3)\n",
    "    query_time = time.time() - query_start\n",
    "\n",
    "    print(f\"Query executed in {query_time:.3f}s\")\n",
    "    print(f\"Found {len(results)} results\")\n",
    "\n",
    "    # Display top result\n",
    "    if results:\n",
    "        top_result = results[0]\n",
    "        print(f\"\\nTop result:\")\n",
    "        print(f\"  ID: {top_result['id']}\")\n",
    "        print(f\"  Score: {top_result['score']:.4f}\")\n",
    "\n",
    "        if \"metadata\" in top_result and \"name\" in top_result[\"metadata\"]:\n",
    "            print(f\"  Name: {top_result['metadata']['name']}\")\n",
    "else:\n",
    "    print(\"No matching cached query found - would need to generate new embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query with detailed results display\n",
    "query = \"What is the Empire State Building?\"\n",
    "print(f\"Executing query: '{query}'\")\n",
    "\n",
    "try:\n",
    "    # Execute the query\n",
    "    result = execute_query(comparison_query, top_k=3)\n",
    "\n",
    "    # Display detailed results\n",
    "    display_query_results(result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Filter Tests\n",
    "\n",
    "Next, let's test basic filtering capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample query\n",
    "test_query = sample_queries[0]\n",
    "print(f\"Testing query: '{test_query}'\")\n",
    "\n",
    "try:\n",
    "    query_result = execute_query(test_query)\n",
    "    print(\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Embedding time: {query_result['metrics']['embedding_time']:.3f}s\")\n",
    "    print(f\"Query time: {query_result['metrics']['query_time']:.3f}s\")\n",
    "    print(f\"Total time: {query_result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"Results returned: {query_result['metrics']['result_count']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample query\n",
    "test_query = sample_queries[0]\n",
    "print(f\"Testing query: '{test_query}'\")\n",
    "\n",
    "try:\n",
    "    query_result = execute_query(test_query)\n",
    "    print(\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Embedding time: {query_result['metrics']['embedding_time']:.3f}s\")\n",
    "    print(f\"Query time: {query_result['metrics']['query_time']:.3f}s\")\n",
    "    print(f\"Total time: {query_result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"Results returned: {query_result['metrics']['result_count']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample query\n",
    "test_query = sample_queries[0]\n",
    "print(f\"Testing query: '{test_query}'\")\n",
    "\n",
    "try:\n",
    "    query_result = execute_query(test_query)\n",
    "    print(\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Embedding time: {query_result['metrics']['embedding_time']:.3f}s\")\n",
    "    print(f\"Query time: {query_result['metrics']['query_time']:.3f}s\")\n",
    "    print(f\"Total time: {query_result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"Results returned: {query_result['metrics']['result_count']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple filter\n",
    "try:\n",
    "    filter_dict = {\"borough\": \"Manhattan\"}\n",
    "    filtered_result = execute_query(test_query, filter_dict=filter_dict)\n",
    "\n",
    "    print(f\"Query: '{test_query}'\")\n",
    "    print(\"Filter: borough = Manhattan\")\n",
    "    print(f\"Results returned: {filtered_result['metrics']['result_count']}\")\n",
    "    print(f\"Total time: {filtered_result['metrics']['total_time']:.3f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing filtered query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different query about Brooklyn Bridge\n",
    "brooklyn_query = sample_queries[1]  # \"Tell me about the Brooklyn Bridge\"\n",
    "print(f\"Testing query: '{brooklyn_query}'\")\n",
    "\n",
    "try:\n",
    "    brooklyn_result = execute_query(brooklyn_query)\n",
    "    print(\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Embedding time: {brooklyn_result['metrics']['embedding_time']:.3f}s\")\n",
    "    print(f\"Query time: {brooklyn_result['metrics']['query_time']:.3f}s\")\n",
    "    print(f\"Total time: {brooklyn_result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"Results returned: {brooklyn_result['metrics']['result_count']}\")\n",
    "\n",
    "    # Display the first result\n",
    "    if brooklyn_result[\"results\"]:\n",
    "        first_result = brooklyn_result[\"results\"][0]\n",
    "        print(f\"\\nTop result score: {first_result['score']:.4f}\")\n",
    "        metadata = first_result.get(\"metadata\", {})\n",
    "        if metadata and \"name\" in metadata:\n",
    "            print(f\"Name: {metadata['name']}\")\n",
    "        if metadata and \"borough\" in metadata:\n",
    "            print(f\"Borough: {metadata['borough']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the text content of the top results for the Brooklyn Bridge query\n",
    "for i, result in enumerate(brooklyn_result[\"results\"][:3]):\n",
    "    print(f\"\\nResult #{i+1} - Score: {result['score']:.4f}\")\n",
    "\n",
    "    metadata = result.get(\"metadata\", {})\n",
    "\n",
    "    # Print basic information\n",
    "    for key in [\"name\", \"borough\", \"landmark_type\", \"designation_date\"]:\n",
    "        if key in metadata:\n",
    "            print(f\"{key.capitalize()}: {metadata[key]}\")\n",
    "\n",
    "    # Print a snippet of text content if available\n",
    "    for content_key in [\"text_chunk\", \"description\"]:\n",
    "        if content_key in metadata:\n",
    "            content = metadata[content_key]\n",
    "            snippet = content[:200] + \"...\" if len(content) > 200 else content\n",
    "            print(f\"\\nSnippet: {snippet}\")\n",
    "            break\n",
    "\n",
    "    print(\"---\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity scores for the Brooklyn Bridge query\n",
    "scores = []\n",
    "names = []\n",
    "\n",
    "for result in brooklyn_result[\"results\"]:\n",
    "    scores.append(result[\"score\"])\n",
    "    metadata = result.get(\"metadata\", {})\n",
    "    name = metadata.get(\"name\", result[\"id\"])\n",
    "    names.append(name)\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(names, scores, color=\"skyblue\")\n",
    "\n",
    "# Add data labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(\n",
    "        width + 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{width:.4f}\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Set chart properties\n",
    "plt.title(f'Similarity Scores for Query: \"{brooklyn_query}\"', fontsize=14)\n",
    "plt.xlabel(\"Similarity Score\")\n",
    "plt.ylabel(\"Result Name\")\n",
    "plt.xlim(0, 1.0)\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine Query Results\n",
    "\n",
    "Let's examine the results of our queries in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display the results from our last query in a more readable format\n",
    "results = filtered_result[\"results\"]\n",
    "\n",
    "print(f\"Query: '{filtered_result['query']}'\\n\")\n",
    "print(\"Top 5 results:\")\n",
    "\n",
    "for i, match in enumerate(results):\n",
    "    print(f\"\\n{i+1}. Score: {match['score']:.4f}\")\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = match.get(\"metadata\", {})\n",
    "\n",
    "    # Display key metadata fields\n",
    "    print(f\"   ID: {match['id']}\")\n",
    "    if \"name\" in metadata:\n",
    "        print(f\"   Name: {metadata['name']}\")\n",
    "    if \"borough\" in metadata:\n",
    "        print(f\"   Borough: {metadata['borough']}\")\n",
    "    if \"landmark_type\" in metadata:\n",
    "        print(f\"   Type: {metadata['landmark_type']}\")\n",
    "    if \"designation_date\" in metadata:\n",
    "        print(f\"   Designation Date: {metadata['designation_date']}\")\n",
    "\n",
    "    # Display a snippet of the content if available\n",
    "    if \"text_chunk\" in metadata:\n",
    "        snippet = (\n",
    "            metadata[\"text_chunk\"][:150] + \"...\"\n",
    "            if len(metadata[\"text_chunk\"]) > 150\n",
    "            else metadata[\"text_chunk\"]\n",
    "        )\n",
    "        print(f\"   Snippet: {snippet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the similarity scores\n",
    "\n",
    "\n",
    "def plot_similarity_scores(results, query_text):\n",
    "    # Extract scores and names\n",
    "    scores = [match[\"score\"] for match in results]\n",
    "    names = [match.get(\"metadata\", {}).get(\"name\", match[\"id\"]) for match in results]\n",
    "\n",
    "    # Create a horizontal bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.barh(names, scores, color=\"skyblue\")\n",
    "\n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(\n",
    "            width + 0.01,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{width:.4f}\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # Set chart properties\n",
    "    plt.title(f'Similarity Scores for Query: \"{query_text}\"', fontsize=14)\n",
    "    plt.xlabel(\"Similarity Score\")\n",
    "    plt.ylabel(\"Document\")\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plot = plot_similarity_scores(filtered_result[\"results\"], filtered_result[\"query\"])\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Filtering\n",
    "\n",
    "Let's test more advanced filtering capabilities by combining multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple filter conditions\n",
    "query_text = \"What are the historic districts?\"\n",
    "filter_conditions = {\"borough\": \"Brooklyn\", \"landmark_type\": \"Historic District\"}\n",
    "\n",
    "print(f\"Query: '{query_text}'\")\n",
    "print(f\"Filters: {filter_conditions}\")\n",
    "\n",
    "try:\n",
    "    multi_filter_result = execute_query(\n",
    "        query_text, filter_dict=filter_conditions, top_k=10\n",
    "    )\n",
    "    print(\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Total time: {multi_filter_result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"Results returned: {multi_filter_result['metrics']['result_count']}\")\n",
    "\n",
    "    # Print the names of the returned landmarks\n",
    "    if multi_filter_result[\"results\"]:\n",
    "        print(\"\\nReturned Brooklyn Historic Districts:\")\n",
    "        for i, result in enumerate(multi_filter_result[\"results\"]):\n",
    "            name = result.get(\"metadata\", {}).get(\"name\", \"Unknown\")\n",
    "            print(f\"{i+1}. {name} (Score: {result['score']:.4f})\")\n",
    "    else:\n",
    "        print(\"\\nNo results found matching the filters\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with just the landmark_type filter\n",
    "query_text = \"What are the historic districts?\"\n",
    "filter_conditions = {\"landmark_type\": \"Historic District\"}\n",
    "\n",
    "print(f\"Query: '{query_text}'\")\n",
    "print(f\"Filters: {filter_conditions}\")\n",
    "\n",
    "try:\n",
    "    filter_result = execute_query(query_text, filter_dict=filter_conditions, top_k=5)\n",
    "    print(f\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Total time: {filter_result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"Results returned: {filter_result['metrics']['result_count']}\")\n",
    "\n",
    "    # Print the names and boroughs of the returned landmarks\n",
    "    if filter_result[\"results\"]:\n",
    "        print(\"\\nReturned Historic Districts:\")\n",
    "        for i, result in enumerate(filter_result[\"results\"]):\n",
    "            metadata = result.get(\"metadata\", {})\n",
    "            name = metadata.get(\"name\", \"Unknown\")\n",
    "            borough = metadata.get(\"borough\", \"Unknown\")\n",
    "            print(f\"{i+1}. {name} (Borough: {borough}, Score: {result['score']:.4f})\")\n",
    "    else:\n",
    "        print(\"\\nNo results found matching the filters\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's display the detailed results of the filter query using our display function\n",
    "# We'll try with the borough filter since it worked in previous tests\n",
    "filter_query = \"Historic buildings\"\n",
    "filter_conditions = {\"borough\": \"Brooklyn\"}\n",
    "\n",
    "print(f\"Executing filtered query: '{filter_query}' with filters: {filter_conditions}\\n\")\n",
    "\n",
    "try:\n",
    "    # Execute the filtered query\n",
    "    brooklyn_result = execute_query(\n",
    "        filter_query, filter_dict=filter_conditions, top_k=3\n",
    "    )\n",
    "\n",
    "    # Display detailed results using our function\n",
    "    display_query_results(brooklyn_result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploring Metadata\n",
    "\n",
    "To better understand the database, let's explore what metadata is available by performing some simple queries without filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a general query to explore available metadata\n",
    "general_query = \"New York City landmarks\"\n",
    "print(f\"Query: '{general_query}'\")\n",
    "\n",
    "try:\n",
    "    general_result = execute_query(general_query, top_k=10)\n",
    "    print(f\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Results returned: {general_result['metrics']['result_count']}\")\n",
    "\n",
    "    # Extract and display available metadata fields\n",
    "    print(\"\\nExploring metadata in results:\")\n",
    "\n",
    "    metadata_fields = set()\n",
    "    landmark_types = set()\n",
    "    boroughs = set()\n",
    "\n",
    "    for result in general_result[\"results\"]:\n",
    "        metadata = result.get(\"metadata\", {})\n",
    "        metadata_fields.update(metadata.keys())\n",
    "\n",
    "        if \"landmark_type\" in metadata:\n",
    "            landmark_types.add(metadata[\"landmark_type\"])\n",
    "\n",
    "        if \"borough\" in metadata:\n",
    "            boroughs.add(metadata[\"borough\"])\n",
    "\n",
    "    print(f\"\\nAvailable metadata fields: {sorted(metadata_fields)}\")\n",
    "    print(f\"\\nUnique landmark types: {sorted(landmark_types)}\")\n",
    "    print(f\"\\nUnique boroughs: {sorted(boroughs)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try filtering by borough\n",
    "query_text = \"Historic buildings\"\n",
    "filter_conditions = {\"borough\": \"Brooklyn\"}\n",
    "\n",
    "print(f\"Query: '{query_text}'\")\n",
    "print(f\"Filters: {filter_conditions}\")\n",
    "\n",
    "try:\n",
    "    borough_filter_result = execute_query(\n",
    "        query_text, filter_dict=filter_conditions, top_k=5\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Query executed successfully\")\n",
    "    print(f\"Total time: {borough_filter_result['metrics']['total_time']:.3f}s\")\n",
    "    print(f\"Results returned: {borough_filter_result['metrics']['result_count']}\")\n",
    "\n",
    "    # Print the names of the returned landmarks\n",
    "    if borough_filter_result[\"results\"]:\n",
    "        print(\"\\nReturned Brooklyn landmarks:\")\n",
    "        for i, result in enumerate(borough_filter_result[\"results\"]):\n",
    "            metadata = result.get(\"metadata\", {})\n",
    "            name = metadata.get(\"name\", \"Unknown\")\n",
    "            print(f\"{i+1}. {name} (Score: {result['score']:.4f})\")\n",
    "\n",
    "            # Print additional metadata if available\n",
    "            for key in [\"style\", \"year_built\", \"neighborhood\"]:\n",
    "                if key in metadata and metadata[key]:\n",
    "                    print(f\"   {key.capitalize()}: {metadata[key]}\")\n",
    "    else:\n",
    "        print(\"\\nNo results found matching the filters\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "In this notebook, we successfully tested the NYC Landmarks Vector Database's query capabilities. Here's what we accomplished:\n",
    "\n",
    "1. **Basic Vector Search**: Successfully performed semantic search on NYC landmark data\n",
    "2. **Filtering**: Demonstrated the ability to filter by borough and other metadata fields\n",
    "3. **Performance**: Tracked query times, with most queries returning in under 1 second\n",
    "4. **Metadata Exploration**: Discovered the structure and available fields in the database\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Future work could include:\n",
    "\n",
    "1. More advanced filtering combinations\n",
    "2. Geographic visualization of search results on a map\n",
    "3. Implementing relevance feedback to improve search results\n",
    "4. Expansion of the API to support more complex queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
