{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279b235d",
   "metadata": {},
   "source": [
    "# NYC Landmarks Vector Database Statistics\n",
    "\n",
    "This notebook provides a comprehensive analysis of the NYC Landmarks data stored in the Pinecone vector database. It examines the vectors, metadata distribution, and overall statistics of the embeddings to give insights about the landmarks collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11db6d",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll import the necessary libraries and set up connections to the Pinecone database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# For map visualizations\n",
    "import folium\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983dac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299ee5d",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "\n",
    "Connect to the Pinecone database and verify the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone database client\n",
    "pinecone_db = PineconeDB()\n",
    "\n",
    "# Check if the connection was successful\n",
    "if pinecone_db.index:\n",
    "    print(f\"✅ Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "    print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "    print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "    print(f\"Metric: {pinecone_db.metric}\")\n",
    "else:\n",
    "    print(\n",
    "        \"❌ Failed to connect to Pinecone. Check your credentials and network connection.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed1c7a",
   "metadata": {},
   "source": [
    "## Vector Metadata Analysis\n",
    "\n",
    "Let's analyze the metadata associated with the vectors to understand the distribution of landmark properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15526ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_vectors(\n",
    "    pinecone_db: Any, sample_size: int = 100, use_mock: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Sample vectors from the Pinecone database to analyze metadata.\n",
    "\n",
    "    Args:\n",
    "        pinecone_db: The Pinecone database client\n",
    "        sample_size: Number of vectors to sample\n",
    "        use_mock: Whether to use mock data instead of real queries\n",
    "\n",
    "    Returns:\n",
    "        List of vector samples with metadata\n",
    "    \"\"\"\n",
    "    # If mock data is requested or we know there's an issue, return mock data\n",
    "    if use_mock:\n",
    "        print(\"Generating mock vector samples for demonstration...\")\n",
    "        # Create realistic mock data that resembles NYC landmark data\n",
    "        mock_samples = []\n",
    "\n",
    "        # Common NYC landmark characteristics\n",
    "        boroughs = [\"Manhattan\", \"Brooklyn\", \"Queens\", \"Bronx\", \"Staten Island\"]\n",
    "        landmark_types = [\n",
    "            \"Individual Landmark\",\n",
    "            \"Interior Landmark\",\n",
    "            \"Historic District\",\n",
    "            \"Scenic Landmark\",\n",
    "        ]\n",
    "        periods = [\n",
    "            \"Federal\",\n",
    "            \"Greek Revival\",\n",
    "            \"Gothic Revival\",\n",
    "            \"Italianate\",\n",
    "            \"Beaux-Arts\",\n",
    "            \"Art Deco\",\n",
    "        ]\n",
    "        architects = [\n",
    "            \"McKim, Mead & White\",\n",
    "            \"Cass Gilbert\",\n",
    "            \"Stanford White\",\n",
    "            \"James Renwick Jr.\",\n",
    "            \"Carrère and Hastings\",\n",
    "            \"Shreve, Lamb & Harmon\",\n",
    "        ]\n",
    "\n",
    "        # Generate realistic mock vector samples\n",
    "        for i in range(sample_size):\n",
    "            borough = np.random.choice(boroughs)\n",
    "            landmark_type = np.random.choice(landmark_types)\n",
    "\n",
    "            # Create coordinates within NYC bounds\n",
    "            if borough == \"Manhattan\":\n",
    "                lat = 40.7831 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.9712 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Brooklyn\":\n",
    "                lat = 40.6782 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.9442 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Queens\":\n",
    "                lat = 40.7282 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.7949 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Bronx\":\n",
    "                lat = 40.8448 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.8648 + (np.random.random() - 0.5) * 0.1\n",
    "            else:  # Staten Island\n",
    "                lat = 40.5795 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -74.1502 + (np.random.random() - 0.5) * 0.1\n",
    "\n",
    "            # Generate sample metadata\n",
    "            metadata = {\n",
    "                \"id\": f\"landmark-{i+1000}\",\n",
    "                \"name\": f\"NYC Landmark #{i+1}\",\n",
    "                \"title\": f\"Example {landmark_type} in {borough}\",\n",
    "                \"borough\": borough,\n",
    "                \"landmark_type\": landmark_type,\n",
    "                \"architectural_style\": np.random.choice(periods),\n",
    "                \"designated_date\": f\"{1965 + np.random.randint(0, 56)}-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}\",\n",
    "                \"architect\": (\n",
    "                    np.random.choice(architects)\n",
    "                    if np.random.random() > 0.3\n",
    "                    else \"Unknown\"\n",
    "                ),\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lng,\n",
    "                \"chunk_index\": np.random.randint(0, 5),\n",
    "                \"text\": f\"This is an example text about a landmark in {borough}, New York City. It was designated as a {landmark_type}.\",\n",
    "            }\n",
    "\n",
    "            # Create a sample with metadata and a score\n",
    "            sample = {\n",
    "                \"id\": f\"vec-{i}\",\n",
    "                \"metadata\": metadata,\n",
    "                \"score\": 0.85 - (np.random.random() * 0.3),\n",
    "            }\n",
    "\n",
    "            mock_samples.append(sample)\n",
    "\n",
    "        print(f\"Created {len(mock_samples)} mock vector samples\")\n",
    "        return mock_samples\n",
    "\n",
    "    # Try to query real data if possible\n",
    "    try:\n",
    "        if not pinecone_db.index:\n",
    "            print(\"❌ Pinecone index not initialized for vector sampling\")\n",
    "            return []\n",
    "\n",
    "        # Use the updated query method that we fixed in the PineconeDB class\n",
    "        if hasattr(pinecone_db, \"query_vectors\") and callable(\n",
    "            getattr(pinecone_db, \"query_vectors\")\n",
    "        ):\n",
    "            print(\"Using PineconeDB query_vectors method...\")\n",
    "            # Use the client's query method\n",
    "            random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "\n",
    "            try:\n",
    "                results = pinecone_db.query_vectors(\n",
    "                    query_vector=random_vector, top_k=sample_size, filter_dict=None\n",
    "                )\n",
    "                print(f\"Successfully queried {len(results)} vectors\")\n",
    "                return results\n",
    "            except Exception as e:\n",
    "                print(f\"Error using query_vectors: {e}\")\n",
    "                # Continue to try direct querying approach\n",
    "                pass\n",
    "\n",
    "        # For direct index querying with the new Pinecone SDK\n",
    "        if hasattr(pinecone_db.index, \"query\") and callable(\n",
    "            getattr(pinecone_db.index, \"query\")\n",
    "        ):\n",
    "            print(\"Using direct Pinecone index query method...\")\n",
    "            # Generate a random query vector\n",
    "            random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "\n",
    "            # Perform the query directly on the index\n",
    "            try:\n",
    "                results = pinecone_db.index.query(\n",
    "                    vector=random_vector,\n",
    "                    top_k=sample_size,\n",
    "                    include_metadata=True,\n",
    "                    namespace=pinecone_db.namespace,\n",
    "                )\n",
    "\n",
    "                # Debug the response structure\n",
    "                print(f\"Query response type: {type(results)}\")\n",
    "                if hasattr(results, \"matches\"):\n",
    "                    print(f\"Found {len(results.matches)} matches\")\n",
    "\n",
    "                # Process the matches safely with the new SDK structure\n",
    "                if hasattr(results, \"matches\"):\n",
    "                    matches = results.matches\n",
    "                    processed_matches = []\n",
    "\n",
    "                    for match in matches:\n",
    "                        # Debug each match\n",
    "                        print(\n",
    "                            f\"Processing match: {match.id if hasattr(match, 'id') else 'unknown'}\"\n",
    "                        )\n",
    "\n",
    "                        # Create a dictionary with safely extracted properties\n",
    "                        match_dict = {}\n",
    "\n",
    "                        # Extract ID safely\n",
    "                        if hasattr(match, \"id\"):\n",
    "                            match_dict[\"id\"] = match.id\n",
    "\n",
    "                        # Extract score safely\n",
    "                        if hasattr(match, \"score\"):\n",
    "                            match_dict[\"score\"] = match.score\n",
    "\n",
    "                        # Extract metadata safely\n",
    "                        if hasattr(match, \"metadata\"):\n",
    "                            if match.metadata is not None:\n",
    "                                match_dict[\"metadata\"] = match.metadata\n",
    "                            else:\n",
    "                                match_dict[\"metadata\"] = {}\n",
    "                        else:\n",
    "                            match_dict[\"metadata\"] = {}\n",
    "\n",
    "                        processed_matches.append(match_dict)\n",
    "\n",
    "                    print(f\"Processed {len(processed_matches)} matches successfully\")\n",
    "                    return processed_matches\n",
    "                else:\n",
    "                    print(\"❌ Query results don't contain 'matches' attribute\")\n",
    "                    # Try to extract data differently based on the actual response structure\n",
    "                    try:\n",
    "                        if isinstance(results, dict) and \"matches\" in results:\n",
    "                            raw_matches = results[\"matches\"]\n",
    "                            processed_matches = []\n",
    "                            for match in raw_matches:\n",
    "                                processed_matches.append(\n",
    "                                    {\n",
    "                                        \"id\": match.get(\"id\"),\n",
    "                                        \"score\": match.get(\"score\"),\n",
    "                                        \"metadata\": match.get(\"metadata\", {}),\n",
    "                                    }\n",
    "                                )\n",
    "                            return processed_matches\n",
    "                    except Exception as extract_error:\n",
    "                        print(f\"Error extracting alternative matches: {extract_error}\")\n",
    "                    return []\n",
    "            except Exception as query_error:\n",
    "                print(f\"Error during direct index query: {query_error}\")\n",
    "                return []\n",
    "\n",
    "        else:\n",
    "            print(\"❌ Neither query_vectors nor query methods are available\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error sampling vectors: {e}\")\n",
    "        print(\"This could be due to connection issues or API version mismatch.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Try to get real vector samples first\n",
    "use_mock_data = False  # Set to True to force using mock data\n",
    "sample_size = 200  # Adjust based on your database size\n",
    "\n",
    "vector_samples = sample_vectors(pinecone_db, sample_size)\n",
    "\n",
    "# If we didn't get any samples, use mock data\n",
    "if not vector_samples:\n",
    "    print(\"No real vector samples retrieved. Using mock data for demonstration...\")\n",
    "    vector_samples = sample_vectors(pinecone_db, sample_size, use_mock=True)\n",
    "\n",
    "print(f\"Working with {len(vector_samples)} vector samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze metadata fields\n",
    "if vector_samples:\n",
    "    print(f\"Analyzing metadata from {len(vector_samples)} vector samples\")\n",
    "\n",
    "    # Debug the structure of vector samples\n",
    "    print(f\"First sample type: {type(vector_samples[0])}\")\n",
    "    print(\n",
    "        f\"First sample keys: {vector_samples[0].keys() if hasattr(vector_samples[0], 'keys') else 'No keys method'}\"\n",
    "    )\n",
    "\n",
    "    # Extract all metadata fields safely\n",
    "    all_metadata = []\n",
    "    for sample in vector_samples:\n",
    "        try:\n",
    "            # Handle different possible formats of vector samples\n",
    "            if isinstance(sample, dict):\n",
    "                metadata = sample.get(\"metadata\", {})\n",
    "            elif hasattr(sample, \"metadata\"):\n",
    "                metadata = sample.metadata if sample.metadata else {}\n",
    "            else:\n",
    "                metadata = {}\n",
    "\n",
    "            if metadata:\n",
    "                all_metadata.append(metadata)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting metadata from sample: {e}\")\n",
    "\n",
    "    print(f\"Successfully extracted metadata from {len(all_metadata)} samples\")\n",
    "\n",
    "    # Proceed only if we have valid metadata\n",
    "    if all_metadata:\n",
    "        try:\n",
    "            # Count metadata fields\n",
    "            field_counts = Counter()\n",
    "            for metadata in all_metadata:\n",
    "                for key in metadata.keys():\n",
    "                    field_counts[key] += 1\n",
    "\n",
    "            # Create DataFrame for field distribution\n",
    "            field_df = (\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Field\": list(field_counts.keys()),\n",
    "                        \"Count\": list(field_counts.values()),\n",
    "                        \"Percentage\": [\n",
    "                            count / len(all_metadata) * 100\n",
    "                            for count in field_counts.values()\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "                .sort_values(\"Count\", ascending=False)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Display field distribution\n",
    "            print(f\"Generated DataFrame with {len(field_df)} metadata fields\")\n",
    "            display(field_df)\n",
    "\n",
    "            # Visualize top 10 metadata fields\n",
    "            if not field_df.empty:\n",
    "                top_fields = field_df.head(10)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                bars = plt.bar(\n",
    "                    top_fields[\"Field\"], top_fields[\"Count\"], color=\"lightgreen\"\n",
    "                )\n",
    "                plt.title(\"Top 10 Metadata Fields\")\n",
    "                plt.xlabel(\"Metadata Field\")\n",
    "                plt.ylabel(\"Count\")\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "                # Add count labels\n",
    "                for bar in bars:\n",
    "                    height = bar.get_height()\n",
    "                    plt.text(\n",
    "                        bar.get_x() + bar.get_width() / 2.0,\n",
    "                        height + 1,\n",
    "                        f\"{int(height)}\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"bottom\",\n",
    "                        rotation=0,\n",
    "                    )\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"No field data to visualize\")\n",
    "        except Exception as analysis_error:\n",
    "            print(f\"Error during metadata analysis: {analysis_error}\")\n",
    "    else:\n",
    "        print(\"No valid metadata extracted from vector samples\")\n",
    "else:\n",
    "    print(\"No vector samples available for metadata analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab734c",
   "metadata": {},
   "source": [
    "## Geographical Distribution\n",
    "\n",
    "If the vectors contain location information, let's visualize the geographical distribution of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geographical information and create a map\n",
    "\n",
    "\n",
    "def create_landmark_map(vector_samples):\n",
    "    \"\"\"\n",
    "    Create a map visualization of landmarks with geographical information.\n",
    "    \"\"\"\n",
    "    # Extract latitude and longitude information if available\n",
    "    geo_data = []\n",
    "    processed_samples = 0\n",
    "    geo_found = 0\n",
    "    errors = 0\n",
    "\n",
    "    print(f\"Processing {len(vector_samples)} vector samples for geographical data\")\n",
    "\n",
    "    for sample in vector_samples:\n",
    "        processed_samples += 1\n",
    "        try:\n",
    "            # Get metadata safely - handle both dict and object formats\n",
    "            if isinstance(sample, dict):\n",
    "                metadata = sample.get(\"metadata\", {})\n",
    "            elif hasattr(sample, \"metadata\"):\n",
    "                metadata = sample.metadata if sample.metadata else {}\n",
    "            else:\n",
    "                metadata = {}\n",
    "\n",
    "            # First try to get location from structured fields\n",
    "            location_data = metadata.get(\"location\", {})\n",
    "            lat = lng = None\n",
    "\n",
    "            # Try to get coordinates from a location object if available\n",
    "            if isinstance(location_data, dict):\n",
    "                lat = location_data.get(\"latitude\") or location_data.get(\"lat\")\n",
    "                lng = location_data.get(\"longitude\") or location_data.get(\"lng\")\n",
    "            elif hasattr(location_data, \"latitude\") and hasattr(\n",
    "                location_data, \"longitude\"\n",
    "            ):\n",
    "                lat = location_data.latitude\n",
    "                lng = location_data.longitude\n",
    "\n",
    "            # If not found in location object, try direct metadata fields\n",
    "            if not (lat and lng):\n",
    "                lat = metadata.get(\"latitude\") or metadata.get(\"lat\")\n",
    "                lng = metadata.get(\"longitude\") or metadata.get(\"lng\")\n",
    "\n",
    "            # Get a name for the landmark\n",
    "            name = metadata.get(\"name\") or metadata.get(\"title\") or \"Unknown\"\n",
    "\n",
    "            # If we have coordinates, add to geo data\n",
    "            if lat and lng:\n",
    "                try:\n",
    "                    geo_data.append(\n",
    "                        {\n",
    "                            \"name\": name,\n",
    "                            \"lat\": float(lat),\n",
    "                            \"lng\": float(lng),\n",
    "                            \"metadata\": metadata,\n",
    "                        }\n",
    "                    )\n",
    "                    geo_found += 1\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip if conversion to float fails\n",
    "                    errors += 1\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if errors <= 3:  # Limit error reporting to avoid flooding output\n",
    "                print(f\"Error processing sample for geo data: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"Processed {processed_samples} samples, found {geo_found} with geographical data, encountered {errors} errors\"\n",
    "    )\n",
    "\n",
    "    # Create map if we have geo data\n",
    "    if geo_data:\n",
    "        # Create a dataframe for the geographical data\n",
    "        geo_df = pd.DataFrame(geo_data)\n",
    "\n",
    "        # Center the map on the mean coordinates\n",
    "        center_lat = geo_df[\"lat\"].mean()\n",
    "        center_lng = geo_df[\"lng\"].mean()\n",
    "\n",
    "        # Create a map\n",
    "        m = folium.Map(location=[center_lat, center_lng], zoom_start=12)\n",
    "\n",
    "        # Add a marker cluster\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Add markers for each landmark\n",
    "        for _, row in geo_df.iterrows():\n",
    "            popup_html = f\"<b>{row['name']}</b>\"\n",
    "            folium.Marker(\n",
    "                location=[row[\"lat\"], row[\"lng\"]],\n",
    "                popup=folium.Popup(popup_html, max_width=300),\n",
    "                icon=folium.Icon(color=\"blue\", icon=\"info-sign\"),\n",
    "            ).add_to(marker_cluster)\n",
    "\n",
    "        # Display the map\n",
    "        return m\n",
    "    else:\n",
    "        print(\"No geographical data found in the vector metadata\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create and display the map\n",
    "landmark_map = create_landmark_map(vector_samples)\n",
    "if landmark_map:\n",
    "    display(landmark_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b645017",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive analysis of the NYC Landmarks vector database. The analysis includes:\n",
    "\n",
    "1. Database size and vector counts by namespace\n",
    "2. Metadata distribution analysis\n",
    "3. Vector distribution across landmarks\n",
    "4. Vector clustering and dimensionality reduction visualization\n",
    "\n",
    "These insights help us understand the structure and content of the vector database, enabling better optimization and usage of the data for landmark information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
