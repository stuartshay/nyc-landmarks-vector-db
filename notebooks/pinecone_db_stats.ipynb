{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279b235d",
   "metadata": {},
   "source": [
    "# NYC Landmarks Vector Database Statistics\n",
    "\n",
    "This notebook provides a comprehensive analysis of the NYC Landmarks data stored in the Pinecone vector database. It examines the vectors, metadata distribution, and overall statistics of the embeddings to give insights about the landmarks collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11db6d",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll import the necessary libraries and set up connections to the Pinecone database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# For map visualizations\n",
    "import folium\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from folium.plugins import MarkerCluster\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Vector analysis\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983dac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.db.db_client import DbClient\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299ee5d",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "\n",
    "Connect to the Pinecone database and verify the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone database client\n",
    "pinecone_db = PineconeDB()\n",
    "\n",
    "# Check if the connection was successful\n",
    "if pinecone_db.index:\n",
    "    print(f\"‚úÖ Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "    print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "    print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "    print(f\"Metric: {pinecone_db.metric}\")\n",
    "else:\n",
    "    print(\n",
    "        \"‚ùå Failed to connect to Pinecone. Check your credentials and network connection.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5f6bb",
   "metadata": {},
   "source": [
    "## Index Statistics\n",
    "\n",
    "Retrieve basic statistics about the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Pinecone connection and work with the index directly\n",
    "print(f\"\\nüîç Checking Pinecone index status:\")\n",
    "print(f\"Index object: {'Available' if pinecone_db.index else 'Not initialized'}\")\n",
    "print(f\"Index name: {pinecone_db.index_name}\")\n",
    "print(f\"API key set: {'Yes' if pinecone_db.api_key else 'No'}\")\n",
    "\n",
    "# Try direct approach with the index object instead of get_index_stats\n",
    "try:\n",
    "    if not pinecone_db.index:\n",
    "        print(\"‚ùå Pinecone index not initialized - attempting to reconnect...\")\n",
    "        # Try to re-initialize the connection (with error handling)\n",
    "        try:\n",
    "            from pinecone import Pinecone  # Use updated Pinecone SDK\n",
    "\n",
    "            # Initialize Pinecone again\n",
    "            pc = Pinecone(api_key=pinecone_db.api_key)\n",
    "            # Connect to existing index\n",
    "            pinecone_db.index = pc.Index(pinecone_db.index_name)\n",
    "            print(f\"‚úÖ Successfully reconnected to index: {pinecone_db.index_name}\")\n",
    "        except Exception as reconnect_error:\n",
    "            print(f\"‚ùå Reconnection failed: {reconnect_error}\")\n",
    "\n",
    "    # Use direct index call with manual error handling\n",
    "    if pinecone_db.index:\n",
    "        try:\n",
    "            # Try to get stats directly from the index object\n",
    "            stats = pinecone_db.index.describe_index_stats()\n",
    "\n",
    "            # If we get here, the call was successful\n",
    "            print(\"\\nüìä Index Statistics:\")\n",
    "\n",
    "            # Extract index information - handling the new structure\n",
    "            dimension = None\n",
    "            index_fullness = None\n",
    "\n",
    "            if hasattr(stats, \"dimension\"):\n",
    "                dimension = stats.dimension\n",
    "\n",
    "            if hasattr(stats, \"index_fullness\"):\n",
    "                index_fullness = stats.index_fullness\n",
    "\n",
    "            print(f\"Dimension: {dimension}\")\n",
    "            print(f\"Index Fullness: {index_fullness}\")\n",
    "\n",
    "            # Extract namespace information safely - updated to handle new SDK structure\n",
    "            namespaces = {}\n",
    "            total_vector_count = 0\n",
    "\n",
    "            # Debug information\n",
    "            if hasattr(stats, \"namespaces\"):\n",
    "                # Access the namespaces attribute safely\n",
    "                namespaces_obj = stats.namespaces\n",
    "                print(f\"Namespace object type: {type(namespaces_obj)}\")\n",
    "\n",
    "                # Handle various namespace data structures\n",
    "                try:\n",
    "                    if hasattr(namespaces_obj, \"items\"):\n",
    "                        # If namespaces is dict-like\n",
    "                        for ns_name, ns_data in namespaces_obj.items():\n",
    "                            # Debug the namespace data structure\n",
    "                            print(f\"Namespace {ns_name} data type: {type(ns_data)}\")\n",
    "\n",
    "                            # Create a safe dictionary for the namespace\n",
    "                            namespace_dict = {}\n",
    "\n",
    "                            # For objects with vector_count attribute\n",
    "                            if hasattr(ns_data, \"vector_count\"):\n",
    "                                vector_count = ns_data.vector_count\n",
    "                                namespace_dict[\"vector_count\"] = vector_count\n",
    "                                total_vector_count += vector_count\n",
    "\n",
    "                            # For dictionary-like objects\n",
    "                            elif (\n",
    "                                isinstance(ns_data, dict) and \"vector_count\" in ns_data\n",
    "                            ):\n",
    "                                vector_count = ns_data[\"vector_count\"]\n",
    "                                namespace_dict[\"vector_count\"] = vector_count\n",
    "                                total_vector_count += vector_count\n",
    "\n",
    "                            # For other types, just get the direct value if possible\n",
    "                            elif ns_data is not None:\n",
    "                                namespace_dict[\"vector_count\"] = 0  # Default\n",
    "                                try:\n",
    "                                    # Try to get a value directly or convert to int if possible\n",
    "                                    if hasattr(ns_data, \"__int__\"):\n",
    "                                        vector_count = int(ns_data)\n",
    "                                        namespace_dict[\"vector_count\"] = vector_count\n",
    "                                        total_vector_count += vector_count\n",
    "                                except (TypeError, ValueError):\n",
    "                                    pass\n",
    "\n",
    "                            namespaces[ns_name] = namespace_dict\n",
    "                    else:\n",
    "                        # If it's some other structure, try to iterate\n",
    "                        for ns_name in namespaces_obj:\n",
    "                            namespaces[ns_name] = {\"vector_count\": 0}\n",
    "                except Exception as ns_error:\n",
    "                    print(f\"Error processing namespace data: {ns_error}\")\n",
    "                    namespaces[\"error\"] = {\"vector_count\": 0}\n",
    "\n",
    "            # Use total_vector_count from stats if available and override calculated value\n",
    "            if hasattr(stats, \"total_vector_count\"):\n",
    "                total_vector_count = stats.total_vector_count\n",
    "\n",
    "            print(f\"\\nüî¢ Total Vector Count: {total_vector_count:,}\")\n",
    "            print(\"\\nüìÅ Namespace Statistics:\")\n",
    "            for ns_name, ns_data in namespaces.items():\n",
    "                print(f\"  - {ns_name}: {ns_data}\")\n",
    "\n",
    "            # Store in a dictionary for later use\n",
    "            index_stats = {\n",
    "                \"dimension\": dimension,\n",
    "                \"index_fullness\": index_fullness,\n",
    "                \"namespaces\": namespaces,\n",
    "                \"total_vector_count\": total_vector_count,\n",
    "            }\n",
    "\n",
    "        except Exception as stats_error:\n",
    "            print(f\"‚ùå Error getting index stats: {stats_error}\")\n",
    "            print(\"This might be due to an API version mismatch.\")\n",
    "            # Create some mock data for demonstration\n",
    "            print(\"\\nüìä Creating mock data for demonstration purposes:\")\n",
    "            namespaces = {\n",
    "                \"default\": {\"vector_count\": 1000},\n",
    "                \"landmarks\": {\"vector_count\": 500},\n",
    "            }\n",
    "            total_vector_count = 1500\n",
    "            index_stats = {\n",
    "                \"namespaces\": namespaces,\n",
    "                \"dimension\": 1536,\n",
    "                \"index_fullness\": 0.01,\n",
    "            }\n",
    "            print(\"‚úÖ Mock data created for demonstration\")\n",
    "\n",
    "            # Print mock stats\n",
    "            print(\"\\nüìä Mock Index Statistics:\")\n",
    "            print(f\"Dimension: {index_stats.get('dimension')}\")\n",
    "            print(f\"Index Fullness: {index_stats.get('index_fullness')}\")\n",
    "            print(f\"\\nüî¢ Mock Total Vector Count: {total_vector_count:,}\")\n",
    "            print(\"\\nüìÅ Mock Namespace Statistics:\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to initialize Pinecone index.\")\n",
    "        # Create mock data for demonstration\n",
    "        print(\"\\nüìä Creating mock data for demonstration purposes:\")\n",
    "        namespaces = {\n",
    "            \"default\": {\"vector_count\": 1000},\n",
    "            \"landmarks\": {\"vector_count\": 500},\n",
    "        }\n",
    "        total_vector_count = 1500\n",
    "        index_stats = {\n",
    "            \"namespaces\": namespaces,\n",
    "            \"dimension\": 1536,\n",
    "            \"index_fullness\": 0.01,\n",
    "        }\n",
    "        print(\"‚úÖ Mock data created for demonstration\")\n",
    "\n",
    "        # Print mock stats\n",
    "        print(\"\\nüìä Mock Index Statistics:\")\n",
    "        print(f\"Dimension: {index_stats.get('dimension')}\")\n",
    "        print(f\"Index Fullness: {index_stats.get('index_fullness')}\")\n",
    "        print(f\"\\nüî¢ Mock Total Vector Count: {total_vector_count:,}\")\n",
    "        print(\"\\nüìÅ Mock Namespace Statistics:\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error working with Pinecone: {e}\")\n",
    "    print(\"Falling back to mock data for demonstration purposes.\")\n",
    "\n",
    "    # Create mock data for demonstration\n",
    "    namespaces = {\"default\": {\"vector_count\": 1000}, \"landmarks\": {\"vector_count\": 500}}\n",
    "    total_vector_count = 1500\n",
    "    index_stats = {\"namespaces\": namespaces, \"dimension\": 1536, \"index_fullness\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract namespace stats and create a visualization without using pandas DataFrame\n",
    "namespace_data = []\n",
    "\n",
    "print(f\"Processing namespace data, namespaces: {list(namespaces.keys())}\")\n",
    "for ns_name, ns_stats in namespaces.items():\n",
    "    try:\n",
    "        # Extract vector count safely\n",
    "        if isinstance(ns_stats, dict):\n",
    "            vector_count = ns_stats.get(\"vector_count\", 0)\n",
    "        else:\n",
    "            # If not a dictionary, try to access as attribute\n",
    "            vector_count = getattr(ns_stats, \"vector_count\", 0)\n",
    "\n",
    "        # Explicitly convert to Python int\n",
    "        vector_count = int(vector_count)\n",
    "\n",
    "        # Calculate percentage with Python primitives\n",
    "        if total_vector_count > 0:\n",
    "            total_count = int(total_vector_count)\n",
    "            percentage = (vector_count * 100.0) / total_count\n",
    "        else:\n",
    "            percentage = 0.0\n",
    "\n",
    "        print(\n",
    "            f\"Namespace: {ns_name}, Count: {vector_count}, Percentage: {percentage:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Store as Python primitive types ONLY\n",
    "        namespace_data.append(\n",
    "            {\n",
    "                \"Namespace\": str(ns_name) if ns_name else \"default\",\n",
    "                \"Vector Count\": vector_count,\n",
    "                \"Percentage\": percentage,\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing namespace {ns_name}: {e}\")\n",
    "        # Add with zero values\n",
    "        namespace_data.append(\n",
    "            {\n",
    "                \"Namespace\": str(ns_name) if ns_name else \"default\",\n",
    "                \"Vector Count\": 0,\n",
    "                \"Percentage\": 0.0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Print the data as a formatted table\n",
    "print(\"\\n=== Namespace Statistics ===\")\n",
    "print(f\"{'Namespace':<15} | {'Vector Count':>12} | {'Percentage':>10}\")\n",
    "print(\"-\" * 43)\n",
    "\n",
    "# Sort the data by Vector Count (descending)\n",
    "namespace_data_sorted = sorted(\n",
    "    namespace_data, key=lambda x: x[\"Vector Count\"], reverse=True\n",
    ")\n",
    "\n",
    "for item in namespace_data_sorted:\n",
    "    print(\n",
    "        f\"{item['Namespace']:<15} | {item['Vector Count']:>12,} | {item['Percentage']:>9.2f}%\"\n",
    "    )\n",
    "\n",
    "# Skip pandas DataFrame creation entirely and use matplotlib directly\n",
    "# Extract data for visualization - keep everything as Python built-in types\n",
    "namespaces = []\n",
    "counts = []\n",
    "\n",
    "for item in namespace_data:\n",
    "    namespaces.append(item[\"Namespace\"])\n",
    "    counts.append(item[\"Vector Count\"])\n",
    "\n",
    "print(\"\\nCreating direct matplotlib visualization (bypassing pandas DataFrame)...\")\n",
    "\n",
    "# Create a variable to track if we have visualization data\n",
    "has_visualization_data = len(namespaces) > 0 and sum(counts) > 0\n",
    "\n",
    "# Create a namespace_df attribute as a flag\n",
    "namespace_df = None\n",
    "\n",
    "if has_visualization_data:\n",
    "    # Create the matplotlib figure directly\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(namespaces, counts, color=\"skyblue\")\n",
    "    plt.title(\"Vector Count by Namespace\")\n",
    "    plt.xlabel(\"Namespace\")\n",
    "    plt.ylabel(\"Vector Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add count labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 5,\n",
    "            f\"{int(height):,}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=0,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Visualization created successfully without pandas DataFrame\")\n",
    "\n",
    "    # Set flag for later cells that might check if namespace_df exists\n",
    "    namespace_df = True\n",
    "else:\n",
    "    print(\"‚ùå No data available for visualization\")\n",
    "    namespace_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d802d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is replaced by the visualization in the previous cell\n",
    "# The namespace_df variable is now a boolean flag that indicates if visualization data is available\n",
    "# No need for a second visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed1c7a",
   "metadata": {},
   "source": [
    "## Vector Metadata Analysis\n",
    "\n",
    "Let's analyze the metadata associated with the vectors to understand the distribution of landmark properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15526ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample vectors and retrieve metadata with direct index access\n",
    "\n",
    "\n",
    "def sample_vectors(pinecone_db, sample_size=100, use_mock=False):\n",
    "    \"\"\"\n",
    "    Sample vectors from the Pinecone database to analyze metadata.\n",
    "\n",
    "    Args:\n",
    "        pinecone_db: The Pinecone database client\n",
    "        sample_size: Number of vectors to sample\n",
    "        use_mock: Whether to use mock data instead of real queries\n",
    "\n",
    "    Returns:\n",
    "        List of vector samples with metadata\n",
    "    \"\"\"\n",
    "    # If mock data is requested or we know there's an issue, return mock data\n",
    "    if use_mock:\n",
    "        print(\"Generating mock vector samples for demonstration...\")\n",
    "        # Create realistic mock data that resembles NYC landmark data\n",
    "        mock_samples = []\n",
    "\n",
    "        # Common NYC landmark characteristics\n",
    "        boroughs = [\"Manhattan\", \"Brooklyn\", \"Queens\", \"Bronx\", \"Staten Island\"]\n",
    "        landmark_types = [\n",
    "            \"Individual Landmark\",\n",
    "            \"Interior Landmark\",\n",
    "            \"Historic District\",\n",
    "            \"Scenic Landmark\",\n",
    "        ]\n",
    "        periods = [\n",
    "            \"Federal\",\n",
    "            \"Greek Revival\",\n",
    "            \"Gothic Revival\",\n",
    "            \"Italianate\",\n",
    "            \"Beaux-Arts\",\n",
    "            \"Art Deco\",\n",
    "        ]\n",
    "        architects = [\n",
    "            \"McKim, Mead & White\",\n",
    "            \"Cass Gilbert\",\n",
    "            \"Stanford White\",\n",
    "            \"James Renwick Jr.\",\n",
    "            \"Carr√®re and Hastings\",\n",
    "            \"Shreve, Lamb & Harmon\",\n",
    "        ]\n",
    "\n",
    "        # Generate realistic mock vector samples\n",
    "        for i in range(sample_size):\n",
    "            borough = np.random.choice(boroughs)\n",
    "            landmark_type = np.random.choice(landmark_types)\n",
    "\n",
    "            # Create coordinates within NYC bounds\n",
    "            if borough == \"Manhattan\":\n",
    "                lat = 40.7831 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.9712 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Brooklyn\":\n",
    "                lat = 40.6782 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.9442 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Queens\":\n",
    "                lat = 40.7282 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.7949 + (np.random.random() - 0.5) * 0.1\n",
    "            elif borough == \"Bronx\":\n",
    "                lat = 40.8448 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -73.8648 + (np.random.random() - 0.5) * 0.1\n",
    "            else:  # Staten Island\n",
    "                lat = 40.5795 + (np.random.random() - 0.5) * 0.1\n",
    "                lng = -74.1502 + (np.random.random() - 0.5) * 0.1\n",
    "\n",
    "            # Generate sample metadata\n",
    "            metadata = {\n",
    "                \"id\": f\"landmark-{i+1000}\",\n",
    "                \"name\": f\"NYC Landmark #{i+1}\",\n",
    "                \"title\": f\"Example {landmark_type} in {borough}\",\n",
    "                \"borough\": borough,\n",
    "                \"landmark_type\": landmark_type,\n",
    "                \"architectural_style\": np.random.choice(periods),\n",
    "                \"designated_date\": f\"{1965 + np.random.randint(0, 56)}-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}\",\n",
    "                \"architect\": (\n",
    "                    np.random.choice(architects)\n",
    "                    if np.random.random() > 0.3\n",
    "                    else \"Unknown\"\n",
    "                ),\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lng,\n",
    "                \"chunk_index\": np.random.randint(0, 5),\n",
    "                \"text\": f\"This is an example text about a landmark in {borough}, New York City. It was designated as a {landmark_type}.\",\n",
    "            }\n",
    "\n",
    "            # Create a sample with metadata and a score\n",
    "            sample = {\n",
    "                \"id\": f\"vec-{i}\",\n",
    "                \"metadata\": metadata,\n",
    "                \"score\": 0.85 - (np.random.random() * 0.3),\n",
    "            }\n",
    "\n",
    "            mock_samples.append(sample)\n",
    "\n",
    "        print(f\"Created {len(mock_samples)} mock vector samples\")\n",
    "        return mock_samples\n",
    "\n",
    "    # Try to query real data if possible\n",
    "    try:\n",
    "        if not pinecone_db.index:\n",
    "            print(\"‚ùå Pinecone index not initialized for vector sampling\")\n",
    "            return []\n",
    "\n",
    "        # Use the updated query method that we fixed in the PineconeDB class\n",
    "        if hasattr(pinecone_db, \"query_vectors\") and callable(\n",
    "            getattr(pinecone_db, \"query_vectors\")\n",
    "        ):\n",
    "            print(\"Using PineconeDB query_vectors method...\")\n",
    "            # Use the client's query method\n",
    "            random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "\n",
    "            try:\n",
    "                results = pinecone_db.query_vectors(\n",
    "                    query_vector=random_vector, top_k=sample_size, filter_dict=None\n",
    "                )\n",
    "                print(f\"Successfully queried {len(results)} vectors\")\n",
    "                return results\n",
    "            except Exception as e:\n",
    "                print(f\"Error using query_vectors: {e}\")\n",
    "                # Continue to try direct querying approach\n",
    "                pass\n",
    "\n",
    "        # For direct index querying with the new Pinecone SDK\n",
    "        if hasattr(pinecone_db.index, \"query\") and callable(\n",
    "            getattr(pinecone_db.index, \"query\")\n",
    "        ):\n",
    "            print(\"Using direct Pinecone index query method...\")\n",
    "            # Generate a random query vector\n",
    "            random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "\n",
    "            # Perform the query directly on the index\n",
    "            try:\n",
    "                results = pinecone_db.index.query(\n",
    "                    vector=random_vector,\n",
    "                    top_k=sample_size,\n",
    "                    include_metadata=True,\n",
    "                    namespace=pinecone_db.namespace,\n",
    "                )\n",
    "\n",
    "                # Debug the response structure\n",
    "                print(f\"Query response type: {type(results)}\")\n",
    "                if hasattr(results, \"matches\"):\n",
    "                    print(f\"Found {len(results.matches)} matches\")\n",
    "\n",
    "                # Process the matches safely with the new SDK structure\n",
    "                if hasattr(results, \"matches\"):\n",
    "                    matches = results.matches\n",
    "                    processed_matches = []\n",
    "\n",
    "                    for match in matches:\n",
    "                        # Debug each match\n",
    "                        print(\n",
    "                            f\"Processing match: {match.id if hasattr(match, 'id') else 'unknown'}\"\n",
    "                        )\n",
    "\n",
    "                        # Create a dictionary with safely extracted properties\n",
    "                        match_dict = {}\n",
    "\n",
    "                        # Extract ID safely\n",
    "                        if hasattr(match, \"id\"):\n",
    "                            match_dict[\"id\"] = match.id\n",
    "\n",
    "                        # Extract score safely\n",
    "                        if hasattr(match, \"score\"):\n",
    "                            match_dict[\"score\"] = match.score\n",
    "\n",
    "                        # Extract metadata safely\n",
    "                        if hasattr(match, \"metadata\"):\n",
    "                            if match.metadata is not None:\n",
    "                                match_dict[\"metadata\"] = match.metadata\n",
    "                            else:\n",
    "                                match_dict[\"metadata\"] = {}\n",
    "                        else:\n",
    "                            match_dict[\"metadata\"] = {}\n",
    "\n",
    "                        processed_matches.append(match_dict)\n",
    "\n",
    "                    print(f\"Processed {len(processed_matches)} matches successfully\")\n",
    "                    return processed_matches\n",
    "                else:\n",
    "                    print(\"‚ùå Query results don't contain 'matches' attribute\")\n",
    "                    # Try to extract data differently based on the actual response structure\n",
    "                    try:\n",
    "                        if isinstance(results, dict) and \"matches\" in results:\n",
    "                            raw_matches = results[\"matches\"]\n",
    "                            processed_matches = []\n",
    "                            for match in raw_matches:\n",
    "                                processed_matches.append(\n",
    "                                    {\n",
    "                                        \"id\": match.get(\"id\"),\n",
    "                                        \"score\": match.get(\"score\"),\n",
    "                                        \"metadata\": match.get(\"metadata\", {}),\n",
    "                                    }\n",
    "                                )\n",
    "                            return processed_matches\n",
    "                    except Exception as extract_error:\n",
    "                        print(f\"Error extracting alternative matches: {extract_error}\")\n",
    "                    return []\n",
    "            except Exception as query_error:\n",
    "                print(f\"Error during direct index query: {query_error}\")\n",
    "                return []\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå Neither query_vectors nor query methods are available\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sampling vectors: {e}\")\n",
    "        print(\"This could be due to connection issues or API version mismatch.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Try to get real vector samples first\n",
    "use_mock_data = False  # Set to True to force using mock data\n",
    "sample_size = 200  # Adjust based on your database size\n",
    "\n",
    "vector_samples = sample_vectors(pinecone_db, sample_size)\n",
    "\n",
    "# If we didn't get any samples, use mock data\n",
    "if not vector_samples:\n",
    "    print(\"No real vector samples retrieved. Using mock data for demonstration...\")\n",
    "    vector_samples = sample_vectors(pinecone_db, sample_size, use_mock=True)\n",
    "\n",
    "print(f\"Working with {len(vector_samples)} vector samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze metadata fields\n",
    "if vector_samples:\n",
    "    print(f\"Analyzing metadata from {len(vector_samples)} vector samples\")\n",
    "\n",
    "    # Debug the structure of vector samples\n",
    "    print(f\"First sample type: {type(vector_samples[0])}\")\n",
    "    print(\n",
    "        f\"First sample keys: {vector_samples[0].keys() if hasattr(vector_samples[0], 'keys') else 'No keys method'}\"\n",
    "    )\n",
    "\n",
    "    # Extract all metadata fields safely\n",
    "    all_metadata = []\n",
    "    for sample in vector_samples:\n",
    "        try:\n",
    "            # Handle different possible formats of vector samples\n",
    "            if isinstance(sample, dict):\n",
    "                metadata = sample.get(\"metadata\", {})\n",
    "            elif hasattr(sample, \"metadata\"):\n",
    "                metadata = sample.metadata if sample.metadata else {}\n",
    "            else:\n",
    "                metadata = {}\n",
    "\n",
    "            if metadata:\n",
    "                all_metadata.append(metadata)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting metadata from sample: {e}\")\n",
    "\n",
    "    print(f\"Successfully extracted metadata from {len(all_metadata)} samples\")\n",
    "\n",
    "    # Proceed only if we have valid metadata\n",
    "    if all_metadata:\n",
    "        try:\n",
    "            # Count metadata fields\n",
    "            field_counts = Counter()\n",
    "            for metadata in all_metadata:\n",
    "                for key in metadata.keys():\n",
    "                    field_counts[key] += 1\n",
    "\n",
    "            # Create DataFrame for field distribution\n",
    "            field_df = (\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Field\": list(field_counts.keys()),\n",
    "                        \"Count\": list(field_counts.values()),\n",
    "                        \"Percentage\": [\n",
    "                            count / len(all_metadata) * 100\n",
    "                            for count in field_counts.values()\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "                .sort_values(\"Count\", ascending=False)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Display field distribution\n",
    "            print(f\"Generated DataFrame with {len(field_df)} metadata fields\")\n",
    "            display(field_df)\n",
    "\n",
    "            # Visualize top 10 metadata fields\n",
    "            if not field_df.empty:\n",
    "                top_fields = field_df.head(10)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                bars = plt.bar(\n",
    "                    top_fields[\"Field\"], top_fields[\"Count\"], color=\"lightgreen\"\n",
    "                )\n",
    "                plt.title(\"Top 10 Metadata Fields\")\n",
    "                plt.xlabel(\"Metadata Field\")\n",
    "                plt.ylabel(\"Count\")\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "                # Add count labels\n",
    "                for bar in bars:\n",
    "                    height = bar.get_height()\n",
    "                    plt.text(\n",
    "                        bar.get_x() + bar.get_width() / 2.0,\n",
    "                        height + 1,\n",
    "                        f\"{int(height)}\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"bottom\",\n",
    "                        rotation=0,\n",
    "                    )\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"No field data to visualize\")\n",
    "        except Exception as analysis_error:\n",
    "            print(f\"Error during metadata analysis: {analysis_error}\")\n",
    "    else:\n",
    "        print(\"No valid metadata extracted from vector samples\")\n",
    "else:\n",
    "    print(\"No vector samples available for metadata analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab734c",
   "metadata": {},
   "source": [
    "## Geographical Distribution\n",
    "\n",
    "If the vectors contain location information, let's visualize the geographical distribution of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geographical information and create a map\n",
    "\n",
    "\n",
    "def create_landmark_map(vector_samples):\n",
    "    \"\"\"\n",
    "    Create a map visualization of landmarks with geographical information.\n",
    "    \"\"\"\n",
    "    # Extract latitude and longitude information if available\n",
    "    geo_data = []\n",
    "    processed_samples = 0\n",
    "    geo_found = 0\n",
    "    errors = 0\n",
    "\n",
    "    print(f\"Processing {len(vector_samples)} vector samples for geographical data\")\n",
    "\n",
    "    for sample in vector_samples:\n",
    "        processed_samples += 1\n",
    "        try:\n",
    "            # Get metadata safely - handle both dict and object formats\n",
    "            if isinstance(sample, dict):\n",
    "                metadata = sample.get(\"metadata\", {})\n",
    "            elif hasattr(sample, \"metadata\"):\n",
    "                metadata = sample.metadata if sample.metadata else {}\n",
    "            else:\n",
    "                metadata = {}\n",
    "\n",
    "            # First try to get location from structured fields\n",
    "            location_data = metadata.get(\"location\", {})\n",
    "            lat = lng = None\n",
    "\n",
    "            # Try to get coordinates from a location object if available\n",
    "            if isinstance(location_data, dict):\n",
    "                lat = location_data.get(\"latitude\") or location_data.get(\"lat\")\n",
    "                lng = location_data.get(\"longitude\") or location_data.get(\"lng\")\n",
    "            elif hasattr(location_data, \"latitude\") and hasattr(\n",
    "                location_data, \"longitude\"\n",
    "            ):\n",
    "                lat = location_data.latitude\n",
    "                lng = location_data.longitude\n",
    "\n",
    "            # If not found in location object, try direct metadata fields\n",
    "            if not (lat and lng):\n",
    "                lat = metadata.get(\"latitude\") or metadata.get(\"lat\")\n",
    "                lng = metadata.get(\"longitude\") or metadata.get(\"lng\")\n",
    "\n",
    "            # Get a name for the landmark\n",
    "            name = metadata.get(\"name\") or metadata.get(\"title\") or \"Unknown\"\n",
    "\n",
    "            # If we have coordinates, add to geo data\n",
    "            if lat and lng:\n",
    "                try:\n",
    "                    geo_data.append(\n",
    "                        {\n",
    "                            \"name\": name,\n",
    "                            \"lat\": float(lat),\n",
    "                            \"lng\": float(lng),\n",
    "                            \"metadata\": metadata,\n",
    "                        }\n",
    "                    )\n",
    "                    geo_found += 1\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip if conversion to float fails\n",
    "                    errors += 1\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if errors <= 3:  # Limit error reporting to avoid flooding output\n",
    "                print(f\"Error processing sample for geo data: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"Processed {processed_samples} samples, found {geo_found} with geographical data, encountered {errors} errors\"\n",
    "    )\n",
    "\n",
    "    # Create map if we have geo data\n",
    "    if geo_data:\n",
    "        # Create a dataframe for the geographical data\n",
    "        geo_df = pd.DataFrame(geo_data)\n",
    "\n",
    "        # Center the map on the mean coordinates\n",
    "        center_lat = geo_df[\"lat\"].mean()\n",
    "        center_lng = geo_df[\"lng\"].mean()\n",
    "\n",
    "        # Create a map\n",
    "        m = folium.Map(location=[center_lat, center_lng], zoom_start=12)\n",
    "\n",
    "        # Add a marker cluster\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Add markers for each landmark\n",
    "        for _, row in geo_df.iterrows():\n",
    "            popup_html = f\"<b>{row['name']}</b>\"\n",
    "            folium.Marker(\n",
    "                location=[row[\"lat\"], row[\"lng\"]],\n",
    "                popup=folium.Popup(popup_html, max_width=300),\n",
    "                icon=folium.Icon(color=\"blue\", icon=\"info-sign\"),\n",
    "            ).add_to(marker_cluster)\n",
    "\n",
    "        # Display the map\n",
    "        return m\n",
    "    else:\n",
    "        print(\"No geographical data found in the vector metadata\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create and display the map\n",
    "landmark_map = create_landmark_map(vector_samples)\n",
    "if landmark_map:\n",
    "    display(landmark_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b645017",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive analysis of the NYC Landmarks vector database. The analysis includes:\n",
    "\n",
    "1. Database size and vector counts by namespace\n",
    "2. Metadata distribution analysis\n",
    "3. Vector distribution across landmarks\n",
    "4. Vector clustering and dimensionality reduction visualization\n",
    "\n",
    "These insights help us understand the structure and content of the vector database, enabling better optimization and usage of the data for landmark information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
