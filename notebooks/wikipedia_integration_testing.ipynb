{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Landmarks Wikipedia Integration Testing\n",
    "\n",
    "This notebook tests the integration of Wikipedia articles for NYC landmarks into the vector database. It demonstrates the process of:\n",
    "\n",
    "1. Fetching landmark information from the CoreDataStore API\n",
    "2. Retrieving associated Wikipedia articles\n",
    "3. Processing article content (fetching, cleaning, chunking)\n",
    "4. Generating embeddings for article chunks\n",
    "5. Storing embeddings in Pinecone vector database\n",
    "6. Querying the vector database to retrieve Wikipedia content\n",
    "7. Analyzing the distribution and quality of Wikipedia content in the vector database\n",
    "\n",
    "The notebook serves as both a testing tool and a demonstration of the Wikipedia integration capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment by creating a Python alias and installing any required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python alias for python3 and verify the Python installation\n",
    "!alias python=python3\n",
    "!python --version\n",
    "\n",
    "# Check if the project is installed correctly\n",
    "!pip list | grep nyc-landmarks-vector-db || echo \"Project not installed - install with 'pip install -e .'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the project in development mode if not already installed\n",
    "import os\n",
    "\n",
    "# Check if we're in the right directory structure\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "# Check for setup.py to confirm we're in the right place\n",
    "setup_py_path = os.path.join(project_root, \"setup.py\")\n",
    "if os.path.exists(setup_py_path):\n",
    "    print(\"setup.py found, installing project in development mode...\")\n",
    "    !cd {project_root} && pip install -e .\n",
    "else:\n",
    "    print(f\"setup.py not found at {setup_py_path}, please check directory structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking environment variables:\n",
      "✓ OPENAI_API_KEY is set\n",
      "✓ PINECONE_API_KEY is set\n",
      "✓ PINECONE_ENVIRONMENT is set\n",
      "✓ PINECONE_INDEX_NAME is set\n"
     ]
    }
   ],
   "source": [
    "# Check for environment variables required by the project\n",
    "import os\n",
    "\n",
    "# List of potential required environment variables\n",
    "env_vars = [\n",
    "    \"OPENAI_API_KEY\",  # For OpenAI embeddings\n",
    "    \"PINECONE_API_KEY\",  # For Pinecone vector DB\n",
    "    \"PINECONE_ENVIRONMENT\",  # Pinecone environment\n",
    "    \"PINECONE_INDEX_NAME\",  # Pinecone index name\n",
    "]\n",
    "\n",
    "print(\"Checking environment variables:\")\n",
    "for var in env_vars:\n",
    "    if var in os.environ:\n",
    "        print(f\"✓ {var} is set\")\n",
    "    else:\n",
    "        print(f\"✗ {var} is NOT set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to path to ensure imports work correctly\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from nyc_landmarks.db.db_client import get_db_client\n",
    "from nyc_landmarks.db.wikipedia_fetcher import WikipediaFetcher\n",
    "from nyc_landmarks.embeddings.generator import EmbeddingGenerator\n",
    "from nyc_landmarks.models.wikipedia_models import (\n",
    "    WikipediaArticleModel,\n",
    ")\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Initialize the components\n",
    "db_client = get_db_client()  # Using db_client instead of api_client\n",
    "wiki_fetcher = WikipediaFetcher()\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "pinecone_db = PineconeDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring Landmark Data\n",
    "\n",
    "Let's start by fetching some landmarks from the CoreDataStore API and explore the data structure using interactive pagination widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting total landmark record count...\n",
      "Total landmark records: 1765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b31698915640c19334a119a2a4cc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Page size:', layout=Layout(width='200px'), options=(10, 20…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the total record count for pagination\n",
    "print(\"Getting total landmark record count...\")\n",
    "total_records = db_client.get_total_record_count()\n",
    "print(f\"Total landmark records: {total_records}\")\n",
    "\n",
    "# Create interactive widgets for landmark data pagination\n",
    "page_size_options = [10, 20, 50, 100]\n",
    "page_size_dropdown = widgets.Dropdown(\n",
    "    options=page_size_options,\n",
    "    value=10,\n",
    "    description='Page size:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Calculate max page number based on total records and page size\n",
    "def get_max_page(page_size):\n",
    "    return math.ceil(total_records / page_size)\n",
    "\n",
    "# Create the page number input with validation\n",
    "page_number = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=get_max_page(page_size_dropdown.value),\n",
    "    step=1,\n",
    "    description='Page:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "# Navigation buttons\n",
    "prev_button = widgets.Button(\n",
    "    description='Previous',\n",
    "    disabled=True,  # Disabled initially since we start at page 1\n",
    "    button_style='',\n",
    "    tooltip='Go to previous page',\n",
    "    icon='arrow-left'\n",
    ")\n",
    "\n",
    "next_button = widgets.Button(\n",
    "    description='Next',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Go to next page',\n",
    "    icon='arrow-right'\n",
    ")\n",
    "\n",
    "# Status label showing page info\n",
    "status_label = widgets.Label(\n",
    "    value=f\"Page 1 of {get_max_page(page_size_dropdown.value)} (Records: {total_records})\"\n",
    ")\n",
    "\n",
    "# Output area for the dataframe\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Function to fetch and display landmarks\n",
    "def fetch_and_display_landmarks(page, page_size):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(f\"Fetching page {page} with {page_size} records per page...\")\n",
    "        try:\n",
    "            # Fetch the data from the API\n",
    "            response = db_client.get_lpc_reports(page=page, limit=page_size)\n",
    "\n",
    "            # Check if we got results\n",
    "            if not response.results:\n",
    "                print(f\"No landmarks found on page {page}\")\n",
    "                return None\n",
    "\n",
    "            # Create a DataFrame for display\n",
    "            landmarks_df = pd.DataFrame([landmark.model_dump() for landmark in response.results])\n",
    "\n",
    "            # Calculate record range on current page\n",
    "            start_record = (page - 1) * page_size + 1\n",
    "            end_record = min(start_record + len(response.results) - 1, total_records)\n",
    "\n",
    "            print(f\"Showing records {start_record}-{end_record} of {total_records}\")\n",
    "            display(landmarks_df)\n",
    "\n",
    "            # Return the landmarks for potential further use\n",
    "            return response.results\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching landmarks: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Event handlers for widgets\n",
    "def on_page_change(change):\n",
    "    if change['name'] == 'value' and change['new'] != change['old']:\n",
    "        page = change['new']\n",
    "        max_page = get_max_page(page_size_dropdown.value)\n",
    "\n",
    "        # Update button states\n",
    "        prev_button.disabled = (page <= 1)\n",
    "        next_button.disabled = (page >= max_page)\n",
    "\n",
    "        # Update status label\n",
    "        status_label.value = f\"Page {page} of {max_page} (Records: {total_records})\"\n",
    "\n",
    "        # Fetch and display landmarks\n",
    "        global landmarks\n",
    "        landmarks = fetch_and_display_landmarks(page, page_size_dropdown.value)\n",
    "\n",
    "def on_page_size_change(change):\n",
    "    if change['name'] == 'value' and change['new'] != change['old']:\n",
    "        # Recalculate max page\n",
    "        new_page_size = change['new']\n",
    "        new_max_page = get_max_page(new_page_size)\n",
    "\n",
    "        # Update page number widget range\n",
    "        page_number.max = new_max_page\n",
    "\n",
    "        # Adjust current page if needed\n",
    "        if page_number.value > new_max_page:\n",
    "            page_number.value = new_max_page\n",
    "\n",
    "        # Update status label\n",
    "        status_label.value = f\"Page {page_number.value} of {new_max_page} (Records: {total_records})\"\n",
    "\n",
    "        # Refetch with new page size\n",
    "        global landmarks\n",
    "        landmarks = fetch_and_display_landmarks(page_number.value, new_page_size)\n",
    "\n",
    "        # Update button states\n",
    "        prev_button.disabled = (page_number.value <= 1)\n",
    "        next_button.disabled = (page_number.value >= new_max_page)\n",
    "\n",
    "def on_prev_button_click(b):\n",
    "    if page_number.value > 1:\n",
    "        page_number.value -= 1\n",
    "\n",
    "def on_next_button_click(b):\n",
    "    if page_number.value < get_max_page(page_size_dropdown.value):\n",
    "        page_number.value += 1\n",
    "\n",
    "# Register event handlers\n",
    "page_number.observe(on_page_change, names='value')\n",
    "page_size_dropdown.observe(on_page_size_change, names='value')\n",
    "prev_button.on_click(on_prev_button_click)\n",
    "next_button.on_click(on_next_button_click)\n",
    "\n",
    "# Layout the widgets\n",
    "controls = widgets.HBox([page_size_dropdown, page_number, prev_button, next_button])\n",
    "dashboard = widgets.VBox([controls, status_label, output_area])\n",
    "\n",
    "# Display the widgets\n",
    "display(dashboard)\n",
    "\n",
    "# Initial display\n",
    "landmarks = fetch_and_display_landmarks(page_number.value, page_size_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving Wikipedia Articles for Landmarks\n",
    "\n",
    "Now let's check which landmarks have associated Wikipedia articles and examine their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and display Wikipedia articles for a landmark\n",
    "\n",
    "def check_wikipedia_articles(landmark_id: str) -> List[WikipediaArticleModel]:\n",
    "    \"\"\"Check if a landmark has associated Wikipedia articles.\n",
    "\n",
    "    Args:\n",
    "        landmark_id: ID of the landmark to check\n",
    "\n",
    "    Returns:\n",
    "        List of WikipediaArticleModel objects\n",
    "    \"\"\"\n",
    "    articles = db_client.get_wikipedia_articles(landmark_id)\n",
    "    print(f\"Found {len(articles)} Wikipedia articles for landmark: {landmark_id}\")\n",
    "    return articles\n",
    "\n",
    "# Check Wikipedia articles for each landmark\n",
    "landmark_articles = {}\n",
    "for landmark in landmarks:\n",
    "    landmark_id = landmark.lpNumber\n",
    "    name = landmark.name\n",
    "    print(f\"Checking {name} ({landmark_id})...\")\n",
    "    articles = check_wikipedia_articles(landmark_id)\n",
    "    if articles:\n",
    "        landmark_articles[landmark_id] = articles\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(landmark_articles)} landmarks with Wikipedia articles out of {len(landmarks)} total\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Wikipedia articles we found\n",
    "if landmark_articles:\n",
    "    # Extract landmark ID, name, article title, and URL into a list of dictionaries\n",
    "    articles_data = []\n",
    "    for landmark_id, articles in landmark_articles.items():\n",
    "        landmark_name = next(\n",
    "            (l.name for l in landmarks if l.lpNumber == landmark_id), \"Unknown\"\n",
    "        )\n",
    "        for article in articles:\n",
    "            articles_data.append(\n",
    "                {\n",
    "                    \"landmark_id\": landmark_id,\n",
    "                    \"landmark_name\": landmark_name,\n",
    "                    \"article_title\": article.title,\n",
    "                    \"article_url\": article.url,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create a DataFrame for easier viewing\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "    articles_df\n",
    "else:\n",
    "    print(\"No landmarks with Wikipedia articles found in the sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fetching and Processing Wikipedia Content\n",
    "\n",
    "Now let's fetch the actual content from a Wikipedia article and process it for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. End-to-End Wikipedia Processing Test\n",
    "\n",
    "This section demonstrates a complete end-to-end workflow for processing and querying Wikipedia articles for NYC landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete end-to-end test for a single landmark\n",
    "def process_landmark_wikipedia_articles(landmark_id):\n",
    "    \"\"\"Process Wikipedia articles for a single landmark.\n",
    "\n",
    "    Args:\n",
    "        landmark_id: ID of the landmark to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with processing results\n",
    "    \"\"\"\n",
    "    print(f\"Processing Wikipedia articles for landmark {landmark_id}\")\n",
    "\n",
    "    # Step 1: Get Wikipedia articles for the landmark\n",
    "    articles = db_client.get_wikipedia_articles(landmark_id)\n",
    "    if not articles:\n",
    "        print(f\"No Wikipedia articles found for landmark {landmark_id}\")\n",
    "        return {\"success\": False, \"reason\": \"No Wikipedia articles found\"}\n",
    "\n",
    "    print(f\"Found {len(articles)} Wikipedia articles\")\n",
    "\n",
    "    # Step 2: Process each article\n",
    "    all_chunks = []\n",
    "    all_vectors = []\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        print(f\"\\nProcessing article {i+1}/{len(articles)}: {article.title}\")\n",
    "\n",
    "        # Fetch article content\n",
    "        content = wiki_fetcher.fetch_wikipedia_content(article.url)\n",
    "        if not content:\n",
    "            print(f\"Failed to fetch content for article: {article.title}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Successfully fetched article content ({len(content)} chars)\")\n",
    "\n",
    "        # Chunk the content\n",
    "        chunks = wiki_fetcher.chunk_wikipedia_text(\n",
    "            content, chunk_size=1000, chunk_overlap=200\n",
    "        )\n",
    "\n",
    "        print(f\"Split article into {len(chunks)} chunks\")\n",
    "\n",
    "        # Add article metadata to chunks\n",
    "        for chunk in chunks:\n",
    "            chunk[\"metadata\"][\"article_title\"] = article.title\n",
    "            chunk[\"metadata\"][\"article_url\"] = article.url\n",
    "            chunk[\"metadata\"][\"source_type\"] = \"wikipedia\"\n",
    "            chunk[\"metadata\"][\"landmark_id\"] = landmark_id\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    # Step 3: Generate embeddings (limit to first 5 chunks for testing)\n",
    "    test_chunks = all_chunks[:5] if len(all_chunks) > 5 else all_chunks\n",
    "    print(f\"\\nGenerating embeddings for {len(test_chunks)} chunks\")\n",
    "\n",
    "    chunks_with_embeddings = embedding_generator.process_chunks(test_chunks)\n",
    "    print(f\"Generated embeddings for {len(chunks_with_embeddings)} chunks\")\n",
    "\n",
    "    # Step 4: Store in Pinecone\n",
    "    print(\"\\nStoring embeddings in Pinecone...\")\n",
    "    vector_ids = pinecone_db.store_chunks(\n",
    "        chunks=chunks_with_embeddings,\n",
    "        id_prefix=f\"wiki-{landmark_id}-\",\n",
    "        landmark_id=landmark_id,\n",
    "        use_fixed_ids=True,\n",
    "        delete_existing=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Stored {len(vector_ids)} vectors in Pinecone\")\n",
    "\n",
    "    # Step 5: Query the vectors\n",
    "    landmark_name = next((l.name for l in landmarks if l.lpNumber == landmark_id), \"landmark\")\n",
    "    test_query = f\"Tell me about the history of {landmark_name}\"\n",
    "    print(f\"\\nTest query: '{test_query}'\")\n",
    "\n",
    "    query_embedding = embedding_generator.generate_embedding(test_query)\n",
    "    results = pinecone_db.query_vectors(\n",
    "        query_embedding,\n",
    "        top_k=3,\n",
    "        filter_dict={\"landmark_id\": landmark_id, \"source_type\": \"wikipedia\"}\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(results)} matching results\")\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"landmark_id\": landmark_id,\n",
    "        \"articles_processed\": len(articles),\n",
    "        \"chunks_generated\": len(all_chunks),\n",
    "        \"vectors_stored\": len(vector_ids),\n",
    "        \"query_results\": results\n",
    "    }\n",
    "\n",
    "# Test with a landmark that has Wikipedia articles\n",
    "if landmark_articles:\n",
    "    test_landmark_id = next(iter(landmark_articles.keys()))\n",
    "    print(f\"Running end-to-end test with landmark: {test_landmark_id}\")\n",
    "    result = process_landmark_wikipedia_articles(test_landmark_id)\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        print(\"\\nTest completed successfully!\")\n",
    "        print(f\"Articles processed: {result['articles_processed']}\")\n",
    "        print(f\"Chunks generated: {result['chunks_generated']}\")\n",
    "        print(f\"Vectors stored: {result['vectors_stored']}\")\n",
    "\n",
    "        # Display query results\n",
    "        print(\"\\nQuery results:\")\n",
    "        for i, match in enumerate(result[\"query_results\"]):\n",
    "            print(f\"\\nMatch {i+1} - Score: {match['score']:.4f}\")\n",
    "            print(f\"Article: {match['metadata'].get('article_title', 'Unknown')}\")\n",
    "            print(f\"Text: {match['metadata'].get('text', '')[:200]}...\")\n",
    "    else:\n",
    "        print(f\"Test failed: {result['reason']}\")\n",
    "else:\n",
    "    print(\"No landmarks with Wikipedia articles available for testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
