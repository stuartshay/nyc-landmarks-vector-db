{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Landmarks Wikipedia Integration Testing\n",
    "\n",
    "This notebook tests the integration of Wikipedia articles for NYC landmarks into the vector database. It demonstrates the process of:\n",
    "\n",
    "1. **Exploring Landmark Data**: Fetching landmark information from the CoreDataStore API with interactive pagination\n",
    "2. **Retrieving Wikipedia Articles**: Finding Wikipedia articles associated with NYC landmarks\n",
    "3. **Fetching and Processing Wikipedia Content**: Retrieving, cleaning, and chunking article content\n",
    "4. **End-to-End Wikipedia Processing Test**: Complete workflow demonstration for processing and querying Wikipedia articles\n",
    "5. **Analyzing Wikipedia Coverage**: Examining Wikipedia article distribution in the vector database, including statistics on coverage percentages, landmark representation, and interactive querying of Wikipedia content\n",
    "6. **Summary Report**: Review of Wikipedia integration status and recommendations\n",
    "\n",
    "The notebook provides both testing tools and interactive visualizations to demonstrate the Wikipedia integration capabilities for the NYC landmarks vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment by creating a Python alias and installing any required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python alias for python3 and verify the Python installation\n",
    "!alias python=python3\n",
    "!python --version\n",
    "\n",
    "# Check if the project is installed correctly\n",
    "!pip list | grep nyc-landmarks-vector-db || echo \"Project not installed - install with 'pip install -e .'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the project in development mode if not already installed\n",
    "import os\n",
    "\n",
    "# Check if we're in the right directory structure\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "# Check for setup.py to confirm we're in the right place\n",
    "setup_py_path = os.path.join(project_root, \"setup.py\")\n",
    "if os.path.exists(setup_py_path):\n",
    "    print(\"setup.py found, installing project in development mode...\")\n",
    "    !cd {project_root} && pip install -e .\n",
    "else:\n",
    "    print(f\"setup.py not found at {setup_py_path}, please check directory structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking environment variables:\n",
      "✓ OPENAI_API_KEY is set\n",
      "✓ PINECONE_API_KEY is set\n",
      "✓ PINECONE_ENVIRONMENT is set\n",
      "✓ PINECONE_INDEX_NAME is set\n"
     ]
    }
   ],
   "source": [
    "# Check for environment variables required by the project\n",
    "import os\n",
    "\n",
    "# List of potential required environment variables\n",
    "env_vars = [\n",
    "    \"OPENAI_API_KEY\",  # For OpenAI embeddings\n",
    "    \"PINECONE_API_KEY\",  # For Pinecone vector DB\n",
    "    \"PINECONE_ENVIRONMENT\",  # Pinecone environment\n",
    "    \"PINECONE_INDEX_NAME\",  # Pinecone index name\n",
    "]\n",
    "\n",
    "print(\"Checking environment variables:\")\n",
    "for var in env_vars:\n",
    "    if var in os.environ:\n",
    "        print(f\"✓ {var} is set\")\n",
    "    else:\n",
    "        print(f\"✗ {var} is NOT set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Add project root to path to ensure imports work correctly\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from nyc_landmarks.db.db_client import get_db_client\n",
    "from nyc_landmarks.db.wikipedia_fetcher import WikipediaFetcher\n",
    "from nyc_landmarks.embeddings.generator import EmbeddingGenerator\n",
    "from nyc_landmarks.models.wikipedia_models import (\n",
    "    WikipediaArticleModel,\n",
    ")\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Initialize the components\n",
    "db_client = get_db_client()  # Using db_client instead of api_client\n",
    "wiki_fetcher = WikipediaFetcher()\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "pinecone_db = PineconeDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring Landmark Data\n",
    "\n",
    "Let's start by fetching some landmarks from the CoreDataStore API and explore the data structure using interactive pagination widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3210770416f648d8ab4e766d77b468fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Page size:', layout=Layout(width='200px'), options=(10, 20…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 4 with 10 records per page...\n"
     ]
    }
   ],
   "source": [
    "# Global flags to prevent re-entrant calls\n",
    "global _is_updating_page, _button_click_in_progress\n",
    "_is_updating_page = False\n",
    "_button_click_in_progress = False\n",
    "\n",
    "# Declare widgets as global to ensure they persist across cell executions\n",
    "global page_size_dropdown, page_number, prev_button, next_button, status_label, output_area, dashboard, total_records, landmarks\n",
    "\n",
    "# Get the total record count for pagination (only once)\n",
    "if \"total_records\" not in globals():\n",
    "    print(\"Getting total landmark record count...\")\n",
    "    total_records = db_client.get_total_record_count()\n",
    "    print(f\"Total landmark records: {total_records}\")\n",
    "\n",
    "# Initialize widgets only once\n",
    "if (\n",
    "    \"dashboard\" not in globals() or dashboard is None\n",
    "):  # Check if dashboard is None as well\n",
    "    # Create interactive widgets for landmark data pagination\n",
    "    page_size_options = [10, 20, 50, 100]\n",
    "    page_size_dropdown = widgets.Dropdown(\n",
    "        options=page_size_options,\n",
    "        value=10,\n",
    "        description=\"Page size:\",\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width=\"200px\"),\n",
    "    )\n",
    "\n",
    "    # Calculate max page number based on total records and page size\n",
    "    def get_max_page(page_size):\n",
    "        return math.ceil(total_records / page_size)\n",
    "\n",
    "    # Create the page number input with validation\n",
    "    page_number = widgets.BoundedIntText(\n",
    "        value=1,\n",
    "        min=1,\n",
    "        max=get_max_page(page_size_dropdown.value),\n",
    "        step=1,\n",
    "        description=\"Page:\",\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width=\"150px\"),\n",
    "    )\n",
    "\n",
    "    # Navigation buttons\n",
    "    prev_button = widgets.Button(\n",
    "        description=\"Previous\",\n",
    "        disabled=True,  # Disabled initially since we start at page 1\n",
    "        button_style=\"\",\n",
    "        tooltip=\"Go to previous page\",\n",
    "        icon=\"arrow-left\",\n",
    "    )\n",
    "\n",
    "    next_button = widgets.Button(\n",
    "        description=\"Next\",\n",
    "        disabled=False,\n",
    "        button_style=\"\",\n",
    "        tooltip=\"Go to next page\",\n",
    "        icon=\"arrow-right\",\n",
    "    )\n",
    "\n",
    "    # Status label showing page info\n",
    "    status_label = widgets.Label(\n",
    "        value=f\"Page 1 of {get_max_page(page_size_dropdown.value)} (Records: {total_records})\"\n",
    "    )\n",
    "\n",
    "    # Output area for the dataframe\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Function to fetch and display landmarks\n",
    "    def fetch_and_display_landmarks(page, page_size):\n",
    "        # Clear any previous output\n",
    "        with output_area:\n",
    "            clear_output(wait=True)  # Clear previous output in the widget\n",
    "            print(f\"Fetching page {page} with {page_size} records per page...\")\n",
    "        try:\n",
    "            # Fetch the data from the API\n",
    "            response = db_client.get_lpc_reports(page=page, limit=page_size)\n",
    "\n",
    "            # Check if we got results\n",
    "            if not response.results:\n",
    "                with output_area:\n",
    "                    print(f\"No landmarks found on page {page}\")\n",
    "                return None\n",
    "\n",
    "            # Create a DataFrame for display\n",
    "            landmarks_df = pd.DataFrame(\n",
    "                [landmark.model_dump() for landmark in response.results]\n",
    "            )\n",
    "\n",
    "            # Calculate record range on current page\n",
    "            start_record = (page - 1) * page_size + 1\n",
    "            end_record = min(start_record + len(response.results) - 1, total_records)\n",
    "\n",
    "            with output_area:\n",
    "                print(f\"Showing records {start_record}-{end_record} of {total_records}\")\n",
    "                display(landmarks_df)\n",
    "\n",
    "            # Return the landmarks for potential further use\n",
    "            return response.results\n",
    "        except Exception as e:\n",
    "            with output_area:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Error fetching landmarks: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # Event handlers for widgets\n",
    "    def on_page_change(change):\n",
    "        global _is_updating_page\n",
    "        if _is_updating_page:\n",
    "            return\n",
    "\n",
    "        if change[\"name\"] == \"value\" and change[\"new\"] != change[\"old\"]:\n",
    "            _is_updating_page = True\n",
    "            try:\n",
    "                page = change[\"new\"]\n",
    "                max_page = get_max_page(page_size_dropdown.value)\n",
    "\n",
    "                # Update button states\n",
    "                prev_button.disabled = page <= 1\n",
    "                next_button.disabled = page >= max_page\n",
    "\n",
    "                # Update status label\n",
    "                status_label.value = (\n",
    "                    f\"Page {page} of {max_page} (Records: {total_records})\"\n",
    "                )\n",
    "\n",
    "                # Fetch and display landmarks\n",
    "                global landmarks\n",
    "                landmarks = fetch_and_display_landmarks(page, page_size_dropdown.value)\n",
    "            finally:\n",
    "                _is_updating_page = False\n",
    "\n",
    "    def on_page_size_change(change):\n",
    "        global _is_updating_page\n",
    "        if _is_updating_page:\n",
    "            return\n",
    "\n",
    "        if change[\"name\"] == \"value\" and change[\"new\"] != change[\"old\"]:\n",
    "            _is_updating_page = True\n",
    "            try:\n",
    "                # Recalculate max page\n",
    "                new_page_size = change[\"new\"]\n",
    "                new_max_page = get_max_page(new_page_size)\n",
    "\n",
    "                # Update page number widget range\n",
    "                page_number.max = new_max_page\n",
    "\n",
    "                # Adjust current page if needed\n",
    "                if page_number.value > new_max_page:\n",
    "                    page_number.value = new_max_page\n",
    "\n",
    "                # Update status label\n",
    "                status_label.value = f\"Page {page_number.value} of {new_max_page} (Records: {total_records})\"\n",
    "\n",
    "                # Refetch with new page size\n",
    "                global landmarks\n",
    "                landmarks = fetch_and_display_landmarks(\n",
    "                    page_number.value, new_page_size\n",
    "                )\n",
    "\n",
    "                # Update button states\n",
    "                prev_button.disabled = page_number.value <= 1\n",
    "                next_button.disabled = page_number.value >= new_max_page\n",
    "            finally:\n",
    "                _is_updating_page = False\n",
    "\n",
    "    def on_prev_button_click(b):\n",
    "        global _button_click_in_progress\n",
    "        if _button_click_in_progress:\n",
    "            return\n",
    "        _button_click_in_progress = True\n",
    "        try:\n",
    "            if page_number.value > 1:\n",
    "                page_number.value -= 1\n",
    "        finally:\n",
    "            _button_click_in_progress = False\n",
    "\n",
    "    def on_next_button_click(b):\n",
    "        global _button_click_in_progress\n",
    "        if _button_click_in_progress:\n",
    "            return\n",
    "        _button_click_in_progress = True\n",
    "        try:\n",
    "            if page_number.value < get_max_page(page_size_dropdown.value):\n",
    "                page_number.value += 1\n",
    "        finally:\n",
    "            _button_click_in_progress = False\n",
    "\n",
    "    # Register event handlers\n",
    "    # Remove all existing observers to prevent multiple calls if the cell is run multiple times\n",
    "    page_number.unobserve_all()\n",
    "    page_size_dropdown.unobserve_all()\n",
    "\n",
    "    page_number.observe(on_page_change, names=\"value\")\n",
    "    page_size_dropdown.observe(on_page_size_change, names=\"value\")\n",
    "    prev_button.on_click(on_prev_button_click)\n",
    "    next_button.on_click(on_next_button_click)\n",
    "\n",
    "    # For VS Code compatibility, create alternative non-widget UI\n",
    "    print(\"\\nVS Code Widget Alternative Controls:\")\n",
    "    print(\n",
    "        \"Use these functions to navigate instead of the widgets if they don't display properly:\"\n",
    "    )\n",
    "\n",
    "    def go_to_page(page_num):\n",
    "        \"\"\"Go to a specific page\"\"\"\n",
    "        if 1 <= page_num <= get_max_page(page_size_dropdown.value):\n",
    "            page_number.value = page_num\n",
    "            return fetch_and_display_landmarks(page_num, page_size_dropdown.value)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Page number must be between 1 and {get_max_page(page_size_dropdown.value)}\"\n",
    "            )\n",
    "\n",
    "    def next_page():\n",
    "        \"\"\"Go to next page\"\"\"\n",
    "        return go_to_page(page_number.value + 1)\n",
    "\n",
    "    def prev_page():\n",
    "        \"\"\"Go to previous page\"\"\"\n",
    "        return go_to_page(page_number.value - 1)\n",
    "\n",
    "    def change_page_size(size):\n",
    "        \"\"\"Change page size\"\"\"\n",
    "        if size in page_size_options:\n",
    "            page_size_dropdown.value = size\n",
    "            return fetch_and_display_landmarks(page_number.value, size)\n",
    "        else:\n",
    "            print(f\"Page size must be one of {page_size_options}\")\n",
    "\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"go_to_page(2)      # Go to page 2\")\n",
    "    print(\"next_page()        # Go to next page\")\n",
    "    print(\"prev_page()        # Go to previous page\")\n",
    "    print(\"change_page_size(20) # Change to 20 records per page\")\n",
    "\n",
    "    # Layout the widgets - if they display properly in your environment\n",
    "    # Put the pagination controls at the top\n",
    "    controls = widgets.HBox([page_size_dropdown, page_number, prev_button, next_button])\n",
    "    # Position status label and controls above the output area\n",
    "    dashboard = widgets.VBox(\n",
    "        [\n",
    "            controls,  # Pagination controls on top\n",
    "            status_label,  # Status label below controls\n",
    "            output_area,  # Output area at the bottom\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Always display the dashboard and fetch initial data when the cell is run\n",
    "try:\n",
    "    display(dashboard)\n",
    "    # Initial display\n",
    "    landmarks = fetch_and_display_landmarks(page_number.value, page_size_dropdown.value)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Note: Widget display error: {str(e)}. Use the alternative functions above instead.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving Wikipedia Articles for Landmarks\n",
    "\n",
    "This section explores which landmarks have associated Wikipedia articles and displays an interactive page-by-page view of the landmarks along with their Wikipedia articles. You can browse through landmarks and see which ones have Wikipedia content available for integration into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and display Wikipedia articles for a landmark\n",
    "\n",
    "\n",
    "def check_wikipedia_articles(landmark_id: str) -> List[WikipediaArticleModel]:\n",
    "    \"\"\"Check if a landmark has associated Wikipedia articles.\n",
    "\n",
    "    Args:\n",
    "        landmark_id: ID of the landmark to check\n",
    "\n",
    "    Returns:\n",
    "        List of WikipediaArticleModel objects\n",
    "    \"\"\"\n",
    "    articles = db_client.get_wikipedia_articles(landmark_id)\n",
    "    print(f\"Found {len(articles)} Wikipedia articles for landmark: {landmark_id}\")\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Check Wikipedia articles for each landmark\n",
    "landmark_articles = {}\n",
    "for landmark in landmarks:\n",
    "    landmark_id = landmark.lpNumber\n",
    "    name = landmark.name\n",
    "    print(f\"Checking {name} ({landmark_id})...\")\n",
    "    articles = check_wikipedia_articles(landmark_id)\n",
    "    if articles:\n",
    "        landmark_articles[landmark_id] = articles\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(landmark_articles)} landmarks with Wikipedia articles out of {len(landmarks)} total\"\n",
    ")\n",
    "\n",
    "# Transform landmark_articles into a list of dictionaries for analysis\n",
    "landmarks_data = [\n",
    "    {\"landmark_id\": landmark_id, \"article_count\": len(articles)}\n",
    "    for landmark_id, articles in landmark_articles.items()\n",
    "]\n",
    "\n",
    "\n",
    "# Function to analyze Wikipedia article coverage for landmarks\n",
    "\n",
    "\n",
    "def analyze_wikipedia_coverage(landmarks_data):\n",
    "    \"\"\"Analyze Wikipedia article coverage for a set of landmarks.\n",
    "\n",
    "    Args:\n",
    "        landmarks_data: List of landmark data including Wikipedia articles\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    total_landmarks = len(landmarks_data)\n",
    "    landmarks_with_articles = sum(1 for ld in landmarks_data if ld[\"article_count\"] > 0)\n",
    "    coverage_percentage = (\n",
    "        (landmarks_with_articles / total_landmarks * 100) if total_landmarks > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Count total articles\n",
    "    total_articles = sum(ld[\"article_count\"] for ld in landmarks_data)\n",
    "\n",
    "    # Calculate articles per landmark statistics\n",
    "    articles_per_landmark = [ld[\"article_count\"] for ld in landmarks_data]\n",
    "    max_articles = max(articles_per_landmark) if articles_per_landmark else 0\n",
    "    avg_articles = (\n",
    "        sum(articles_per_landmark) / landmarks_with_articles\n",
    "        if landmarks_with_articles > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"total_landmarks\": total_landmarks,\n",
    "        \"landmarks_with_articles\": landmarks_with_articles,\n",
    "        \"coverage_percentage\": coverage_percentage,\n",
    "        \"total_articles\": total_articles,\n",
    "        \"max_articles_per_landmark\": max_articles,\n",
    "        \"avg_articles_per_landmark\": avg_articles,\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze the current page of landmarks\n",
    "analysis = analyze_wikipedia_coverage(landmarks_data)\n",
    "\n",
    "# Display analysis results\n",
    "print(\"Wikipedia Coverage Analysis:\")\n",
    "print(f\"- Landmarks analyzed: {analysis['total_landmarks']}\")\n",
    "print(\n",
    "    f\"- Landmarks with Wikipedia articles: {analysis['landmarks_with_articles']} ({analysis['coverage_percentage']:.1f}%)\"\n",
    ")\n",
    "print(f\"- Total Wikipedia articles: {analysis['total_articles']}\")\n",
    "print(f\"- Max articles per landmark: {analysis['max_articles_per_landmark']}\")\n",
    "print(f\"- Average articles per landmark: {analysis['avg_articles_per_landmark']:.2f}\")\n",
    "\n",
    "# Transform landmark_articles into a list of dictionaries for sorting\n",
    "landmarks_with_articles = [\n",
    "    {\"landmark_id\": landmark_id, \"article_count\": len(articles)}\n",
    "    for landmark_id, articles in landmark_articles.items()\n",
    "]\n",
    "\n",
    "# Sort landmarks by article count\n",
    "sorted_landmarks = sorted(\n",
    "    landmarks_with_articles, key=lambda x: x[\"article_count\"], reverse=True\n",
    ")\n",
    "\n",
    "# Display top landmarks with the most articles\n",
    "print(\"\\nLandmarks with the most Wikipedia articles:\")\n",
    "for landmark in sorted_landmarks[:3]:  # Show top 3\n",
    "    print(\n",
    "        f\"Landmark ID: {landmark['landmark_id']}, Articles: {landmark['article_count']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of all Wikipedia articles found across all pages\n",
    "\n",
    "\n",
    "def get_all_wikipedia_articles():\n",
    "    \"\"\"Fetch Wikipedia articles for multiple pages of landmarks to get a larger sample\"\"\"\n",
    "    all_articles_data = []\n",
    "    max_pages_to_check = 3\n",
    "    page_size = 20\n",
    "\n",
    "    print(\n",
    "        f\"Fetching Wikipedia articles across {max_pages_to_check} pages (page size: {page_size})...\"\n",
    "    )\n",
    "\n",
    "    for page in range(1, max_pages_to_check + 1):\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        response = db_client.get_lpc_reports(page=page, limit=page_size)\n",
    "\n",
    "        if not response.results:\n",
    "            print(f\"No landmarks found on page {page}\")\n",
    "            break\n",
    "\n",
    "        landmarks_batch = response.results\n",
    "\n",
    "        for landmark in landmarks_batch:\n",
    "            landmark_id = landmark.lpNumber\n",
    "            landmark_name = landmark.name\n",
    "            articles = db_client.get_wikipedia_articles(landmark_id)\n",
    "\n",
    "            for article in articles:\n",
    "                all_articles_data.append(\n",
    "                    {\n",
    "                        \"landmark_id\": landmark_id,\n",
    "                        \"landmark_name\": landmark_name,\n",
    "                        \"article_title\": article.title,\n",
    "                        \"article_url\": article.url,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    print(f\"Found {len(all_articles_data)} Wikipedia articles\")\n",
    "    return all_articles_data\n",
    "\n",
    "\n",
    "# Get a sample of Wikipedia articles across multiple pages\n",
    "all_wiki_articles = get_all_wikipedia_articles()\n",
    "\n",
    "# Create a DataFrame\n",
    "if all_wiki_articles:\n",
    "    # Create a DataFrame for easier viewing\n",
    "    articles_df = pd.DataFrame(all_wiki_articles)\n",
    "\n",
    "    # Show summary statistics\n",
    "    landmark_count = len(set(articles_df[\"landmark_id\"]))\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"- Total Wikipedia articles found: {len(articles_df)}\")\n",
    "    print(f\"- Total landmarks with articles: {landmark_count}\")\n",
    "    print(f\"- Average articles per landmark: {len(articles_df) / landmark_count:.2f}\")\n",
    "\n",
    "    # Display the full DataFrame\n",
    "    display(articles_df)\n",
    "\n",
    "    # Display article title word cloud (if matplotlib and wordcloud are available)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        from wordcloud import WordCloud\n",
    "\n",
    "        # Join all article titles\n",
    "        all_titles = \" \".join(articles_df[\"article_title\"])\n",
    "\n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color=\"white\",\n",
    "            max_words=100,\n",
    "            contour_width=3,\n",
    "        ).generate(all_titles)\n",
    "\n",
    "        # Display the word cloud\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Word Cloud of Wikipedia Article Titles\")\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"\\nNote: Install wordcloud and matplotlib packages to generate a word cloud visualization\"\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(\"No Wikipedia articles found in the sampled landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive display for landmarks and their Wikipedia articles with pagination\n",
    "\n",
    "# Create widgets for pagination\n",
    "landmark_page_size_options = [5, 10, 20, 50]\n",
    "landmark_page_size_dropdown = widgets.Dropdown(\n",
    "    options=landmark_page_size_options,\n",
    "    value=5,\n",
    "    description=\"Items/page:\",\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"150px\"),\n",
    ")\n",
    "\n",
    "# Initialize landmarks data\n",
    "all_landmarks_with_articles = []\n",
    "\n",
    "# Function to fetch Wikipedia articles for all landmarks on the current page\n",
    "\n",
    "\n",
    "def fetch_landmark_articles(page, page_size):\n",
    "    global all_landmarks_with_articles\n",
    "    all_landmarks_with_articles = []\n",
    "\n",
    "    # Fetch the current page of landmarks\n",
    "    response = db_client.get_lpc_reports(page=page, limit=page_size)\n",
    "\n",
    "    if not response.results:\n",
    "        return []\n",
    "\n",
    "    # Get Wikipedia articles for each landmark\n",
    "    for landmark in response.results:\n",
    "        landmark_id = landmark.lpNumber\n",
    "        name = landmark.name\n",
    "\n",
    "        # Fetch articles for this landmark\n",
    "        articles = db_client.get_wikipedia_articles(landmark_id)\n",
    "\n",
    "        # Add to our data structure\n",
    "        landmark_data = {\n",
    "            \"landmark_id\": landmark_id,\n",
    "            \"landmark_name\": name,\n",
    "            \"articles\": articles,\n",
    "            \"article_count\": len(articles),\n",
    "        }\n",
    "        all_landmarks_with_articles.append(landmark_data)\n",
    "\n",
    "    return all_landmarks_with_articles\n",
    "\n",
    "\n",
    "# Calculate max page number\n",
    "\n",
    "\n",
    "def get_landmarks_max_page(page_size):\n",
    "    return math.ceil(total_records / page_size)\n",
    "\n",
    "\n",
    "# Create the page number input\n",
    "landmark_page_number = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=get_landmarks_max_page(landmark_page_size_dropdown.value),\n",
    "    step=1,\n",
    "    description=\"Page:\",\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"150px\"),\n",
    ")\n",
    "\n",
    "# Navigation buttons\n",
    "landmark_prev_button = widgets.Button(\n",
    "    description=\"Previous\",\n",
    "    disabled=True,  # Disabled initially\n",
    "    button_style=\"\",\n",
    "    tooltip=\"Go to previous page\",\n",
    "    icon=\"arrow-left\",\n",
    ")\n",
    "\n",
    "landmark_next_button = widgets.Button(\n",
    "    description=\"Next\",\n",
    "    disabled=False,\n",
    "    button_style=\"\",\n",
    "    tooltip=\"Go to next page\",\n",
    "    icon=\"arrow-right\",\n",
    ")\n",
    "\n",
    "# Status label\n",
    "landmark_status_label = widgets.Label(\n",
    "    value=f\"Page 1 of {get_landmarks_max_page(landmark_page_size_dropdown.value)} (Records: {total_records})\"\n",
    ")\n",
    "\n",
    "# Output area\n",
    "landmark_output_area = widgets.Output()\n",
    "\n",
    "# Function to display landmarks with their Wikipedia articles\n",
    "\n",
    "\n",
    "def display_landmarks_with_articles():\n",
    "    # Clear previous output\n",
    "    if not all_landmarks_with_articles:\n",
    "        print(\"No landmarks found on this page\")\n",
    "        return\n",
    "\n",
    "    # Create a basic table for landmarks\n",
    "    landmarks_table = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"ID\": item[\"landmark_id\"],\n",
    "                \"Name\": item[\"landmark_name\"],\n",
    "                \"Wikipedia Articles\": item[\"article_count\"],\n",
    "            }\n",
    "            for item in all_landmarks_with_articles\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Landmarks:\")\n",
    "    display(landmarks_table)\n",
    "\n",
    "    # For each landmark, show their articles if any exist\n",
    "    for landmark in all_landmarks_with_articles:\n",
    "        if landmark[\"article_count\"] > 0:\n",
    "            print(\n",
    "                f\"\\n{landmark['landmark_name']} ({landmark['landmark_id']}) - {landmark['article_count']} Wikipedia articles:\"\n",
    "            )\n",
    "\n",
    "            # Create articles table\n",
    "            articles_data = [\n",
    "                {\"Title\": article.title, \"URL\": article.url}\n",
    "                for article in landmark[\"articles\"]\n",
    "            ]\n",
    "\n",
    "            articles_table = pd.DataFrame(articles_data)\n",
    "            display(articles_table)\n",
    "            print(\"-\" * 80)  # Separator\n",
    "\n",
    "\n",
    "# Event handlers\n",
    "\n",
    "\n",
    "def on_landmark_page_change(change):\n",
    "    if change[\"name\"] == \"value\" and change[\"new\"] != change[\"old\"]:\n",
    "        page = change[\"new\"]\n",
    "        page_size = landmark_page_size_dropdown.value\n",
    "        max_page = get_landmarks_max_page(page_size)\n",
    "\n",
    "        # Update button states\n",
    "        landmark_prev_button.disabled = page <= 1\n",
    "        landmark_next_button.disabled = page >= max_page\n",
    "\n",
    "        # Update status label\n",
    "        landmark_status_label.value = (\n",
    "            f\"Page {page} of {max_page} (Records: {total_records})\"\n",
    "        )\n",
    "\n",
    "        # Fetch and display landmarks with articles\n",
    "        print(f\"Fetching page {page} with {page_size} landmarks per page...\")\n",
    "        fetch_landmark_articles(page, page_size)\n",
    "        display_landmarks_with_articles()\n",
    "\n",
    "\n",
    "def on_landmark_page_size_change(change):\n",
    "    if change[\"name\"] == \"value\" and change[\"new\"] != change[\"old\"]:\n",
    "        new_page_size = change[\"new\"]\n",
    "        new_max_page = get_landmarks_max_page(new_page_size)\n",
    "\n",
    "        # Update page number widget range\n",
    "        landmark_page_number.max = new_max_page\n",
    "\n",
    "        # Adjust current page if needed\n",
    "        if landmark_page_number.value > new_max_page:\n",
    "            landmark_page_number.value = new_max_page\n",
    "\n",
    "        # Update status label\n",
    "        landmark_status_label.value = f\"Page {landmark_page_number.value} of {new_max_page} (Records: {total_records})\"\n",
    "\n",
    "        # Refetch with new page size\n",
    "        fetch_landmark_articles(landmark_page_number.value, new_page_size)\n",
    "        display_landmarks_with_articles()\n",
    "\n",
    "\n",
    "def on_landmark_prev_button_click(b):\n",
    "    if landmark_page_number.value > 1:\n",
    "        landmark_page_number.value -= 1\n",
    "\n",
    "\n",
    "def on_landmark_next_button_click(b):\n",
    "    if landmark_page_number.value < get_landmarks_max_page(\n",
    "        landmark_page_size_dropdown.value\n",
    "    ):\n",
    "        landmark_page_number.value += 1\n",
    "\n",
    "\n",
    "# Register event handlers\n",
    "# Remove existing observers to prevent multiple calls if the cell is run multiple times\n",
    "if hasattr(landmark_page_number, \"observers\"):\n",
    "    landmark_page_number.unobserve(on_landmark_page_change, names=\"value\")\n",
    "if hasattr(landmark_page_size_dropdown, \"observers\"):\n",
    "    landmark_page_size_dropdown.unobserve(on_landmark_page_size_change, names=\"value\")\n",
    "if hasattr(landmark_prev_button, \"observers\"):\n",
    "    landmark_prev_button.unobserve(on_landmark_prev_button_click)\n",
    "if hasattr(landmark_next_button, \"observers\"):\n",
    "    landmark_next_button.unobserve(on_landmark_next_button_click)\n",
    "\n",
    "landmark_page_number.observe(on_landmark_page_change, names=\"value\")\n",
    "landmark_page_size_dropdown.observe(on_landmark_page_size_change, names=\"value\")\n",
    "landmark_prev_button.on_click(on_landmark_prev_button_click)\n",
    "landmark_next_button.on_click(on_landmark_next_button_click)\n",
    "\n",
    "# For VS Code compatibility, create alternative non-widget UI\n",
    "print(\"\\nVS Code Widget Alternative Controls for Landmarks with Wikipedia Articles:\")\n",
    "print(\n",
    "    \"Use these functions to navigate instead of the widgets if they don't display properly:\"\n",
    ")\n",
    "\n",
    "\n",
    "def view_landmarks_page(page_num):\n",
    "    \"\"\"Go to a specific page of landmarks with Wikipedia articles\"\"\"\n",
    "    if 1 <= page_num <= get_landmarks_max_page(landmark_page_size_dropdown.value):\n",
    "        landmark_page_number.value = page_num\n",
    "        fetch_landmark_articles(page_num, landmark_page_size_dropdown.value)\n",
    "        display_landmarks_with_articles()\n",
    "    else:\n",
    "        print(\n",
    "            f\"Page number must be between 1 and {get_landmarks_max_page(landmark_page_size_dropdown.value)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def next_landmarks_page():\n",
    "    \"\"\"Go to next page of landmarks\"\"\"\n",
    "    view_landmarks_page(landmark_page_number.value + 1)\n",
    "\n",
    "\n",
    "def prev_landmarks_page():\n",
    "    \"\"\"Go to previous page of landmarks\"\"\"\n",
    "    view_landmarks_page(landmark_page_number.value - 1)\n",
    "\n",
    "\n",
    "def change_landmarks_page_size(size):\n",
    "    \"\"\"Change page size for landmarks\"\"\"\n",
    "    if size in landmark_page_size_options:\n",
    "        landmark_page_size_dropdown.value = size\n",
    "        fetch_landmark_articles(landmark_page_number.value, size)\n",
    "        display_landmarks_with_articles()\n",
    "    else:\n",
    "        print(f\"Page size must be one of {landmark_page_size_options}\")\n",
    "\n",
    "\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"view_landmarks_page(2)        # Go to page 2\")\n",
    "print(\"next_landmarks_page()         # Go to next page\")\n",
    "print(\"prev_landmarks_page()         # Go to previous page\")\n",
    "print(\"change_landmarks_page_size(10) # Change to 10 records per page\")\n",
    "\n",
    "# Initial fetch and display\n",
    "fetch_landmark_articles(landmark_page_number.value, landmark_page_size_dropdown.value)\n",
    "display_landmarks_with_articles()\n",
    "\n",
    "# Layout the widgets - display if they render properly in your environment\n",
    "landmark_controls = widgets.HBox(\n",
    "    [\n",
    "        landmark_page_size_dropdown,\n",
    "        landmark_page_number,\n",
    "        landmark_prev_button,\n",
    "        landmark_next_button,\n",
    "    ]\n",
    ")\n",
    "landmark_dashboard = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\"<h3>Landmarks with Wikipedia Articles</h3>\"),\n",
    "        landmark_controls,\n",
    "        landmark_status_label,\n",
    "        landmark_output_area,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Try displaying the dashboard\n",
    "try:\n",
    "    display(landmark_dashboard)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"\\nNote: Widget display error: {str(e)}. Use the alternative functions above instead.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fetching and Processing Wikipedia Content\n",
    "\n",
    "Now let's fetch the actual content from a Wikipedia article and process it for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and process content from a single Wikipedia article\n",
    "\n",
    "# Create widgets for article selection\n",
    "article_selector = None\n",
    "if all_wiki_articles:\n",
    "    # Create a dropdown with available articles\n",
    "    article_options = [\n",
    "        (f\"{article['landmark_name']} - {article['article_title']}\", i)\n",
    "        for i, article in enumerate(all_wiki_articles)\n",
    "    ]\n",
    "\n",
    "    article_selector = widgets.Dropdown(\n",
    "        options=article_options,\n",
    "        description=\"Select article:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"600px\"),\n",
    "    )\n",
    "\n",
    "    # Display the selector\n",
    "    print(\"\\nVS Code Widget Alternative for Article Selection:\")\n",
    "    print(\n",
    "        \"Use this function to select an article if the dropdown widget doesn't display properly:\"\n",
    "    )\n",
    "\n",
    "    # Create a list of available articles for selection\n",
    "    print(\"\\nAvailable articles:\")\n",
    "    for i, (label, _) in enumerate(article_options):\n",
    "        print(f\"{i}: {label}\")\n",
    "\n",
    "    def select_article(index):\n",
    "        \"\"\"Select an article by its index\"\"\"\n",
    "        if 0 <= index < len(article_options):\n",
    "            fetch_article_content(index)\n",
    "        else:\n",
    "            print(f\"Index must be between 0 and {len(article_options) - 1}\")\n",
    "\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"select_article(0)  # Select the first article\")\n",
    "\n",
    "    # Try to display the selector widget\n",
    "    try:\n",
    "        display(article_selector)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"\\nNote: Widget display error: {str(e)}. Use the select_article function instead.\"\n",
    "        )\n",
    "\n",
    "    # Create an output area for article content\n",
    "    article_output = widgets.Output()\n",
    "    display(article_output)\n",
    "\n",
    "    # Function to fetch and display article content\n",
    "    def fetch_article_content(article_idx):\n",
    "        if article_idx is None:\n",
    "            print(\"Please select an article\")\n",
    "            return\n",
    "\n",
    "        article = all_wiki_articles[article_idx]\n",
    "        print(f\"Fetching content for: {article['article_title']}\")\n",
    "        print(f\"URL: {article['article_url']}\")\n",
    "        print(f\"Landmark: {article['landmark_name']} ({article['landmark_id']})\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Fetch article content\n",
    "        content = wiki_fetcher.fetch_wikipedia_content(article[\"article_url\"])\n",
    "        if not content:\n",
    "            print(\"Failed to fetch article content\")\n",
    "            return\n",
    "\n",
    "        # Display content statistics\n",
    "        print(f\"Article length: {len(content)} characters\")\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = wiki_fetcher.chunk_wikipedia_text(\n",
    "            content, chunk_size=1000, chunk_overlap=200\n",
    "        )\n",
    "\n",
    "        print(f\"Split article into {len(chunks)} chunks\")\n",
    "\n",
    "        # Display a sample of the content\n",
    "        print(\"\\nArticle preview:\")\n",
    "        preview_length = min(500, len(content))\n",
    "        print(content[:preview_length] + \"...\")\n",
    "\n",
    "        # Display first chunk\n",
    "        if chunks:\n",
    "            print(\"\\nFirst chunk:\")\n",
    "            print(f\"Text: {chunks[0]['text'][:300]}...\")\n",
    "            print(f\"Metadata: {chunks[0]['metadata']}\")\n",
    "\n",
    "    # Observe article selection\n",
    "    def on_article_select(change):\n",
    "        if change[\"name\"] == \"value\" and change[\"new\"] is not None:\n",
    "            fetch_article_content(change[\"new\"])\n",
    "\n",
    "    article_selector.observe(on_article_select, names=\"value\")\n",
    "\n",
    "    # Initial display if we have articles\n",
    "    if article_options:\n",
    "        fetch_article_content(article_options[0][1])\n",
    "\n",
    "else:\n",
    "    print(\"No Wikipedia articles available to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. End-to-End Wikipedia Processing Test\n",
    "\n",
    "This section demonstrates a complete end-to-end workflow for processing and querying Wikipedia articles for NYC landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete end-to-end test for a single landmark\n",
    "\n",
    "\n",
    "def process_landmark_wikipedia_articles(landmark_id):\n",
    "    \"\"\"Process Wikipedia articles for a single landmark.\n",
    "\n",
    "    Args:\n",
    "        landmark_id: ID of the landmark to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with processing results\n",
    "    \"\"\"\n",
    "    print(f\"Processing Wikipedia articles for landmark {landmark_id}\")\n",
    "\n",
    "    # Progress tracker\n",
    "    progress_output = widgets.Output()\n",
    "    progress_bar = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=5,  # 5 steps in our process\n",
    "        description=\"Progress:\",\n",
    "        bar_style=\"info\",\n",
    "        orientation=\"horizontal\",\n",
    "    )\n",
    "    display(\n",
    "        widgets.VBox(\n",
    "            [widgets.HTML(\"<b>Processing Steps</b>\"), progress_bar, progress_output]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def update_progress(step, message):\n",
    "        progress_bar.value = step\n",
    "        with progress_output:\n",
    "            print(f\"Step {step}/5: {message}\")\n",
    "\n",
    "    update_progress(1, \"Fetching Wikipedia articles\")\n",
    "\n",
    "    # Step 1: Get Wikipedia articles for the landmark\n",
    "    try:\n",
    "        articles = db_client.get_wikipedia_articles(landmark_id)\n",
    "        if not articles:\n",
    "            with progress_output:\n",
    "                print(f\"No Wikipedia articles found for landmark {landmark_id}\")\n",
    "            return {\"success\": False, \"reason\": \"No Wikipedia articles found\"}\n",
    "\n",
    "        with progress_output:\n",
    "            print(f\"Found {len(articles)} Wikipedia articles\")\n",
    "    except Exception as e:\n",
    "        with progress_output:\n",
    "            print(f\"Error fetching Wikipedia articles: {str(e)}\")\n",
    "        return {\"success\": False, \"reason\": f\"Error fetching articles: {str(e)}\"}\n",
    "\n",
    "    # Step 2: Process each article\n",
    "    update_progress(2, \"Fetching article content\")\n",
    "    all_chunks = []\n",
    "    article_details = []\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        with progress_output:\n",
    "            print(f\"\\nProcessing article {i+1}/{len(articles)}: {article.title}\")\n",
    "\n",
    "        # Fetch article content\n",
    "        try:\n",
    "            content = wiki_fetcher.fetch_wikipedia_content(article.url)\n",
    "            if not content:\n",
    "                with progress_output:\n",
    "                    print(f\"Failed to fetch content for article: {article.title}\")\n",
    "                continue\n",
    "\n",
    "            with progress_output:\n",
    "                print(f\"Successfully fetched article content ({len(content)} chars)\")\n",
    "\n",
    "            article_details.append(\n",
    "                {\n",
    "                    \"title\": article.title,\n",
    "                    \"url\": article.url,\n",
    "                    \"content_length\": len(content),\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            with progress_output:\n",
    "                print(f\"Error fetching content for {article.title}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Chunk the content\n",
    "        update_progress(3, \"Chunking article content\")\n",
    "        try:\n",
    "            chunks = wiki_fetcher.chunk_wikipedia_text(\n",
    "                content, chunk_size=1000, chunk_overlap=200\n",
    "            )\n",
    "\n",
    "            with progress_output:\n",
    "                print(f\"Split article into {len(chunks)} chunks\")\n",
    "\n",
    "            # Add article metadata to chunks\n",
    "            for chunk in chunks:\n",
    "                chunk[\"metadata\"][\"article_title\"] = article.title\n",
    "                chunk[\"metadata\"][\"article_url\"] = article.url\n",
    "                chunk[\"metadata\"][\"source_type\"] = \"wikipedia\"\n",
    "                chunk[\"metadata\"][\"landmark_id\"] = landmark_id\n",
    "\n",
    "            all_chunks.extend(chunks)\n",
    "        except Exception as e:\n",
    "            with progress_output:\n",
    "                print(f\"Error chunking content for {article.title}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Step 3: Generate embeddings\n",
    "    update_progress(4, \"Generating embeddings\")\n",
    "    # Limit to first 5 chunks for testing to reduce processing time\n",
    "    test_chunks = all_chunks[:5] if len(all_chunks) > 5 else all_chunks\n",
    "\n",
    "    with progress_output:\n",
    "        print(f\"\\nGenerating embeddings for {len(test_chunks)} chunks\")\n",
    "\n",
    "    try:\n",
    "        chunks_with_embeddings = embedding_generator.process_chunks(test_chunks)\n",
    "        with progress_output:\n",
    "            print(f\"Generated embeddings for {len(chunks_with_embeddings)} chunks\")\n",
    "    except Exception as e:\n",
    "        with progress_output:\n",
    "            print(f\"Error generating embeddings: {str(e)}\")\n",
    "        return {\"success\": False, \"reason\": f\"Error generating embeddings: {str(e)}\"}\n",
    "\n",
    "    # Step 4: Store in Pinecone\n",
    "    update_progress(5, \"Storing in vector database\")\n",
    "    with progress_output:\n",
    "        print(\"\\nStoring embeddings in Pinecone...\")\n",
    "\n",
    "    try:\n",
    "        vector_ids = pinecone_db.store_chunks(\n",
    "            chunks=chunks_with_embeddings,\n",
    "            id_prefix=f\"wiki-{landmark_id}-\",\n",
    "            landmark_id=landmark_id,\n",
    "            use_fixed_ids=True,\n",
    "            delete_existing=True,\n",
    "        )\n",
    "\n",
    "        with progress_output:\n",
    "            print(f\"Stored {len(vector_ids)} vectors in Pinecone\")\n",
    "    except Exception as e:\n",
    "        with progress_output:\n",
    "            print(f\"Error storing vectors: {str(e)}\")\n",
    "        return {\"success\": False, \"reason\": f\"Error storing vectors: {str(e)}\"}\n",
    "\n",
    "    # Step 5: Query the vectors\n",
    "    with progress_output:\n",
    "        print(\"\\nTesting retrieval with a query...\")\n",
    "\n",
    "    landmark_name = next(\n",
    "        (l.name for l in landmarks if l.lpNumber == landmark_id), \"landmark\"\n",
    "    )\n",
    "    test_query = f\"Tell me about the history of {landmark_name}\"\n",
    "\n",
    "    with progress_output:\n",
    "        print(f\"Test query: '{test_query}'\")\n",
    "\n",
    "    try:\n",
    "        query_embedding = embedding_generator.generate_embedding(test_query)\n",
    "        results = pinecone_db.query_vectors(\n",
    "            query_embedding,\n",
    "            top_k=3,\n",
    "            filter_dict={\"landmark_id\": landmark_id, \"source_type\": \"wikipedia\"},\n",
    "        )\n",
    "\n",
    "        with progress_output:\n",
    "            print(f\"Found {len(results)} matching results\")\n",
    "\n",
    "            if results:\n",
    "                for i, match in enumerate(results):\n",
    "                    print(f\"\\nMatch {i+1} - Score: {match['score']:.4f}\")\n",
    "                    print(\n",
    "                        f\"Article: {match['metadata'].get('article_title', 'Unknown')}\"\n",
    "                    )\n",
    "                    print(f\"Text: {match['metadata'].get('text', '')[:150]}...\")\n",
    "    except Exception as e:\n",
    "        with progress_output:\n",
    "            print(f\"Error querying vectors: {str(e)}\")\n",
    "        # Don't fail the overall process just for the query test\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"landmark_id\": landmark_id,\n",
    "        \"landmark_name\": landmark_name,\n",
    "        \"articles_processed\": len(articles),\n",
    "        \"article_details\": article_details,\n",
    "        \"chunks_generated\": len(all_chunks),\n",
    "        \"vectors_stored\": len(vector_ids),\n",
    "        \"query_results\": results if \"results\" in locals() else [],\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a dropdown to select a landmark for testing\n",
    "if landmark_articles:\n",
    "    # Create options for the dropdown\n",
    "    landmark_options = [\n",
    "        (\n",
    "            f\"{landmark_id} - {next((l.name for l in landmarks if l.lpNumber == landmark_id), 'Unknown')}\",\n",
    "            landmark_id,\n",
    "        )\n",
    "        for landmark_id in landmark_articles.keys()\n",
    "    ]\n",
    "\n",
    "    test_landmark_selector = widgets.Dropdown(\n",
    "        options=landmark_options,\n",
    "        description=\"Test landmark:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"600px\"),\n",
    "    )\n",
    "\n",
    "    # Create a button to start the test\n",
    "    test_button = widgets.Button(\n",
    "        description=\"Run End-to-End Test\",\n",
    "        button_style=\"primary\",\n",
    "        tooltip=\"Process Wikipedia articles for the selected landmark\",\n",
    "    )\n",
    "\n",
    "    # Display the selector and button\n",
    "    display(\n",
    "        widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\"<h3>End-to-End Wikipedia Processing Test</h3>\"),\n",
    "                test_landmark_selector,\n",
    "                test_button,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create an output area for test results\n",
    "    test_output = widgets.Output()\n",
    "    display(test_output)\n",
    "\n",
    "    # Function to run when button is clicked\n",
    "    def on_test_button_click(b):\n",
    "        with test_output:\n",
    "            clear_output()\n",
    "            if test_landmark_selector.value:\n",
    "                print(\n",
    "                    f\"Starting end-to-end test for landmark: {test_landmark_selector.value}\"\n",
    "                )\n",
    "                result = process_landmark_wikipedia_articles(\n",
    "                    test_landmark_selector.value\n",
    "                )\n",
    "\n",
    "                if result[\"success\"]:\n",
    "                    print(\"\\nTest completed successfully!\")\n",
    "                    print(\n",
    "                        f\"Landmark: {result['landmark_name']} ({result['landmark_id']})\"\n",
    "                    )\n",
    "                    print(f\"Articles processed: {result['articles_processed']}\")\n",
    "                    print(f\"Chunks generated: {result['chunks_generated']}\")\n",
    "                    print(f\"Vectors stored: {result['vectors_stored']}\")\n",
    "                else:\n",
    "                    print(f\"\\nTest failed: {result['reason']}\")\n",
    "            else:\n",
    "                print(\"Please select a landmark to test\")\n",
    "\n",
    "    # Register button click handler\n",
    "    test_button.on_click(on_test_button_click)\n",
    "\n",
    "    # Automatically run the test if only one landmark is available\n",
    "    if len(landmark_options) == 1:\n",
    "        with test_output:\n",
    "            print(\n",
    "                f\"Only one landmark with Wikipedia articles found. Running test automatically...\"\n",
    "            )\n",
    "            test_landmark_selector.value = landmark_options[0][1]\n",
    "            on_test_button_click(test_button)\n",
    "else:\n",
    "    print(\n",
    "        \"No landmarks with Wikipedia articles available for testing. Please browse to a page with landmarks that have Wikipedia articles.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Wikipedia Coverage in Vector Database\n",
    "\n",
    "This section analyzes how Wikipedia articles are represented in the vector database. We'll examine:\n",
    "\n",
    "- The total number of Wikipedia vectors compared to other content\n",
    "- Coverage percentage of landmarks with Wikipedia content\n",
    "- Distribution of Wikipedia content across different landmarks\n",
    "- Performance of semantic searches against Wikipedia content\n",
    "\n",
    "This analysis helps assess the current state of Wikipedia integration and identify opportunities for improvement in coverage and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Wikipedia vectors in the Pinecone database\n",
    "\n",
    "# Create an output area for database information\n",
    "db_output = widgets.Output()\n",
    "display(db_output)\n",
    "\n",
    "with db_output:\n",
    "    # Get statistics from Pinecone\n",
    "    print(\"Checking vector database for Wikipedia content...\")\n",
    "    try:\n",
    "        # Get index statistics\n",
    "        index_stats = pinecone_db.get_index_stats()\n",
    "        print(\n",
    "            f\"Total vectors in the database: {index_stats.get('total_vector_count', 'N/A')}\"\n",
    "        )\n",
    "        print(f\"Dimensions: {index_stats.get('dimension', 'N/A')}\")\n",
    "\n",
    "        # Query for Wikipedia vectors specifically\n",
    "        wikipedia_count = pinecone_db.count_vectors({\"source_type\": \"wikipedia\"})\n",
    "        print(f\"Wikipedia vectors in the database: {wikipedia_count}\")\n",
    "\n",
    "        # Calculate percentage\n",
    "        if index_stats.get(\"total_vector_count\", 0) > 0:\n",
    "            wikipedia_percentage = (\n",
    "                wikipedia_count / index_stats[\"total_vector_count\"] * 100\n",
    "            )\n",
    "            print(\n",
    "                f\"Wikipedia content makes up {wikipedia_percentage:.2f}% of the vector database\"\n",
    "            )\n",
    "\n",
    "            # Create a pie chart of content distribution if matplotlib is available\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                # Create the pie chart data\n",
    "                labels = [\"Wikipedia Content\", \"Other Content\"]\n",
    "                sizes = [\n",
    "                    wikipedia_count,\n",
    "                    index_stats[\"total_vector_count\"] - wikipedia_count,\n",
    "                ]\n",
    "                colors = [\"#ff9999\", \"#66b3ff\"]\n",
    "                explode = (0.1, 0)  # explode the 1st slice (Wikipedia)\n",
    "\n",
    "                # Plot the pie chart\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.pie(\n",
    "                    sizes,\n",
    "                    explode=explode,\n",
    "                    labels=labels,\n",
    "                    colors=colors,\n",
    "                    autopct=\"%1.1f%%\",\n",
    "                    shadow=True,\n",
    "                    startangle=90,\n",
    "                )\n",
    "                plt.axis(\n",
    "                    \"equal\"\n",
    "                )  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "                plt.title(\"Vector Database Content Distribution\")\n",
    "                plt.show()\n",
    "            except ImportError:\n",
    "                print(\"\\nNote: Install matplotlib to generate visualization\")\n",
    "\n",
    "        # Get landmarks with Wikipedia content\n",
    "        all_landmark_ids = set()\n",
    "        wikipedia_landmark_ids = set()\n",
    "\n",
    "        # This is a simplified approach - in a real production environment,\n",
    "        # you might need to paginate through all vectors\n",
    "        sample_size = 1000\n",
    "        vectors = pinecone_db.list_vectors(limit=sample_size, filter_dict=None)\n",
    "\n",
    "        # Data for analysis\n",
    "        landmark_vector_counts = {}\n",
    "        wikipedia_vector_counts = {}\n",
    "\n",
    "        for vector in vectors:\n",
    "            if \"landmark_id\" in vector[\"metadata\"]:\n",
    "                landmark_id = vector[\"metadata\"][\"landmark_id\"]\n",
    "                all_landmark_ids.add(landmark_id)\n",
    "\n",
    "                # Count vectors per landmark\n",
    "                landmark_vector_counts[landmark_id] = (\n",
    "                    landmark_vector_counts.get(landmark_id, 0) + 1\n",
    "                )\n",
    "\n",
    "                if vector[\"metadata\"].get(\"source_type\") == \"wikipedia\":\n",
    "                    wikipedia_landmark_ids.add(landmark_id)\n",
    "\n",
    "                    # Count wikipedia vectors per landmark\n",
    "                    wikipedia_vector_counts[landmark_id] = (\n",
    "                        wikipedia_vector_counts.get(landmark_id, 0) + 1\n",
    "                    )\n",
    "\n",
    "        print(f\"\\nCoverage analysis (based on sample of {len(vectors)} vectors):\")\n",
    "        print(f\"Landmarks with any content: {len(all_landmark_ids)}\")\n",
    "        print(f\"Landmarks with Wikipedia content: {len(wikipedia_landmark_ids)}\")\n",
    "\n",
    "        if len(all_landmark_ids) > 0:\n",
    "            coverage_percentage = (\n",
    "                len(wikipedia_landmark_ids) / len(all_landmark_ids) * 100\n",
    "            )\n",
    "            print(f\"Landmark Wikipedia coverage: {coverage_percentage:.2f}%\")\n",
    "\n",
    "            # Detailed distribution analysis\n",
    "            if wikipedia_vector_counts:\n",
    "                print(\"\\nWikipedia content distribution:\")\n",
    "                average_wiki_vectors = sum(wikipedia_vector_counts.values()) / len(\n",
    "                    wikipedia_vector_counts\n",
    "                )\n",
    "                max_wiki_vectors = (\n",
    "                    max(wikipedia_vector_counts.values())\n",
    "                    if wikipedia_vector_counts\n",
    "                    else 0\n",
    "                )\n",
    "                min_wiki_vectors = (\n",
    "                    min(wikipedia_vector_counts.values())\n",
    "                    if wikipedia_vector_counts\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"- Average Wikipedia vectors per landmark: {average_wiki_vectors:.2f}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"- Maximum Wikipedia vectors for a single landmark: {max_wiki_vectors}\"\n",
    "                )\n",
    "                print(f\"- Minimum Wikipedia vectors for a landmark: {min_wiki_vectors}\")\n",
    "\n",
    "                # Plot distribution if matplotlib is available\n",
    "                try:\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    import numpy as np\n",
    "\n",
    "                    # Get top landmarks by Wikipedia vector count\n",
    "                    top_landmarks = sorted(\n",
    "                        wikipedia_vector_counts.items(),\n",
    "                        key=lambda x: x[1],\n",
    "                        reverse=True,\n",
    "                    )[:10]\n",
    "\n",
    "                    if top_landmarks:\n",
    "                        # Create bar chart for top landmarks\n",
    "                        landmark_ids = [item[0] for item in top_landmarks]\n",
    "                        vector_counts = [item[1] for item in top_landmarks]\n",
    "\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        bars = plt.bar(range(len(landmark_ids)), vector_counts)\n",
    "                        plt.xticks(range(len(landmark_ids)), landmark_ids, rotation=45)\n",
    "                        plt.xlabel(\"Landmark ID\")\n",
    "                        plt.ylabel(\"Wikipedia Vector Count\")\n",
    "                        plt.title(\"Top Landmarks by Wikipedia Vector Count\")\n",
    "\n",
    "                        # Add count labels on top of bars\n",
    "                        for bar in bars:\n",
    "                            height = bar.get_height()\n",
    "                            plt.text(\n",
    "                                bar.get_x() + bar.get_width() / 2.0,\n",
    "                                height,\n",
    "                                f\"{height}\",\n",
    "                                ha=\"center\",\n",
    "                                va=\"bottom\",\n",
    "                                rotation=0,\n",
    "                            )\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                except ImportError:\n",
    "                    print(\"Note: Install matplotlib to generate visualizations\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving database statistics: {str(e)}\")\n",
    "\n",
    "# Create a section for testing semantic search capabilities\n",
    "print(\"\\nTest Semantic Search Against Wikipedia Content\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# Create input field for custom queries\n",
    "query_input = widgets.Text(\n",
    "    value=\"Tell me about the history of NYC landmarks\",\n",
    "    placeholder=\"Type your query here...\",\n",
    "    description=\"Query:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "# Create filter options\n",
    "filter_options = widgets.Dropdown(\n",
    "    options=[(\"All Wikipedia Content\", \"all\"), (\"Filter by Landmark ID\", \"landmark\")],\n",
    "    value=\"all\",\n",
    "    description=\"Filter:\",\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"300px\"),\n",
    ")\n",
    "\n",
    "# Create input for landmark ID filter\n",
    "landmark_id_input = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Enter landmark ID to filter results\",\n",
    "    description=\"Landmark ID:\",\n",
    "    disabled=True,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"400px\"),\n",
    ")\n",
    "\n",
    "# Number of results to return\n",
    "top_k_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description=\"Results to show:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\"d\",\n",
    "    layout=widgets.Layout(width=\"400px\"),\n",
    ")\n",
    "\n",
    "# Create button to run query\n",
    "query_button = widgets.Button(\n",
    "    description=\"Run Query\",\n",
    "    button_style=\"info\",\n",
    "    tooltip=\"Test a semantic query against Wikipedia content\",\n",
    "    icon=\"search\",\n",
    ")\n",
    "\n",
    "# Output area for results\n",
    "query_output = widgets.Output()\n",
    "\n",
    "# Function to update landmark ID input based on filter selection\n",
    "\n",
    "\n",
    "def update_landmark_filter(change):\n",
    "    if change[\"name\"] == \"value\":\n",
    "        landmark_id_input.disabled = change[\"new\"] != \"landmark\"\n",
    "\n",
    "\n",
    "# Register the callback\n",
    "filter_options.observe(update_landmark_filter, names=\"value\")\n",
    "\n",
    "# Function to run when button is clicked\n",
    "\n",
    "\n",
    "def on_query_button_click(b):\n",
    "    with query_output:\n",
    "        clear_output()\n",
    "        query_text = query_input.value\n",
    "        top_k = top_k_slider.value\n",
    "\n",
    "        print(f\"Query: '{query_text}'\")\n",
    "        print(f\"Retrieving top {top_k} results\")\n",
    "\n",
    "        # Prepare filter\n",
    "        filter_dict = {\"source_type\": \"wikipedia\"}\n",
    "        if filter_options.value == \"landmark\" and landmark_id_input.value.strip():\n",
    "            filter_dict[\"landmark_id\"] = landmark_id_input.value.strip()\n",
    "            print(f\"Filtering by landmark ID: {landmark_id_input.value.strip()}\")\n",
    "\n",
    "        # Generate embedding for the query\n",
    "        try:\n",
    "            query_embedding = embedding_generator.generate_embedding(query_text)\n",
    "\n",
    "            # Query the vector database\n",
    "            results = pinecone_db.query_vectors(\n",
    "                query_embedding, top_k=top_k, filter_dict=filter_dict\n",
    "            )\n",
    "\n",
    "            print(f\"\\nFound {len(results)} matching results\")\n",
    "\n",
    "            # Display the results\n",
    "            if results:\n",
    "                # Create a DataFrame for better visualization\n",
    "                results_data = []\n",
    "                for i, match in enumerate(results):\n",
    "                    results_data.append(\n",
    "                        {\n",
    "                            \"Rank\": i + 1,\n",
    "                            \"Score\": f\"{match['score']:.4f}\",\n",
    "                            \"Landmark ID\": match[\"metadata\"].get(\n",
    "                                \"landmark_id\", \"Unknown\"\n",
    "                            ),\n",
    "                            \"Article Title\": match[\"metadata\"].get(\n",
    "                                \"article_title\", \"Unknown\"\n",
    "                            ),\n",
    "                            \"Text Snippet\": match[\"metadata\"].get(\"text\", \"\")[:100]\n",
    "                            + \"...\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                results_df = pd.DataFrame(results_data)\n",
    "                display(results_df)\n",
    "\n",
    "                # Show detailed view of the first result\n",
    "                if results_data:\n",
    "                    print(\"\\nFirst Match Details:\")\n",
    "                    match = results[0]\n",
    "                    print(f\"Score: {match['score']:.4f}\")\n",
    "                    print(\n",
    "                        f\"Landmark ID: {match['metadata'].get('landmark_id', 'Unknown')}\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Article: {match['metadata'].get('article_title', 'Unknown')}\"\n",
    "                    )\n",
    "                    print(f\"URL: {match['metadata'].get('article_url', 'Unknown')}\")\n",
    "                    print(\"\\nText content:\")\n",
    "                    print(match[\"metadata\"].get(\"text\", \"\")[:500] + \"...\")\n",
    "            else:\n",
    "                print(\"No Wikipedia content found matching the query\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error performing query: {str(e)}\")\n",
    "\n",
    "\n",
    "# Register button click handler\n",
    "query_button.on_click(on_query_button_click)\n",
    "\n",
    "# Display the widgets\n",
    "display(widgets.HTML(\"<h3>Semantic Search of Wikipedia Content</h3>\"))\n",
    "display(query_input)\n",
    "display(widgets.HBox([filter_options, landmark_id_input]))\n",
    "display(top_k_slider)\n",
    "display(query_button)\n",
    "display(query_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Report\n",
    "\n",
    "This section provides a summary of the Wikipedia integration testing and the current state of Wikipedia content in the vector database. We'll examine:\n",
    "\n",
    "- The total number of Wikipedia vectors compared to other content\n",
    "- Coverage percentage of landmarks with Wikipedia content\n",
    "- Distribution of Wikipedia content across different landmarks\n",
    "- Performance of semantic searches against Wikipedia content\n",
    "\n",
    "This analysis helps assess the current state of Wikipedia integration and identify opportunities for improvement in coverage and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary report of Wikipedia integration\n",
    "\n",
    "# Create a styled output area for the summary report\n",
    "summary_output = widgets.Output()\n",
    "display(widgets.HTML(\"<h3>Wikipedia Integration Summary Report</h3>\"), summary_output)\n",
    "\n",
    "with summary_output:\n",
    "    print(\"Generating Wikipedia integration summary report...\\n\")\n",
    "\n",
    "    # 1. API Connectivity\n",
    "    print(\"1. API Connectivity\")\n",
    "    try:\n",
    "        record_count = db_client.get_total_record_count()\n",
    "        print(\n",
    "            f\"✅ Successfully connected to CoreDataStore API. Found {record_count} landmark records.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to CoreDataStore API: {str(e)}\")\n",
    "\n",
    "    # 2. Wikipedia Data\n",
    "    print(\"\\n2. Wikipedia Data\")\n",
    "    total_articles = (\n",
    "        sum(len(articles) for articles in landmark_articles.values())\n",
    "        if landmark_articles\n",
    "        else 0\n",
    "    )\n",
    "    print(\n",
    "        f\"✅ Found {total_articles} Wikipedia articles across {len(landmark_articles) if landmark_articles else 0} landmarks in the sample.\"\n",
    "    )\n",
    "\n",
    "    # 3. Vector Database Integration\n",
    "    print(\"\\n3. Vector Database Integration\")\n",
    "    try:\n",
    "        # Get vector database stats\n",
    "        index_stats = pinecone_db.get_index_stats()\n",
    "        wiki_count = pinecone_db.count_vectors({\"source_type\": \"wikipedia\"})\n",
    "\n",
    "        print(\"✅ Vector database connection established.\")\n",
    "        print(f\"   - Total vectors: {index_stats.get('total_vector_count', 'N/A')}\")\n",
    "        print(\n",
    "            f\"   - Wikipedia vectors: {wiki_count} ({(wiki_count/index_stats['total_vector_count']*100) if index_stats.get('total_vector_count', 0) > 0 else 0:.2f}%)\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to vector database: {str(e)}\")\n",
    "\n",
    "    # 4. Embedding Generation\n",
    "    print(\"\\n4. Embedding Generation\")\n",
    "    try:\n",
    "        test_text = \"This is a test for embedding generation.\"\n",
    "        embedding = embedding_generator.generate_embedding(test_text)\n",
    "        print(f\"✅ Embedding generation is working. Dimension: {len(embedding)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating embeddings: {str(e)}\")\n",
    "\n",
    "    # 5. Overall Assessment\n",
    "    print(\"\\n5. Overall Assessment\")\n",
    "\n",
    "    # Let's make a simple assessment based on what we found\n",
    "    if total_articles > 0 and \"wiki_count\" in locals() and wiki_count > 0:\n",
    "        print(\"✅ Wikipedia integration is functioning correctly.\")\n",
    "        print(\"   - Wikipedia articles can be fetched from the API\")\n",
    "        print(\"   - Articles can be processed and chunked\")\n",
    "        print(\"   - Embeddings can be generated\")\n",
    "        print(\"   - Vectors can be stored in and retrieved from the database\")\n",
    "    elif total_articles > 0:\n",
    "        print(\"⚠️ Wikipedia integration is partially working.\")\n",
    "        print(\"   - Wikipedia articles can be fetched from the API\")\n",
    "        print(\"   - More testing needed for processing, embedding, and storage\")\n",
    "    else:\n",
    "        print(\"❌ Wikipedia integration needs attention.\")\n",
    "        print(\"   - No Wikipedia articles found or unable to access them\")\n",
    "        print(\"   - Review API connectivity and data availability\")\n",
    "\n",
    "    # 6. Recommendations\n",
    "    print(\"\\n6. Recommendations\")\n",
    "    if \"wiki_count\" in locals() and index_stats.get(\"total_vector_count\", 0) > 0:\n",
    "        wiki_percentage = wiki_count / index_stats[\"total_vector_count\"] * 100\n",
    "        if wiki_percentage < 10:\n",
    "            print(\"→ Consider increasing Wikipedia coverage in the vector database\")\n",
    "        if total_articles > wiki_count:\n",
    "            print(\"→ Many Wikipedia articles are not yet processed into vectors\")\n",
    "    else:\n",
    "        print(\"→ Begin processing Wikipedia articles into the vector database\")\n",
    "\n",
    "    print(\"→ Run the end-to-end test on more landmarks to ensure robustness\")\n",
    "    print(\"→ Consider implementing automatic updates when Wikipedia content changes\")\n",
    "\n",
    "    print(\"\\nTesting completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
