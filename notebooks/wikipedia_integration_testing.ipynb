{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Landmarks Wikipedia Integration Testing\n",
    "\n",
    "This notebook tests the integration of Wikipedia articles for NYC landmarks into the vector database. It demonstrates the process of:\n",
    "\n",
    "1. Fetching landmark information from the CoreDataStore API\n",
    "2. Retrieving associated Wikipedia articles\n",
    "3. Processing article content (fetching, cleaning, chunking)\n",
    "4. Generating embeddings for article chunks\n",
    "5. Storing embeddings in Pinecone vector database\n",
    "6. Querying the vector database to retrieve Wikipedia content\n",
    "7. Analyzing the distribution and quality of Wikipedia content in the vector database\n",
    "\n",
    "The notebook serves as both a testing tool and a demonstration of the Wikipedia integration capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment by creating a Python alias and installing any required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python alias for python3 and verify the Python installation\n",
    "!alias python=python3\n",
    "!python --version\n",
    "\n",
    "# Check if the project is installed correctly\n",
    "!pip list | grep nyc-landmarks-vector-db || echo \"Project not installed - install with 'pip install -e .'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the project in development mode if not already installed\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if we're in the right directory structure\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "# Check for setup.py to confirm we're in the right place\n",
    "setup_py_path = os.path.join(project_root, \"setup.py\")\n",
    "if os.path.exists(setup_py_path):\n",
    "    print(\"setup.py found, installing project in development mode...\")\n",
    "    !cd {project_root} && pip install -e .\n",
    "else:\n",
    "    print(f\"setup.py not found at {setup_py_path}, please check directory structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for environment variables required by the project\n",
    "import os\n",
    "\n",
    "# List of potential required environment variables\n",
    "env_vars = [\n",
    "    \"OPENAI_API_KEY\",  # For OpenAI embeddings\n",
    "    \"PINECONE_API_KEY\",  # For Pinecone vector DB\n",
    "    \"PINECONE_ENVIRONMENT\",  # Pinecone environment\n",
    "    \"PINECONE_INDEX_NAME\",  # Pinecone index name\n",
    "]\n",
    "\n",
    "print(\"Checking environment variables:\")\n",
    "for var in env_vars:\n",
    "    if var in os.environ:\n",
    "        print(f\"✓ {var} is set\")\n",
    "    else:\n",
    "        print(f\"✗ {var} is NOT set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Add project root to path to ensure imports work correctly\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.db.coredatastore_api import CoreDataStoreAPI\n",
    "from nyc_landmarks.db.wikipedia_fetcher import WikipediaFetcher\n",
    "from nyc_landmarks.embeddings.generator import EmbeddingGenerator\n",
    "from nyc_landmarks.models.wikipedia_models import (\n",
    "    WikipediaArticleModel,\n",
    "    WikipediaContentModel,\n",
    "    WikipediaProcessingResult,\n",
    ")\n",
    "from nyc_landmarks.vectordb.pinecone_db import pinecone\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Initialize the components\n",
    "api_client = CoreDataStoreAPI()\n",
    "wiki_fetcher = WikipediaFetcher()\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "pinecone_db = pinecone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring Landmark Data\n",
    "\n",
    "Let's start by fetching some landmarks from the CoreDataStore API and explore the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a small number of landmarks for exploration\n",
    "landmarks = api_client.get_all_landmarks(limit=10)\n",
    "\n",
    "# Create a DataFrame for easier viewing\n",
    "landmarks_df = pd.DataFrame(landmarks)\n",
    "landmarks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving Wikipedia Articles for Landmarks\n",
    "\n",
    "Now let's check which landmarks have associated Wikipedia articles and examine their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and display Wikipedia articles for a landmark\n",
    "\n",
    "\n",
    "def check_wikipedia_articles(landmark_id: str) -> List[WikipediaArticleModel]:\n",
    "    \"\"\"Check if a landmark has associated Wikipedia articles.\n",
    "\n",
    "    Args:\n",
    "        landmark_id: ID of the landmark to check\n",
    "\n",
    "    Returns:\n",
    "        List of WikipediaArticleModel objects\n",
    "    \"\"\"\n",
    "    articles = api_client.get_wikipedia_articles(landmark_id)\n",
    "    print(f\"Found {len(articles)} Wikipedia articles for landmark: {landmark_id}\")\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Check Wikipedia articles for each landmark\n",
    "landmark_articles = {}\n",
    "for landmark in landmarks:\n",
    "    landmark_id = landmark[\"id\"]\n",
    "    name = landmark[\"name\"]\n",
    "    print(f\"Checking {name} ({landmark_id})...\")\n",
    "    articles = check_wikipedia_articles(landmark_id)\n",
    "    if articles:\n",
    "        landmark_articles[landmark_id] = articles\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(landmark_articles)} landmarks with Wikipedia articles out of {len(landmarks)} total\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Wikipedia articles we found\n",
    "if landmark_articles:\n",
    "    # Extract landmark ID, name, article title, and URL into a list of dictionaries\n",
    "    articles_data = []\n",
    "    for landmark_id, articles in landmark_articles.items():\n",
    "        landmark_name = next(\n",
    "            (l[\"name\"] for l in landmarks if l[\"id\"] == landmark_id), \"Unknown\"\n",
    "        )\n",
    "        for article in articles:\n",
    "            articles_data.append(\n",
    "                {\n",
    "                    \"landmark_id\": landmark_id,\n",
    "                    \"landmark_name\": landmark_name,\n",
    "                    \"article_title\": article.title,\n",
    "                    \"article_url\": article.url,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create a DataFrame for easier viewing\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "    articles_df\n",
    "else:\n",
    "    print(\"No landmarks with Wikipedia articles found in the sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fetching and Processing Wikipedia Content\n",
    "\n",
    "Now let's fetch the actual content from a Wikipedia article and process it for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a landmark with Wikipedia articles for testing\n",
    "test_landmark_id = next(iter(landmark_articles.keys())) if landmark_articles else None\n",
    "\n",
    "if test_landmark_id:\n",
    "    test_articles = landmark_articles[test_landmark_id]\n",
    "    test_article = test_articles[0]  # Just take the first article for the test\n",
    "\n",
    "    print(f\"Testing with landmark {test_landmark_id}, article: {test_article.title}\")\n",
    "\n",
    "    # Fetch the article content\n",
    "    print(\"Fetching Wikipedia content...\")\n",
    "    content = wiki_fetcher.fetch_wikipedia_content(test_article.url)\n",
    "\n",
    "    if content:\n",
    "        print(f\"Successfully fetched Wikipedia content ({len(content)} chars)\")\n",
    "        print(\"\\nPreview of the content:\")\n",
    "        print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    else:\n",
    "        print(\"Failed to fetch Wikipedia content\")\n",
    "else:\n",
    "    print(\"No landmarks with Wikipedia articles found for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the article content (chunking)\n",
    "if test_landmark_id and content:\n",
    "    print(\"Chunking Wikipedia content...\")\n",
    "    chunks = wiki_fetcher.chunk_wikipedia_text(\n",
    "        content, chunk_size=1000, chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    print(f\"Split Wikipedia article into {len(chunks)} chunks\")\n",
    "\n",
    "    # Display the first chunk\n",
    "    if chunks:\n",
    "        print(\"\\nFirst chunk:\")\n",
    "        print(f\"Chunk index: {chunks[0]['chunk_index']}\")\n",
    "        print(f\"Content: {chunks[0]['text'][:300]}...\")\n",
    "        print(f\"Metadata: {chunks[0]['metadata']}\")\n",
    "\n",
    "    # Enhance chunks with article metadata\n",
    "    for chunk in chunks:\n",
    "        chunk[\"metadata\"][\"article_title\"] = test_article.title\n",
    "        chunk[\"metadata\"][\"article_url\"] = test_article.url\n",
    "        chunk[\"metadata\"][\"source_type\"] = \"wikipedia\"\n",
    "        chunk[\"metadata\"][\"landmark_id\"] = test_landmark_id\n",
    "\n",
    "    print(\"\\nEnhanced first chunk metadata:\")\n",
    "    print(chunks[0][\"metadata\"] if chunks else \"No chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Wikipedia Content\n",
    "\n",
    "Now let's generate embeddings for the Wikipedia chunks using OpenAI's embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the chunks\n",
    "if test_landmark_id and chunks:\n",
    "    print(\"Generating embeddings for Wikipedia chunks...\")\n",
    "\n",
    "    # To avoid excessive API calls in testing, let's just use the first few chunks\n",
    "    test_chunks = chunks[:2] if len(chunks) > 2 else chunks\n",
    "\n",
    "    # Generate embeddings\n",
    "    chunks_with_embeddings = embedding_generator.process_chunks(test_chunks)\n",
    "\n",
    "    print(f\"Generated embeddings for {len(chunks_with_embeddings)} chunks\")\n",
    "\n",
    "    # Check the structure of a processed chunk\n",
    "    if chunks_with_embeddings:\n",
    "        print(\"\\nProcessed chunk keys:\")\n",
    "        print(chunks_with_embeddings[0].keys())\n",
    "\n",
    "        print(\"\\nEmbedding dimensions:\")\n",
    "        print(len(chunks_with_embeddings[0][\"embedding\"]))\n",
    "\n",
    "        print(\"\\nEmbedding preview (first 5 values):\")\n",
    "        print(chunks_with_embeddings[0][\"embedding\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storing Wikipedia Embeddings in Pinecone\n",
    "\n",
    "Now let's store the embeddings in Pinecone with appropriate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in Pinecone\n",
    "if test_landmark_id and chunks_with_embeddings:\n",
    "    print(\"Storing Wikipedia embeddings in Pinecone...\")\n",
    "\n",
    "    # Store with deterministic IDs (with wiki- prefix to distinguish from PDF chunks)\n",
    "    vector_ids = pinecone_db.store_chunks(\n",
    "        chunks=chunks_with_embeddings,\n",
    "        id_prefix=f\"wiki-{test_landmark_id}-{test_article.title.replace(' ', '_')}-\",\n",
    "        landmark_id=test_landmark_id,\n",
    "        use_fixed_ids=True,\n",
    "        delete_existing=True,  # Delete existing vectors for this landmark/article\n",
    "    )\n",
    "\n",
    "    print(f\"Stored {len(vector_ids)} vectors in Pinecone\")\n",
    "    print(f\"Vector IDs: {vector_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Querying Wikipedia Content from Pinecone\n",
    "\n",
    "Now let's test querying the vector database to retrieve Wikipedia content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test query\n",
    "if test_landmark_id:\n",
    "    print(\"Generating test query...\")\n",
    "\n",
    "    # Create a test query based on the landmark name\n",
    "    landmark_name = next(\n",
    "        (l[\"name\"] for l in landmarks if l[\"id\"] == test_landmark_id), \"landmark\"\n",
    "    )\n",
    "    test_query = f\"What is the history of {landmark_name}?\"\n",
    "\n",
    "    print(f\"Test query: {test_query}\")\n",
    "\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_generator.generate_embedding(test_query)\n",
    "\n",
    "    print(\"\\nQuerying Pinecone...\")\n",
    "\n",
    "    # Query Pinecone with different filter options\n",
    "    # 1. No filter\n",
    "    results_no_filter = pinecone_db.query_vectors(query_embedding, top_k=3)\n",
    "\n",
    "    # 2. Filter by landmark ID\n",
    "    results_landmark_filter = pinecone_db.query_vectors(\n",
    "        query_embedding, top_k=3, filter_dict={\"landmark_id\": test_landmark_id}\n",
    "    )\n",
    "\n",
    "    # 3. Filter by source type (wikipedia)\n",
    "    results_wiki_filter = pinecone_db.query_vectors(\n",
    "        query_embedding, top_k=3, filter_dict={\"source_type\": \"wikipedia\"}\n",
    "    )\n",
    "\n",
    "    # 4. Combined filter (landmark ID and source type)\n",
    "    results_combined_filter = pinecone_db.query_vectors(\n",
    "        query_embedding,\n",
    "        top_k=3,\n",
    "        filter_dict={\"landmark_id\": test_landmark_id, \"source_type\": \"wikipedia\"},\n",
    "    )\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"\\nQuery results with no filter: {len(results_no_filter)} matches\")\n",
    "    print(f\"Query results with landmark filter: {len(results_landmark_filter)} matches\")\n",
    "    print(f\"Query results with wiki filter: {len(results_wiki_filter)} matches\")\n",
    "    print(f\"Query results with combined filter: {len(results_combined_filter)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Wikipedia query results\n",
    "\n",
    "\n",
    "def display_query_results(results, title):\n",
    "    \"\"\"Display query results in a formatted way.\n",
    "\n",
    "    Args:\n",
    "        results: List of query result dictionaries\n",
    "        title: Title for the results section\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title} ({len(results)} results)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Result {i+1} - Score: {result['score']:.4f}\")\n",
    "        print(f\"Source: {result['metadata'].get('source_type', 'unknown')}\")\n",
    "\n",
    "        # Display article info if available\n",
    "        if \"article_title\" in result[\"metadata\"]:\n",
    "            print(f\"Article: {result['metadata']['article_title']}\")\n",
    "\n",
    "        # Display landmark info\n",
    "        print(f\"Landmark: {result['metadata'].get('landmark_id', 'unknown')}\")\n",
    "\n",
    "        # Display text content (truncated for clarity)\n",
    "        text = result[\"metadata\"].get(\"text\", \"\")\n",
    "        if text:\n",
    "            preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "            print(f\"\\nContent: {preview}\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# Display the combined filter results (most relevant for our test)\n",
    "if \"results_combined_filter\" in locals() and results_combined_filter:\n",
    "    display_query_results(\n",
    "        results_combined_filter, \"Wikipedia Results (Landmark + Wiki Filter)\"\n",
    "    )\n",
    "elif \"results_wiki_filter\" in locals() and results_wiki_filter:\n",
    "    display_query_results(results_wiki_filter, \"Wikipedia Results (Wiki Filter Only)\")\n",
    "else:\n",
    "    print(\"No Wikipedia results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. End-to-End Wikipedia Processing Test\n",
    "\n",
    "Now let's test the complete flow using the processing functions from the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the processing function from our script\n",
    "from scripts.process_wikipedia_articles import process_landmark_wikipedia\n",
    "\n",
    "# Process Wikipedia articles for a specific landmark\n",
    "if test_landmark_id:\n",
    "    print(f\"Processing Wikipedia articles for landmark: {test_landmark_id}\")\n",
    "\n",
    "    # Process the landmark's Wikipedia articles\n",
    "    result = process_landmark_wikipedia(\n",
    "        landmark_id=test_landmark_id,\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        recreate_index=False,\n",
    "        delete_existing=True,\n",
    "    )\n",
    "\n",
    "    if result:\n",
    "        print(f\"\\nProcessing summary: {str(result)}\")\n",
    "    else:\n",
    "        print(\"Processing failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyzing Wikipedia Coverage and Distribution\n",
    "\n",
    "Let's analyze the coverage of Wikipedia articles across the landmarks in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a larger set of landmarks for analysis\n",
    "print(\"Fetching landmarks for analysis...\")\n",
    "all_landmarks = api_client.get_all_landmarks(limit=50)  # Adjust limit as needed\n",
    "print(f\"Fetched {len(all_landmarks)} landmarks\")\n",
    "\n",
    "# Get landmarks with Wikipedia articles\n",
    "print(\"\\nChecking for Wikipedia articles...\")\n",
    "landmarks_with_wikipedia = {}\n",
    "for landmark in all_landmarks:\n",
    "    landmark_id = landmark[\"id\"]\n",
    "    articles = api_client.get_wikipedia_articles(landmark_id)\n",
    "    if articles:\n",
    "        landmarks_with_wikipedia[landmark_id] = articles\n",
    "    time.sleep(0.5)  # Add a small delay to avoid rate limiting\n",
    "\n",
    "print(\n",
    "    f\"Found {len(landmarks_with_wikipedia)} landmarks with Wikipedia articles out of {len(all_landmarks)} total\"\n",
    ")\n",
    "\n",
    "# Calculate coverage percentage\n",
    "coverage_percentage = (\n",
    "    (len(landmarks_with_wikipedia) / len(all_landmarks)) * 100 if all_landmarks else 0\n",
    ")\n",
    "print(f\"Wikipedia coverage: {coverage_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Wikipedia articles per landmark\n",
    "if landmarks_with_wikipedia:\n",
    "    articles_per_landmark = [\n",
    "        len(articles) for articles in landmarks_with_wikipedia.values()\n",
    "    ]\n",
    "\n",
    "    # Calculate statistics\n",
    "    avg_articles = sum(articles_per_landmark) / len(articles_per_landmark)\n",
    "    max_articles = max(articles_per_landmark)\n",
    "    min_articles = min(articles_per_landmark)\n",
    "\n",
    "    print(f\"Articles per landmark statistics:\")\n",
    "    print(f\"Average: {avg_articles:.2f}\")\n",
    "    print(f\"Maximum: {max_articles}\")\n",
    "    print(f\"Minimum: {min_articles}\")\n",
    "\n",
    "    # Create a distribution histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(\n",
    "        articles_per_landmark,\n",
    "        bins=range(1, max_articles + 2),\n",
    "        alpha=0.7,\n",
    "        color=\"skyblue\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    plt.xlabel(\"Number of Wikipedia Articles\")\n",
    "    plt.ylabel(\"Number of Landmarks\")\n",
    "    plt.title(\"Distribution of Wikipedia Articles per Landmark\")\n",
    "    plt.grid(axis=\"y\", alpha=0.75)\n",
    "    plt.xticks(range(1, max_articles + 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze landmark attributes that might correlate with Wikipedia coverage\n",
    "# Create a DataFrame with landmark details and Wikipedia status\n",
    "landmark_analysis_data = []\n",
    "for landmark in all_landmarks:\n",
    "    landmark_id = landmark[\"id\"]\n",
    "    has_wikipedia = landmark_id in landmarks_with_wikipedia\n",
    "    wikipedia_articles_count = (\n",
    "        len(landmarks_with_wikipedia.get(landmark_id, [])) if has_wikipedia else 0\n",
    "    )\n",
    "\n",
    "    landmark_analysis_data.append(\n",
    "        {\n",
    "            \"id\": landmark_id,\n",
    "            \"name\": landmark[\"name\"],\n",
    "            \"borough\": landmark[\"borough\"],\n",
    "            \"type\": landmark[\"type\"],\n",
    "            \"has_wikipedia\": has_wikipedia,\n",
    "            \"wikipedia_articles_count\": wikipedia_articles_count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "landmark_analysis_df = pd.DataFrame(landmark_analysis_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "landmark_analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Wikipedia coverage by borough\n",
    "if not landmark_analysis_df.empty:\n",
    "    # Group by borough and calculate percentage with Wikipedia\n",
    "    borough_analysis = (\n",
    "        landmark_analysis_df.groupby(\"borough\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"has_wikipedia\": \"mean\",  # Average of True/False gives percentage\n",
    "                \"id\": \"count\",  # Count total landmarks in borough\n",
    "                \"wikipedia_articles_count\": \"sum\",  # Total Wikipedia articles\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    borough_analysis.columns = [\n",
    "        \"Borough\",\n",
    "        \"Wikipedia Coverage\",\n",
    "        \"Total Landmarks\",\n",
    "        \"Total Wikipedia Articles\",\n",
    "    ]\n",
    "\n",
    "    # Convert coverage to percentage\n",
    "    borough_analysis[\"Wikipedia Coverage\"] = (\n",
    "        borough_analysis[\"Wikipedia Coverage\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate articles per landmark\n",
    "    borough_analysis[\"Articles per Landmark\"] = (\n",
    "        borough_analysis[\"Total Wikipedia Articles\"]\n",
    "        / borough_analysis[\"Total Landmarks\"]\n",
    "    )\n",
    "\n",
    "    # Sort by coverage percentage\n",
    "    borough_analysis = borough_analysis.sort_values(\n",
    "        \"Wikipedia Coverage\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Display the analysis\n",
    "    borough_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Wikipedia coverage by borough\n",
    "if \"borough_analysis\" in locals() and not borough_analysis.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create bar chart\n",
    "    bars = plt.bar(\n",
    "        borough_analysis[\"Borough\"],\n",
    "        borough_analysis[\"Wikipedia Coverage\"],\n",
    "        color=\"skyblue\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.1f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=0,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Borough\")\n",
    "    plt.ylabel(\"Wikipedia Coverage (%)\")\n",
    "    plt.title(\"Wikipedia Coverage by Borough\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "This notebook has demonstrated the process of integrating Wikipedia articles for NYC landmarks into the vector database. We've seen how to:\n",
    "\n",
    "1. Fetch landmark information and associated Wikipedia articles\n",
    "2. Process Wikipedia content by fetching, cleaning, and chunking\n",
    "3. Generate embeddings for the article chunks\n",
    "4. Store the embeddings in Pinecone with appropriate metadata\n",
    "5. Query the vector database to retrieve Wikipedia content\n",
    "6. Analyze the coverage and distribution of Wikipedia articles across the landmarks\n",
    "\n",
    "The implementation successfully extends the existing PDF-based vector database to include Wikipedia content, which will provide additional context and information for the vector search and chat functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
