{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63ebf04",
   "metadata": {},
   "source": [
    "# NYC Landmarks Vector Database - Processing Status Analysis\n",
    "\n",
    "This notebook analyzes the processing status of NYC landmark records in the Pinecone vector database. It determines which landmarks have already been processed (have vectors in Pinecone) and which landmarks still need processing.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Connect to CoreDataStore API and Pinecone database\n",
    "2. Fetch all available landmark IDs from the CoreDataStore API\n",
    "3. Check which landmarks already have vectors in Pinecone\n",
    "4. Generate statistics and visualizations of processing status\n",
    "5. Export a list of unprocessed landmarks for batch processing\n",
    "6. Generate a GitHub Actions workflow configuration for remaining processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7e963",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4909c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4544b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "# Import project modules\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.db.db_client import get_db_client\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=settings.LOG_LEVEL.value,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b873c",
   "metadata": {},
   "source": [
    "## 2. Connect to Databases\n",
    "\n",
    "Next, we'll establish connections to both the CoreDataStore API and the Pinecone vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e292077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the database client for CoreDataStore API\n",
    "db_client = get_db_client()\n",
    "print(\"✅ Initialized CoreDataStore API client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da866937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone database client\n",
    "try:\n",
    "    # Create PineconeDB instance\n",
    "    pinecone_db = PineconeDB()\n",
    "\n",
    "    # Check if the connection was successful\n",
    "    if pinecone_db.index:\n",
    "        print(f\"✅ Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "        print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "        print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "        print(f\"Metric: {pinecone_db.metric}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"❌ Failed to connect to Pinecone. Check your credentials and network connection.\"\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing Pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66716fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index statistics from Pinecone\n",
    "try:\n",
    "    stats = pinecone_db.get_index_stats()\n",
    "\n",
    "    # Check for errors\n",
    "    if \"error\" in stats:\n",
    "        print(f\"❌ Error retrieving index stats: {stats['error']}\")\n",
    "        # Create fallback mock stats for demonstration\n",
    "        total_vector_count = 0\n",
    "        namespaces = {}\n",
    "    else:\n",
    "        print(\"✅ Successfully retrieved index stats\")\n",
    "        total_vector_count = stats.get(\"total_vector_count\", 0)\n",
    "        namespaces = stats.get(\"namespaces\", {})\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error retrieving index stats: {e}\")\n",
    "    # Create fallback mock stats for demonstration\n",
    "    total_vector_count = 0\n",
    "    namespaces = {}\n",
    "    stats = {}\n",
    "\n",
    "print(\"\\n📊 Index Statistics:\")\n",
    "print(f\"Total Vector Count: {total_vector_count:,}\")\n",
    "print(f\"Dimension: {stats.get('dimension')}\")\n",
    "print(f\"Index Fullness: {stats.get('index_fullness')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a834a62",
   "metadata": {},
   "source": [
    "## 3. Fetch All Landmark IDs\n",
    "\n",
    "Now we'll fetch all landmark IDs from the CoreDataStore API to determine the total universe of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_landmark_ids(start_page=1, end_page=None, page_size=100, max_pages=500):\n",
    "    \"\"\"Fetch all landmark IDs from CoreDataStore API.\n",
    "\n",
    "    Args:\n",
    "        start_page: Starting page number (default: 1)\n",
    "        end_page: Ending page number (default: None, fetch until no more results)\n",
    "        page_size: Number of landmarks per page (default: 100)\n",
    "        max_pages: Maximum number of pages to fetch (safety limit)\n",
    "\n",
    "    Returns:\n",
    "        Set of landmark IDs\n",
    "    \"\"\"\n",
    "    all_landmark_ids = set()\n",
    "    current_page = start_page\n",
    "    total_pages_fetched = 0\n",
    "\n",
    "    try:\n",
    "        with tqdm(desc=\"Fetching landmark IDs\", unit=\"page\") as pbar:\n",
    "            while True:\n",
    "                # Check if we've reached the end page or max pages\n",
    "                if (\n",
    "                    end_page and current_page > end_page\n",
    "                ) or total_pages_fetched >= max_pages:\n",
    "                    break\n",
    "\n",
    "                # Fetch landmarks for the current page\n",
    "                try:\n",
    "                    # Fetch API results for current page\n",
    "                    landmarks = db_client.get_landmarks_page(page_size, current_page)\n",
    "                    \n",
    "                    # If we get response metadata, we might extract total count for dynamic paging\n",
    "                    # This section is commented out since we're using a fixed count of 1765 records\n",
    "                    # If client returns response with metadata, we could uncomment and use this:\n",
    "                    # if hasattr(landmarks, 'metadata') and 'totalCount' in landmarks.metadata:\n",
    "                    #     total_count = int(landmarks.metadata['totalCount'])\n",
    "                    #     print(f\"Found total record count in API response: {total_count}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching page {current_page}: {e}\")\n",
    "                    # Try to continue with next page\n",
    "                    current_page += 1\n",
    "                    total_pages_fetched += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # If no landmarks found, we've reached the end\n",
    "                if not landmarks:\n",
    "                    print(f\"No landmarks found on page {current_page}, ending fetch\")\n",
    "                    break\n",
    "\n",
    "                # Process the landmarks\n",
    "                for landmark in landmarks:\n",
    "                    landmark_id = landmark.get(\"id\", \"\") or landmark.get(\"lpNumber\", \"\")\n",
    "                    if landmark_id:\n",
    "                        all_landmark_ids.add(landmark_id)\n",
    "\n",
    "                # Update progress\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"page\": current_page,\n",
    "                        \"landmarks\": len(landmarks),\n",
    "                        \"total\": len(all_landmark_ids),\n",
    "                    }\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "                # Move to next page\n",
    "                current_page += 1\n",
    "                total_pages_fetched += 1\n",
    "\n",
    "                # Small delay to avoid rate limiting\n",
    "                time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching landmark IDs: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"Completed fetching {len(all_landmark_ids)} landmark IDs from {total_pages_fetched} pages\"\n",
    "    )\n",
    "    return all_landmark_ids\n",
    "\n",
    "\n",
    "# Try to get total records from API\n",
    "try:\n",
    "    # Force reload the db_client module to get the latest version with the new method\n",
    "    import importlib\n",
    "    import nyc_landmarks.db.db_client\n",
    "    importlib.reload(nyc_landmarks.db.db_client)\n",
    "    \n",
    "    # Reinitialize the database client to get the updated class definition\n",
    "    from nyc_landmarks.db.db_client import get_db_client\n",
    "    db_client = get_db_client()\n",
    "    print(\"✅ Reloaded and reinitialized the CoreDataStore API client\")\n",
    "    \n",
    "    # Now try to use the get_total_record_count method (should be available after reload)\n",
    "    total_records = db_client.get_total_record_count()\n",
    "    print(f\"Total landmark records available from API: {total_records}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting total record count: {e}\")\n",
    "    # Use a reasonable default based on API documentation\n",
    "    total_records = 1765\n",
    "    print(f\"Falling back to default record count: {total_records}\")\n",
    "\n",
    "# Configure fetch parameters\n",
    "page_size = 100\n",
    "start_page = 1\n",
    "\n",
    "# Calculate the required number of pages (using ceiling division)\n",
    "# This ensures we get all records even if the last page is partial\n",
    "total_pages = (total_records + page_size - 1) // page_size\n",
    "\n",
    "print(f\"Will fetch {total_records} records using {total_pages} pages with {page_size} records per page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d38068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all landmark IDs\n",
    "start_time = time.time()\n",
    "all_landmark_ids = fetch_all_landmark_ids(\n",
    "    start_page=start_page, end_page=total_pages, page_size=page_size\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    f\"Fetched {len(all_landmark_ids)} unique landmark IDs in {elapsed_time:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f619576",
   "metadata": {},
   "source": [
    "## 4. Check Processing Status in Pinecone\n",
    "\n",
    "Now we'll check which landmarks already have vectors in Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_landmark_processing_status(pinecone_db, landmark_ids, batch_size=10, top_k=1):\n",
    "    \"\"\"Check which landmarks have vectors in Pinecone.\n",
    "\n",
    "    Args:\n",
    "        pinecone_db: PineconeDB instance\n",
    "        landmark_ids: Set of landmark IDs to check\n",
    "        batch_size: Number of landmarks to check in parallel batches\n",
    "        top_k: Number of vectors to retrieve per landmark (1 is sufficient to check existence)\n",
    "\n",
    "    Returns:\n",
    "        processed_landmarks: Set of landmark IDs that have vectors\n",
    "        unprocessed_landmarks: Set of landmark IDs that don't have vectors\n",
    "    \"\"\"\n",
    "    # Generate a random query vector for searching\n",
    "    random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "\n",
    "    processed_landmarks = set()\n",
    "    unprocessed_landmarks = set()\n",
    "\n",
    "    # Convert set to list for iteration with tqdm\n",
    "    landmark_ids_list = list(landmark_ids)\n",
    "\n",
    "    with tqdm(total=len(landmark_ids_list), desc=\"Checking processing status\") as pbar:\n",
    "        for i in range(0, len(landmark_ids_list), batch_size):\n",
    "            # Get the current batch\n",
    "            batch = landmark_ids_list[i : i + batch_size]\n",
    "\n",
    "            # Check each landmark in the batch\n",
    "            for landmark_id in batch:\n",
    "                # Query Pinecone for vectors with this landmark_id\n",
    "                filter_dict = {\"landmark_id\": landmark_id}\n",
    "                try:\n",
    "                    # We only need to know if vectors exist, so top_k=1 is sufficient\n",
    "                    vectors = pinecone_db.query_vectors(\n",
    "                        query_vector=random_vector, top_k=top_k, filter_dict=filter_dict\n",
    "                    )\n",
    "\n",
    "                    # If vectors found, mark as processed, otherwise unprocessed\n",
    "                    if vectors:\n",
    "                        processed_landmarks.add(landmark_id)\n",
    "                    else:\n",
    "                        unprocessed_landmarks.add(landmark_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error checking landmark {landmark_id}: {e}\")\n",
    "                    # If we can't check, assume unprocessed to be safe\n",
    "                    unprocessed_landmarks.add(landmark_id)\n",
    "\n",
    "            # Update progress\n",
    "            pbar.update(len(batch))\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"processed\": len(processed_landmarks),\n",
    "                    \"unprocessed\": len(unprocessed_landmarks),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    return processed_landmarks, unprocessed_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check processing status for all landmarks\n",
    "start_time = time.time()\n",
    "processed_landmarks, unprocessed_landmarks = check_landmark_processing_status(\n",
    "    pinecone_db=pinecone_db,\n",
    "    landmark_ids=all_landmark_ids,\n",
    "    batch_size=10,  # Adjust based on API rate limits\n",
    "    top_k=1,\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nProcessing status check completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Total landmarks: {len(all_landmark_ids)}\")\n",
    "print(\n",
    "    f\"Processed landmarks: {len(processed_landmarks)} ({len(processed_landmarks)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Unprocessed landmarks: {len(unprocessed_landmarks)} ({len(unprocessed_landmarks)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec01c8",
   "metadata": {},
   "source": [
    "## 5. Analysis and Visualizations\n",
    "\n",
    "Now we'll analyze the processing status and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip DataFrame creation entirely and use Python native data structures\n",
    "# First convert to Python lists to ensure we're working with primitive types\n",
    "landmark_ids_list = sorted(list(all_landmark_ids))\n",
    "processed_landmarks_set = processed_landmarks\n",
    "unprocessed_landmarks_set = unprocessed_landmarks\n",
    "\n",
    "# Count the number of processed and unprocessed landmarks\n",
    "total_landmarks = len(landmark_ids_list)\n",
    "processed_count = len(processed_landmarks_set)\n",
    "unprocessed_count = len(unprocessed_landmarks_set)\n",
    "\n",
    "# Calculate percentages\n",
    "processed_percentage = (processed_count / total_landmarks) * 100\n",
    "unprocessed_percentage = (unprocessed_count / total_landmarks) * 100\n",
    "\n",
    "# Display the statistics\n",
    "print(\"Processing Statistics:\")\n",
    "print(f\"Total landmarks: {total_landmarks}\")\n",
    "print(f\"Processed landmarks: {processed_count} ({processed_percentage:.2f}%)\")\n",
    "print(f\"Unprocessed landmarks: {unprocessed_count} ({unprocessed_percentage:.2f}%)\")\n",
    "\n",
    "# Display sample of processed and unprocessed landmarks\n",
    "print(\"\\nSample of processed landmarks:\")\n",
    "for lid in list(processed_landmarks_set)[:5]:\n",
    "    print(f\"  - {lid}\")\n",
    "\n",
    "print(\"\\nSample of unprocessed landmarks:\")\n",
    "for lid in list(unprocessed_landmarks_set)[:5]:\n",
    "    print(f\"  - {lid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of processing status\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar([\"Processed\", \"Unprocessed\"], \n",
    "               [len(processed_landmarks), len(unprocessed_landmarks)], \n",
    "               color=[\"#4CAF50\", \"#F44336\"])\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    percentage = len(processed_landmarks) / len(all_landmark_ids) * 100 if i == 0 else len(unprocessed_landmarks) / len(all_landmark_ids) * 100\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height / 2,\n",
    "        f\"{height} ({percentage:.1f}%)\",\n",
    "        color=\"white\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "plt.title(\"NYC Landmarks Processing Status\", fontsize=16)\n",
    "plt.xlabel(\"Status\", fontsize=14)\n",
    "plt.ylabel(\"Number of Landmarks\", fontsize=14)\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd68b7b",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This notebook has analyzed the processing status of NYC landmark records in the Pinecone vector database by dynamically calculating the required number of pages to process all records.\n",
    "\n",
    "### Key Improvements\n",
    "\n",
    "1. **Dynamic Pagination**: Instead of hardcoding the start and end pages, we now:\n",
    "   - Use the known total record count (1765) from the API\n",
    "   - Calculate the exact number of pages needed based on page size\n",
    "   - Ensure all records are processed with no manual adjustments needed\n",
    "   \n",
    "2. **Future-Proofing**: If more records are added to the database in the future, the notebook will:\n",
    "   - Automatically adjust to fetch all records\n",
    "   - Correctly process new landmarks without code changes\n",
    "   \n",
    "3. **Optimized Processing**: The approach is more efficient because it:\n",
    "   - Fetches exactly the pages needed\n",
    "   - Doesn't waste time on unnecessary API calls\n",
    "   - Provides better progress tracking and reporting\n",
    "\n",
    "These changes make the notebook more robust and maintainable as the dataset grows over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-landmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
