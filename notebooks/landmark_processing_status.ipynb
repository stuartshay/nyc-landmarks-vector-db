{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NYC Landmarks Vector Database - Processing Status Analysis\n",
    "\n",
    "This notebook analyzes the processing status of NYC landmark records in the Pinecone vector database. It determines which landmarks have already been processed (have vectors in Pinecone) and which landmarks still need processing.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Connect to CoreDataStore API and Pinecone database\n",
    "2. Fetch all available landmark IDs from the CoreDataStore API\n",
    "3. Check which landmarks already have vectors in Pinecone\n",
    "4. Generate statistics and visualizations of processing status\n",
    "5. Export a list of unprocessed landmarks for batch processing\n",
    "6. Analysis and Visualizations\n",
    "7. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "# Import project modules\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.db.db_client import get_db_client\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=settings.LOG_LEVEL.value,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Connect to Databases\n",
    "\n",
    "Next, we'll establish connections to both the CoreDataStore API and the Pinecone vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the database client for CoreDataStore API\n",
    "db_client = get_db_client()\n",
    "print(\"✅ Initialized CoreDataStore API client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone database client\n",
    "try:\n",
    "    # Create PineconeDB instance\n",
    "    pinecone_db = PineconeDB()\n",
    "\n",
    "    # Check if the connection was successful\n",
    "    if pinecone_db.index:\n",
    "        print(f\"✅ Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "        print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "        print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"❌ Failed to connect to Pinecone. Check your credentials and network connection.\"\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing Pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index statistics from Pinecone\n",
    "try:\n",
    "    stats = pinecone_db.get_index_stats()\n",
    "\n",
    "    # Check for errors\n",
    "    if \"error\" in stats:\n",
    "        print(f\"❌ Error retrieving index stats: {stats['error']}\")\n",
    "        # Create fallback mock stats for demonstration\n",
    "        total_vector_count = 0\n",
    "        namespaces = {}\n",
    "    else:\n",
    "        print(\"✅ Successfully retrieved index stats\")\n",
    "        total_vector_count = stats.get(\"total_vector_count\", 0)\n",
    "        namespaces = stats.get(\"namespaces\", {})\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error retrieving index stats: {e}\")\n",
    "    # Create fallback mock stats for demonstration\n",
    "    total_vector_count = 0\n",
    "    namespaces = {}\n",
    "    stats = {}\n",
    "\n",
    "print(\"\\n📊 Index Statistics:\")\n",
    "print(f\"Total Vector Count: {total_vector_count:,}\")\n",
    "print(f\"Dimension: {stats.get('dimension')}\")\n",
    "print(f\"Index Fullness: {stats.get('index_fullness')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Fetch All Landmark IDs\n",
    "\n",
    "Now we'll fetch all landmark IDs from the CoreDataStore API to determine the total universe of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_landmark_ids(\n",
    "    start_page: int = 1,\n",
    "    end_page: int = None,  # type: ignore\n",
    "    page_size: int = 100,\n",
    "    max_pages: int = 500,\n",
    ") -> set[str]:\n",
    "    \"\"\"Fetch all landmark IDs from CoreDataStore API.\n",
    "\n",
    "    Args:\n",
    "        start_page: Starting page number (default: 1)\n",
    "        end_page: Ending page number (default: None, fetch until no more results)\n",
    "        page_size: Number of landmarks per page (default: 100)\n",
    "        max_pages: Maximum number of pages to fetch (safety limit)\n",
    "\n",
    "    Returns:\n",
    "        Set of landmark IDs\n",
    "    \"\"\"\n",
    "    all_landmark_ids: set[str] = set()\n",
    "    current_page = start_page\n",
    "    total_pages_fetched = 0\n",
    "\n",
    "    try:\n",
    "        with tqdm(desc=\"Fetching landmark IDs\", unit=\"page\") as pbar:\n",
    "            while True:\n",
    "                # Check if we've reached the end page or max pages\n",
    "                if (\n",
    "                    end_page and current_page > end_page\n",
    "                ) or total_pages_fetched >= max_pages:\n",
    "                    break\n",
    "\n",
    "                # Fetch landmarks for the current page\n",
    "                try:\n",
    "                    # Fetch API results for current page\n",
    "                    landmarks = db_client.get_landmarks_page(page_size, current_page)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching page {current_page}: {e}\")\n",
    "                    # Try to continue with next page\n",
    "                    current_page += 1\n",
    "                    total_pages_fetched += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # If no landmarks found, we've reached the end\n",
    "                if not landmarks:\n",
    "                    print(f\"No landmarks found on page {current_page}, ending fetch\")\n",
    "                    break\n",
    "\n",
    "                # Process the landmarks\n",
    "                for landmark in landmarks:\n",
    "                    landmark_id = landmark.get(\"id\", \"\") or landmark.get(\"lpNumber\", \"\")\n",
    "                    if landmark_id:\n",
    "                        all_landmark_ids.add(landmark_id)\n",
    "\n",
    "                # Update progress\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"page\": current_page,\n",
    "                        \"landmarks\": len(landmarks),\n",
    "                        \"total\": len(all_landmark_ids),\n",
    "                    }\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "                # Move to next page\n",
    "                current_page += 1\n",
    "                total_pages_fetched += 1\n",
    "\n",
    "                # Small delay to avoid rate limiting\n",
    "                time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching landmark IDs: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"Completed fetching {len(all_landmark_ids)} landmark IDs from {total_pages_fetched} pages\"\n",
    "    )\n",
    "    return all_landmark_ids\n",
    "\n",
    "\n",
    "# Try to get total records from API\n",
    "try:\n",
    "    # Force reload the db_client module to get the latest version with the new method\n",
    "    import importlib\n",
    "\n",
    "    import nyc_landmarks.db.db_client\n",
    "\n",
    "    importlib.reload(nyc_landmarks.db.db_client)\n",
    "\n",
    "    # Reinitialize the database client to get the updated class definition\n",
    "    from nyc_landmarks.db.db_client import get_db_client\n",
    "\n",
    "    db_client = get_db_client()\n",
    "    print(\"✅ Reloaded and reinitialized the CoreDataStore API client\")\n",
    "\n",
    "    # Now try to use the get_total_record_count method (should be available after reload)\n",
    "    total_records = db_client.get_total_record_count()\n",
    "    print(f\"Total landmark records available from API: {total_records}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting total record count: {e}\")\n",
    "    # Use a reasonable default based on API documentation\n",
    "    total_records = 1765\n",
    "    print(f\"Falling back to default record count: {total_records}\")\n",
    "\n",
    "# Configure fetch parameters\n",
    "page_size = 100\n",
    "start_page = 1\n",
    "\n",
    "# Calculate the required number of pages (using ceiling division)\n",
    "# This ensures we get all records even if the last page is partial\n",
    "total_pages = (total_records + page_size - 1) // page_size\n",
    "\n",
    "print(\n",
    "    f\"Will fetch {total_records} records using {total_pages} pages with {page_size} records per page\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all landmark IDs\n",
    "start_time = time.time()\n",
    "all_landmark_ids = fetch_all_landmark_ids(\n",
    "    start_page=start_page, end_page=total_pages, page_size=page_size\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    f\"Fetched {len(all_landmark_ids)} unique landmark IDs in {elapsed_time:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 4. Check Processing Status in Pinecone\n",
    "\n",
    "Now we'll check which landmarks already have vectors in Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_landmark_processing_status_by_source(\n",
    "    pinecone_db, landmark_ids, batch_size=10, top_k=1\n",
    "):\n",
    "    \"\"\"Check which landmarks have vectors in Pinecone by source type.\n",
    "\n",
    "    Args:\n",
    "        pinecone_db: PineconeDB instance\n",
    "        landmark_ids: Set of landmark IDs to check\n",
    "        batch_size: Number of landmarks to check in parallel batches\n",
    "        top_k: Number of vectors to retrieve per landmark (1 is sufficient to check existence)\n",
    "\n",
    "    Returns:\n",
    "        dict with keys 'pdf' and 'wikipedia', each containing sets of landmark IDs\n",
    "    \"\"\"\n",
    "    # Generate a random query vector for searching\n",
    "    random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "\n",
    "    results = {\n",
    "        \"pdf\": {\"processed\": set(), \"unprocessed\": set()},\n",
    "        \"wikipedia\": {\"processed\": set(), \"unprocessed\": set()},\n",
    "    }\n",
    "\n",
    "    # Convert set to list for iteration with tqdm\n",
    "    landmark_ids_list = list(landmark_ids)\n",
    "\n",
    "    with tqdm(\n",
    "        total=len(landmark_ids_list) * 2, desc=\"Checking processing status by source\"\n",
    "    ) as pbar:\n",
    "        # Check each source type separately\n",
    "        for source_type in [\"pdf\", \"wikipedia\"]:\n",
    "            pbar.set_description(f\"Checking {source_type} vectors\")\n",
    "\n",
    "            for i in range(0, len(landmark_ids_list), batch_size):\n",
    "                # Get the current batch\n",
    "                batch = landmark_ids_list[i : i + batch_size]\n",
    "\n",
    "                # Check each landmark in the batch\n",
    "                for landmark_id in batch:\n",
    "                    # Query Pinecone for vectors with this landmark_id AND source_type\n",
    "                    filter_dict = {\n",
    "                        \"landmark_id\": landmark_id,\n",
    "                        \"source_type\": source_type,\n",
    "                    }\n",
    "                    try:\n",
    "                        # We only need to know if vectors exist, so top_k=1 is sufficient\n",
    "                        vectors = pinecone_db.query_vectors(\n",
    "                            query_vector=random_vector,\n",
    "                            top_k=top_k,\n",
    "                            filter_dict=filter_dict,\n",
    "                        )\n",
    "\n",
    "                        # If vectors found, mark as processed, otherwise unprocessed\n",
    "                        if vectors:\n",
    "                            results[source_type][\"processed\"].add(landmark_id)\n",
    "                        else:\n",
    "                            results[source_type][\"unprocessed\"].add(landmark_id)\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"Error checking {source_type} for landmark {landmark_id}: {e}\"\n",
    "                        )\n",
    "                        # If we can't check, assume unprocessed to be safe\n",
    "                        results[source_type][\"unprocessed\"].add(landmark_id)\n",
    "\n",
    "                # Update progress\n",
    "                pbar.update(len(batch))\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        f\"{source_type}_processed\": len(\n",
    "                            results[source_type][\"processed\"]\n",
    "                        ),\n",
    "                        f\"{source_type}_unprocessed\": len(\n",
    "                            results[source_type][\"unprocessed\"]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Small delay to avoid rate limiting\n",
    "                time.sleep(0.2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check processing status for all landmarks by source type\n",
    "start_time = time.time()\n",
    "processing_results = check_landmark_processing_status_by_source(\n",
    "    pinecone_db=pinecone_db,\n",
    "    landmark_ids=all_landmark_ids,\n",
    "    batch_size=10,  # Adjust based on API rate limits\n",
    "    top_k=1,\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nProcessing status check completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Total landmarks: {len(all_landmark_ids)}\")\n",
    "\n",
    "# PDF results\n",
    "pdf_processed = processing_results[\"pdf\"][\"processed\"]\n",
    "pdf_unprocessed = processing_results[\"pdf\"][\"unprocessed\"]\n",
    "print(\"\\n📄 PDF Processing Status:\")\n",
    "print(\n",
    "    f\"  Processed landmarks: {len(pdf_processed)} ({len(pdf_processed)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Unprocessed landmarks: {len(pdf_unprocessed)} ({len(pdf_unprocessed)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "\n",
    "# Wikipedia results\n",
    "wiki_processed = processing_results[\"wikipedia\"][\"processed\"]\n",
    "wiki_unprocessed = processing_results[\"wikipedia\"][\"unprocessed\"]\n",
    "print(\"\\n📖 Wikipedia Processing Status:\")\n",
    "print(\n",
    "    f\"  Processed landmarks: {len(wiki_processed)} ({len(wiki_processed)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Unprocessed landmarks: {len(wiki_unprocessed)} ({len(wiki_unprocessed)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "\n",
    "# Analysis of coverage\n",
    "landmarks_with_both = pdf_processed & wiki_processed\n",
    "landmarks_with_pdf_only = pdf_processed - wiki_processed\n",
    "landmarks_with_wiki_only = wiki_processed - pdf_processed\n",
    "landmarks_with_neither = pdf_unprocessed & wiki_unprocessed\n",
    "\n",
    "print(\"\\n🔍 Coverage Analysis:\")\n",
    "print(\n",
    "    f\"  Landmarks with both PDF and Wikipedia: {len(landmarks_with_both)} ({len(landmarks_with_both)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Landmarks with PDF only: {len(landmarks_with_pdf_only)} ({len(landmarks_with_pdf_only)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Landmarks with Wikipedia only: {len(landmarks_with_wiki_only)} ({len(landmarks_with_wiki_only)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Landmarks with neither: {len(landmarks_with_neither)} ({len(landmarks_with_neither)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "\n",
    "# Identify landmarks that should have PDFs but don't\n",
    "print(\"\\n⚠️  Landmarks missing PDF content (should be investigated):\")\n",
    "if pdf_unprocessed:\n",
    "    missing_pdf_sample = sorted(pdf_unprocessed)[:10]  # Show first 10\n",
    "    for landmark_id in missing_pdf_sample:\n",
    "        print(f\"  - {landmark_id}\")\n",
    "    if len(pdf_unprocessed) > 10:\n",
    "        print(f\"  ... and {len(pdf_unprocessed) - 10} more\")\n",
    "else:\n",
    "    print(\"  ✅ All landmarks have PDF content!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Export a list of unprocessed landmarks for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export landmarks that need PDF processing (should be investigated as every landmark should have PDF)\n",
    "pdf_missing_landmarks = sorted(processing_results[\"pdf\"][\"unprocessed\"]))\n",
    "print(f\"\\n📋 Landmarks missing PDF content ({len(pdf_missing_landmarks)} total):\")\n",
    "if pdf_missing_landmarks:\n",
    "    print(\"First 20:\")\n",
    "    for landmark_id in pdf_missing_landmarks[:20]:\n",
    "        print(f\"  {landmark_id}\")\n",
    "    if len(pdf_missing_landmarks) > 20:\n",
    "        print(f\"  ... and {len(pdf_missing_landmarks) - 20} more\")\n",
    "\n",
    "    # Save to file for further investigation\n",
    "    import os\n",
    "\n",
    "    output_dir = \"../test_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(f\"{output_dir}/landmarks_missing_pdf.txt\", \"w\") as f:\n",
    "        f.write(\"# Landmarks missing PDF content\\n\")\n",
    "        f.write(f\"# Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"# Total count: {len(pdf_missing_landmarks)}\\n\\n\")\n",
    "        for landmark_id in pdf_missing_landmarks:\n",
    "            f.write(f\"{landmark_id}\\n\")\n",
    "    print(f\"\\n💾 Saved list to {output_dir}/landmarks_missing_pdf.txt\")\n",
    "else:\n",
    "    print(\"  ✅ All landmarks have PDF content!\")\n",
    "\n",
    "# Export landmarks that could benefit from Wikipedia processing (optional enhancement)\n",
    "wiki_missing_landmarks = sorted(processing_results[\"wikipedia\"][\"unprocessed\"]))\n",
    "print(\n",
    "    f\"\\n📋 Landmarks without Wikipedia content ({len(wiki_missing_landmarks)} total):\"\n",
    ")\n",
    "if wiki_missing_landmarks:\n",
    "    print(\"First 20:\")\n",
    "    for landmark_id in wiki_missing_landmarks[:20]:\n",
    "        print(f\"  {landmark_id}\")\n",
    "    if len(wiki_missing_landmarks) > 20:\n",
    "        print(f\"  ... and {len(wiki_missing_landmarks) - 20} more\")\n",
    "\n",
    "    # Save to file for potential Wikipedia article creation/processing\n",
    "    with open(f\"{output_dir}/landmarks_missing_wikipedia.txt\", \"w\") as f:\n",
    "        f.write(\"# Landmarks without Wikipedia content\\n\")\n",
    "        f.write(f\"# Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"# Total count: {len(wiki_missing_landmarks)}\\n\")\n",
    "        f.write(\n",
    "            \"# Note: This is optional enhancement - not all landmarks need Wikipedia articles\\n\\n\"\n",
    "        )\n",
    "        for landmark_id in wiki_missing_landmarks:\n",
    "            f.write(f\"{landmark_id}\\n\")\n",
    "    print(f\"\\n💾 Saved list to {output_dir}/landmarks_missing_wikipedia.txt\")\n",
    "else:\n",
    "    print(\"  ✅ All landmarks have Wikipedia content!\")\n",
    "\n",
    "# Export landmarks with complete coverage (both PDF and Wikipedia)\n",
    "complete_landmarks = sorted(landmarks_with_both)\n",
    "print(f\"\\n📋 Landmarks with complete coverage ({len(complete_landmarks)} total):\")\n",
    "if complete_landmarks:\n",
    "    print(\"First 10:\")\n",
    "    for landmark_id in complete_landmarks[:10]:\n",
    "        print(f\"  {landmark_id}\")\n",
    "    if len(complete_landmarks) > 10:\n",
    "        print(f\"  ... and {len(complete_landmarks) - 10} more\")\n",
    "\n",
    "    # Save to file\n",
    "    with open(f\"{output_dir}/landmarks_complete_coverage.txt\", \"w\") as f:\n",
    "        f.write(\"# Landmarks with both PDF and Wikipedia content\\n\")\n",
    "        f.write(f\"# Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"# Total count: {len(complete_landmarks)}\\n\\n\")\n",
    "        for landmark_id in complete_landmarks:\n",
    "            f.write(f\"{landmark_id}\\n\")\n",
    "    print(f\"\\n💾 Saved list to {output_dir}/landmarks_complete_coverage.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. Analysis and Visualizations\n",
    "\n",
    "Now we'll analyze the processing status and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the results from the source-type filtered processing check\n",
    "pdf_processed_count = len(processing_results[\"pdf\"][\"processed\"])\n",
    "pdf_unprocessed_count = len(processing_results[\"pdf\"][\"unprocessed\"])\n",
    "wiki_processed_count = len(processing_results[\"wikipedia\"][\"processed\"])\n",
    "wiki_unprocessed_count = len(processing_results[\"wikipedia\"][\"unprocessed\"])\n",
    "\n",
    "total_landmarks = len(all_landmark_ids)\n",
    "\n",
    "# Calculate percentages\n",
    "pdf_processed_percentage = (pdf_processed_count / total_landmarks) * 100\n",
    "pdf_unprocessed_percentage = (pdf_unprocessed_count / total_landmarks) * 100\n",
    "wiki_processed_percentage = (wiki_processed_count / total_landmarks) * 100\n",
    "wiki_unprocessed_percentage = (wiki_unprocessed_count / total_landmarks) * 100\n",
    "\n",
    "# Display detailed statistics by source type\n",
    "print(\"\\nDetailed Processing Statistics by Source Type:\")\n",
    "print(f\"Total landmarks: {total_landmarks}\")\n",
    "print(\"\\nPDF Processing:\")\n",
    "print(f\"  Processed landmarks: {pdf_processed_count} ({pdf_processed_percentage:.2f}%)\")\n",
    "print(\n",
    "    f\"  Unprocessed landmarks: {pdf_unprocessed_count} ({pdf_unprocessed_percentage:.2f}%)\"\n",
    ")\n",
    "print(\"\\nWikipedia Processing:\")\n",
    "print(\n",
    "    f\"  Processed landmarks: {wiki_processed_count} ({wiki_processed_percentage:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Unprocessed landmarks: {wiki_unprocessed_count} ({wiki_unprocessed_percentage:.2f}%)\"\n",
    ")\n",
    "\n",
    "# Display sample of landmarks by source processing status\n",
    "print(\"\\nSample of landmarks with PDF content:\")\n",
    "for lid in sorted(processing_results[\"pdf\"][\"processed\"]))[:5]:\n",
    "    print(f\"  - {lid}\")\n",
    "\n",
    "print(\"\\nSample of landmarks with Wikipedia content:\")\n",
    "for lid in sorted(processing_results[\"wikipedia\"][\"processed\"]))[:5]:\n",
    "    print(f\"  - {lid}\")\n",
    "\n",
    "print(\"\\nSample of landmarks missing PDF content:\")\n",
    "for lid in sorted(processing_results[\"pdf\"][\"unprocessed\"]))[:5]:\n",
    "    print(f\"  - {lid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of processing status by source type\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PDF processing status\n",
    "ax1.bar(\n",
    "    [\"Processed\", \"Unprocessed\"],\n",
    "    [pdf_processed_count, pdf_unprocessed_count],\n",
    "    color=[\"#4CAF50\", \"#F44336\"],\n",
    ")\n",
    "for i, (count, percentage) in enumerate(\n",
    "    [\n",
    "        (pdf_processed_count, pdf_processed_percentage),\n",
    "        (pdf_unprocessed_count, pdf_unprocessed_percentage),\n",
    "    ]\n",
    "):\n",
    "    ax1.text(\n",
    "        i,\n",
    "        count / 2,\n",
    "        f\"{count}\\n({percentage:.1f}%)\",\n",
    "        color=\"white\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "ax1.set_title(\"📄 PDF Content Processing Status\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Status\", fontsize=12)\n",
    "ax1.set_ylabel(\"Number of Landmarks\", fontsize=12)\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Wikipedia processing status\n",
    "ax2.bar(\n",
    "    [\"Processed\", \"Unprocessed\"],\n",
    "    [wiki_processed_count, wiki_unprocessed_count],\n",
    "    color=[\"#2196F3\", \"#FF9800\"],\n",
    ")\n",
    "for i, (count, percentage) in enumerate(\n",
    "    [\n",
    "        (wiki_processed_count, wiki_processed_percentage),\n",
    "        (wiki_unprocessed_count, wiki_unprocessed_percentage),\n",
    "    ]\n",
    "):\n",
    "    ax2.text(\n",
    "        i,\n",
    "        count / 2,\n",
    "        f\"{count}\\n({percentage:.1f}%)\",\n",
    "        color=\"white\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "ax2.set_title(\"📖 Wikipedia Content Processing Status\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Status\", fontsize=12)\n",
    "ax2.set_ylabel(\"Number of Landmarks\", fontsize=12)\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"NYC Landmarks Processing Status by Source Type\", fontsize=16, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define coverage analysis variables for the combined visualization\n",
    "pdf_processed = processing_results[\"pdf\"][\"processed\"]\n",
    "pdf_unprocessed = processing_results[\"pdf\"][\"unprocessed\"]\n",
    "wiki_processed = processing_results[\"wikipedia\"][\"processed\"]\n",
    "wiki_unprocessed = processing_results[\"wikipedia\"][\"unprocessed\"]\n",
    "\n",
    "landmarks_with_both = pdf_processed & wiki_processed\n",
    "landmarks_with_pdf_only = pdf_processed - wiki_processed\n",
    "landmarks_with_wiki_only = wiki_processed - pdf_processed\n",
    "landmarks_with_neither = pdf_unprocessed & wiki_unprocessed\n",
    "\n",
    "# Create a combined coverage visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "coverage_labels = [\"Both PDF & Wikipedia\", \"PDF Only\", \"Wikipedia Only\", \"Neither\"]\n",
    "coverage_counts = [\n",
    "    len(landmarks_with_both),\n",
    "    len(landmarks_with_pdf_only),\n",
    "    len(landmarks_with_wiki_only),\n",
    "    len(landmarks_with_neither),\n",
    "]\n",
    "coverage_colors = [\"#4CAF50\", \"#2196F3\", \"#FF9800\", \"#F44336\"]\n",
    "\n",
    "bars = plt.bar(coverage_labels, coverage_counts, color=coverage_colors)\n",
    "\n",
    "# Add count and percentage labels on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, coverage_counts)):\n",
    "    percentage = (count / total_landmarks) * 100\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        bar.get_height() / 2,\n",
    "        f\"{count}\\n({percentage:.1f}%)\",\n",
    "        color=\"white\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "plt.title(\"Content Coverage Analysis\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Coverage Type\", fontsize=14)\n",
    "plt.ylabel(\"Number of Landmarks\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook has analyzed the processing status of NYC landmark records in the Pinecone vector database by source type, providing separate analysis for PDF and Wikipedia content.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Source Type Filtering**: The notebook now distinguishes between:\n",
    "   - **PDF Content**: Expected for all landmarks (official LPC reports)\n",
    "   - **Wikipedia Content**: Optional enhancement, not all landmarks have Wikipedia articles\n",
    "\n",
    "2. **Comprehensive Analysis**: The status check provides:\n",
    "   - Individual processing status by source type (PDF vs Wikipedia)\n",
    "   - Coverage analysis showing landmarks with both, either, or neither source type\n",
    "   - Identification of landmarks missing expected PDF content\n",
    "   - Export of filtered lists for targeted processing\n",
    "\n",
    "3. **Dynamic Pagination**: The original approach of:\n",
    "   - Using the known total record count from the API\n",
    "   - Calculating exact pages needed based on page size\n",
    "   - Processing all records without manual adjustments\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **PDF Processing**: Shows which landmarks have their official LPC reports processed\n",
    "- **Wikipedia Processing**: Shows which landmarks have additional Wikipedia content\n",
    "- **Coverage Gaps**: Identifies landmarks that may need investigation (missing PDF content)\n",
    "- **Enhancement Opportunities**: Lists landmarks that could benefit from Wikipedia article processing\n",
    "\n",
    "### Outputs\n",
    "\n",
    "The notebook generates several files for further action:\n",
    "- `landmarks_missing_pdf.txt`: Landmarks that should be investigated (every landmark should have PDF)\n",
    "- `landmarks_missing_wikipedia.txt`: Landmarks that could benefit from Wikipedia content (optional)\n",
    "- `landmarks_complete_coverage.txt`: Landmarks with both PDF and Wikipedia content\n",
    "\n",
    "### Usage for Processing Planning\n",
    "\n",
    "This analysis helps prioritize processing efforts:\n",
    "1. **Critical**: Address landmarks missing PDF content first\n",
    "2. **Enhancement**: Consider Wikipedia article processing for landmarks without it\n",
    "3. **Maintenance**: Verify landmarks with complete coverage are working correctly\n",
    "\n",
    "These improvements make the notebook more actionable for maintaining and expanding the landmark vector database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
