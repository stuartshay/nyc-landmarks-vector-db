{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63ebf04",
   "metadata": {},
   "source": [
    "# NYC Landmarks Vector Database - Processing Status Analysis\n",
    "\n",
    "This notebook analyzes the processing status of NYC landmark records in the Pinecone vector database. It determines which landmarks have already been processed (have vectors in Pinecone) and which landmarks still need processing.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Connect to CoreDataStore API and Pinecone database\n",
    "2. Fetch all available landmark IDs from the CoreDataStore API\n",
    "3. Check which landmarks already have vectors in Pinecone\n",
    "4. Generate statistics and visualizations of processing status\n",
    "5. Export a list of unprocessed landmarks for batch processing\n",
    "6. Generate a GitHub Actions workflow configuration for remaining processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7e963",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4909c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4544b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "# Import project modules\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.db.db_client import get_db_client\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=settings.LOG_LEVEL.value,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b873c",
   "metadata": {},
   "source": [
    "## 2. Connect to Databases\n",
    "\n",
    "Next, we'll establish connections to both the CoreDataStore API and the Pinecone vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e292077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the database client for CoreDataStore API\n",
    "db_client = get_db_client()\n",
    "print(\"âœ… Initialized CoreDataStore API client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da866937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone database client\n",
    "try:\n",
    "    # Create PineconeDB instance\n",
    "    pinecone_db = PineconeDB()\n",
    "\n",
    "    # Check if the connection was successful\n",
    "    if pinecone_db.index:\n",
    "        print(f\"âœ… Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "        print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "        print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "        print(f\"Metric: {pinecone_db.metric}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"âŒ Failed to connect to Pinecone. Check your credentials and network connection.\"\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error initializing Pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66716fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index statistics from Pinecone\n",
    "try:\n",
    "    stats = pinecone_db.get_index_stats()\n",
    "\n",
    "    # Check for errors\n",
    "    if \"error\" in stats:\n",
    "        print(f\"âŒ Error retrieving index stats: {stats['error']}\")\n",
    "        # Create fallback mock stats for demonstration\n",
    "        total_vector_count = 0\n",
    "        namespaces = {}\n",
    "    else:\n",
    "        print(\"âœ… Successfully retrieved index stats\")\n",
    "        total_vector_count = stats.get(\"total_vector_count\", 0)\n",
    "        namespaces = stats.get(\"namespaces\", {})\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error retrieving index stats: {e}\")\n",
    "    # Create fallback mock stats for demonstration\n",
    "    total_vector_count = 0\n",
    "    namespaces = {}\n",
    "    stats = {}\n",
    "\n",
    "print(\"\\nðŸ“Š Index Statistics:\")\n",
    "print(f\"Total Vector Count: {total_vector_count:,}\")\n",
    "print(f\"Dimension: {stats.get('dimension')}\")\n",
    "print(f\"Index Fullness: {stats.get('index_fullness')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a834a62",
   "metadata": {},
   "source": [
    "## 3. Fetch All Landmark IDs\n",
    "\n",
    "Now we'll fetch all landmark IDs from the CoreDataStore API to determine the total universe of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_landmark_ids(start_page=1, end_page=None, page_size=100, max_pages=500):\n",
    "    \"\"\"Fetch all landmark IDs from CoreDataStore API.\n",
    "\n",
    "    Args:\n",
    "        start_page: Starting page number (default: 1)\n",
    "        end_page: Ending page number (default: None, fetch until no more results)\n",
    "        page_size: Number of landmarks per page (default: 100)\n",
    "        max_pages: Maximum number of pages to fetch (safety limit)\n",
    "\n",
    "    Returns:\n",
    "        Set of landmark IDs\n",
    "    \"\"\"\n",
    "    all_landmark_ids = set()\n",
    "    current_page = start_page\n",
    "    total_pages_fetched = 0\n",
    "\n",
    "    try:\n",
    "        with tqdm(desc=\"Fetching landmark IDs\", unit=\"page\") as pbar:\n",
    "            while True:\n",
    "                # Check if we've reached the end page or max pages\n",
    "                if (\n",
    "                    end_page and current_page > end_page\n",
    "                ) or total_pages_fetched >= max_pages:\n",
    "                    break\n",
    "\n",
    "                # Fetch landmarks for the current page\n",
    "                try:\n",
    "                    landmarks = db_client.get_landmarks_page(page_size, current_page)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching page {current_page}: {e}\")\n",
    "                    # Try to continue with next page\n",
    "                    current_page += 1\n",
    "                    total_pages_fetched += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # If no landmarks found, we've reached the end\n",
    "                if not landmarks:\n",
    "                    print(f\"No landmarks found on page {current_page}, ending fetch\")\n",
    "                    break\n",
    "\n",
    "                # Process the landmarks\n",
    "                for landmark in landmarks:\n",
    "                    landmark_id = landmark.get(\"id\", \"\") or landmark.get(\"lpNumber\", \"\")\n",
    "                    if landmark_id:\n",
    "                        all_landmark_ids.add(landmark_id)\n",
    "\n",
    "                # Update progress\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"page\": current_page,\n",
    "                        \"landmarks\": len(landmarks),\n",
    "                        \"total\": len(all_landmark_ids),\n",
    "                    }\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "                # Move to next page\n",
    "                current_page += 1\n",
    "                total_pages_fetched += 1\n",
    "\n",
    "                # Small delay to avoid rate limiting\n",
    "                time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching landmark IDs: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"Completed fetching {len(all_landmark_ids)} landmark IDs from {total_pages_fetched} pages\"\n",
    "    )\n",
    "    return all_landmark_ids\n",
    "\n",
    "\n",
    "# Configure fetch parameters\n",
    "page_size = 100\n",
    "start_page = 1\n",
    "# Set a reasonable end_page based on expected data volume - adjust if needed\n",
    "# Leave as None to fetch all available landmarks\n",
    "end_page = 10  # Example limit for testing - set to None for full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d38068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all landmark IDs\n",
    "start_time = time.time()\n",
    "all_landmark_ids = fetch_all_landmark_ids(\n",
    "    start_page=start_page, end_page=end_page, page_size=page_size\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    f\"Fetched {len(all_landmark_ids)} unique landmark IDs in {elapsed_time:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f619576",
   "metadata": {},
   "source": [
    "## 4. Check Processing Status in Pinecone\n",
    "\n",
    "Now we'll check which landmarks already have vectors in Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_landmark_processing_status(pinecone_db, landmark_ids, batch_size=10, top_k=1):\n",
    "    \"\"\"Check which landmarks have vectors in Pinecone.\n",
    "\n",
    "    Args:\n",
    "        pinecone_db: PineconeDB instance\n",
    "        landmark_ids: Set of landmark IDs to check\n",
    "        batch_size: Number of landmarks to check in parallel batches\n",
    "        top_k: Number of vectors to retrieve per landmark (1 is sufficient to check existence)\n",
    "\n",
    "    Returns:\n",
    "        processed_landmarks: Set of landmark IDs that have vectors\n",
    "        unprocessed_landmarks: Set of landmark IDs that don't have vectors\n",
    "    \"\"\"\n",
    "    # Generate a random query vector for searching\n",
    "    random_vector = np.random.rand(pinecone_db.dimensions).tolist()\n",
    "\n",
    "    processed_landmarks = set()\n",
    "    unprocessed_landmarks = set()\n",
    "\n",
    "    # Convert set to list for iteration with tqdm\n",
    "    landmark_ids_list = list(landmark_ids)\n",
    "\n",
    "    with tqdm(total=len(landmark_ids_list), desc=\"Checking processing status\") as pbar:\n",
    "        for i in range(0, len(landmark_ids_list), batch_size):\n",
    "            # Get the current batch\n",
    "            batch = landmark_ids_list[i : i + batch_size]\n",
    "\n",
    "            # Check each landmark in the batch\n",
    "            for landmark_id in batch:\n",
    "                # Query Pinecone for vectors with this landmark_id\n",
    "                filter_dict = {\"landmark_id\": landmark_id}\n",
    "                try:\n",
    "                    # We only need to know if vectors exist, so top_k=1 is sufficient\n",
    "                    vectors = pinecone_db.query_vectors(\n",
    "                        query_vector=random_vector, top_k=top_k, filter_dict=filter_dict\n",
    "                    )\n",
    "\n",
    "                    # If vectors found, mark as processed, otherwise unprocessed\n",
    "                    if vectors:\n",
    "                        processed_landmarks.add(landmark_id)\n",
    "                    else:\n",
    "                        unprocessed_landmarks.add(landmark_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error checking landmark {landmark_id}: {e}\")\n",
    "                    # If we can't check, assume unprocessed to be safe\n",
    "                    unprocessed_landmarks.add(landmark_id)\n",
    "\n",
    "            # Update progress\n",
    "            pbar.update(len(batch))\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"processed\": len(processed_landmarks),\n",
    "                    \"unprocessed\": len(unprocessed_landmarks),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    return processed_landmarks, unprocessed_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check processing status for all landmarks\n",
    "start_time = time.time()\n",
    "processed_landmarks, unprocessed_landmarks = check_landmark_processing_status(\n",
    "    pinecone_db=pinecone_db,\n",
    "    landmark_ids=all_landmark_ids,\n",
    "    batch_size=10,  # Adjust based on API rate limits\n",
    "    top_k=1,\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nProcessing status check completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Total landmarks: {len(all_landmark_ids)}\")\n",
    "print(\n",
    "    f\"Processed landmarks: {len(processed_landmarks)} ({len(processed_landmarks)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Unprocessed landmarks: {len(unprocessed_landmarks)} ({len(unprocessed_landmarks)/len(all_landmark_ids)*100:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec01c8",
   "metadata": {},
   "source": [
    "## 5. Analysis and Visualizations\n",
    "\n",
    "Now we'll analyze the processing status and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip DataFrame creation entirely and use Python native data structures\n",
    "# First convert to Python lists to ensure we're working with primitive types\n",
    "landmark_ids_list = sorted(list(all_landmark_ids))\n",
    "processed_landmarks_set = processed_landmarks\n",
    "unprocessed_landmarks_set = unprocessed_landmarks\n",
    "\n",
    "# Count the number of processed and unprocessed landmarks\n",
    "total_landmarks = len(landmark_ids_list)\n",
    "processed_count = len(processed_landmarks_set)\n",
    "unprocessed_count = len(unprocessed_landmarks_set)\n",
    "\n",
    "# Calculate percentages\n",
    "processed_percentage = (processed_count / total_landmarks) * 100\n",
    "unprocessed_percentage = (unprocessed_count / total_landmarks) * 100\n",
    "\n",
    "# Create a simple dictionary to display the statistics\n",
    "stats_dict = {\n",
    "    \"Status\": [\"Processed\", \"Unprocessed\"],\n",
    "    \"Count\": [processed_count, unprocessed_count],\n",
    "    \"Percentage\": [processed_percentage, unprocessed_percentage],\n",
    "}\n",
    "\n",
    "# Display the statistics\n",
    "print(\"Processing Statistics:\")\n",
    "print(f\"Total landmarks: {total_landmarks}\")\n",
    "print(f\"Processed landmarks: {processed_count} ({processed_percentage:.2f}%)\")\n",
    "print(f\"Unprocessed landmarks: {unprocessed_count} ({unprocessed_percentage:.2f}%)\")\n",
    "\n",
    "# Display sample of processed and unprocessed landmarks\n",
    "print(\"\\nSample of processed landmarks:\")\n",
    "for lid in list(processed_landmarks_set)[:5]:\n",
    "    print(f\"  - {lid}\")\n",
    "\n",
    "print(\"\\nSample of unprocessed landmarks:\")\n",
    "for lid in list(unprocessed_landmarks_set)[:5]:\n",
    "    print(f\"  - {lid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create visualization of processing status using our native Python data structures\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "status_labels = [\"Processed\", \"Unprocessed\"]\n",
    "status_counts = [len(processed_landmarks), len(unprocessed_landmarks)]\n",
    "status_percentages = [\n",
    "    len(processed_landmarks) / len(all_landmark_ids) * 100,\n",
    "    len(unprocessed_landmarks) / len(all_landmark_ids) * 100,\n",
    "]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(status_labels, status_counts, color=[\"#4CAF50\", \"#F44336\"])\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height / 2,\n",
    "        f\"{status_counts[i]} ({status_percentages[i]:.1f}%)\",\n",
    "        color=\"white\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"NYC Landmarks Processing Status\", fontsize=16)\n",
    "plt.xlabel(\"Status\", fontsize=14)\n",
    "plt.ylabel(\"Number of Landmarks\", fontsize=14)\n",
    "plt.ylim(0, len(all_landmark_ids) * 1.1)  # Add some space above bars\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart of processing status using native Python data structures\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    status_counts,\n",
    "    labels=status_labels,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[\"#4CAF50\", \"#F44336\"],\n",
    "    explode=[0.05, 0],  # Slightly explode the 'Processed' slice\n",
    "    startangle=90,\n",
    "    shadow=True,\n",
    "    textprops={\"fontsize\": 14},\n",
    ")\n",
    "plt.title(\"NYC Landmarks Processing Status\", fontsize=16)\n",
    "plt.axis(\"equal\")  # Equal aspect ratio ensures that pie is circular\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f64e9",
   "metadata": {},
   "source": [
    "## 6. Export Results for Batch Processing\n",
    "\n",
    "Now we'll export the list of unprocessed landmarks for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a888e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure output directory\n",
    "output_dir = Path(\"../data/processing_status\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Generate timestamp for filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83282ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare report data\n",
    "report = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_landmarks\": len(all_landmark_ids),\n",
    "    \"processed_count\": len(processed_landmarks),\n",
    "    \"unprocessed_count\": len(unprocessed_landmarks),\n",
    "    \"processed_percentage\": len(processed_landmarks) / len(all_landmark_ids) * 100,\n",
    "    \"unprocessed_percentage\": len(unprocessed_landmarks) / len(all_landmark_ids) * 100,\n",
    "    \"processed_landmarks\": sorted(list(processed_landmarks)),\n",
    "    \"unprocessed_landmarks\": sorted(list(unprocessed_landmarks)),\n",
    "}\n",
    "\n",
    "# Save full report\n",
    "report_file = output_dir / f\"landmark_processing_report_{timestamp}.json\"\n",
    "with open(report_file, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"Saved processing report to {report_file}\")\n",
    "\n",
    "# Save just the unprocessed landmarks list for easier use\n",
    "unprocessed_file = output_dir / f\"unprocessed_landmarks_{timestamp}.json\"\n",
    "with open(unprocessed_file, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\"unprocessed_landmarks\": sorted(list(unprocessed_landmarks))}, f, indent=2\n",
    "    )\n",
    "print(f\"Saved unprocessed landmarks list to {unprocessed_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de564d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a GitHub Actions workflow configuration\n",
    "# This can be used to process the remaining landmarks\n",
    "\n",
    "# Optimize batch size and worker count based on total remaining landmarks\n",
    "unprocessed_count = len(unprocessed_landmarks)\n",
    "recommended_batch_size = min(10, max(1, unprocessed_count // 100))\n",
    "recommended_workers = min(8, max(2, unprocessed_count // 500))\n",
    "\n",
    "workflow_config = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_unprocessed_landmarks\": unprocessed_count,\n",
    "    \"api_page_size\": 100,  # Standard page size for API\n",
    "    \"job_batch_size\": recommended_batch_size,\n",
    "    \"parallel_workers\": recommended_workers,\n",
    "    \"recreate_index\": False,  # Don't recreate index since we already have processed landmarks\n",
    "    \"specific_landmarks\": sorted(list(unprocessed_landmarks)),\n",
    "}\n",
    "\n",
    "# Save the workflow configuration\n",
    "workflow_file = output_dir / f\"workflow_config_{timestamp}.json\"\n",
    "with open(workflow_file, \"w\") as f:\n",
    "    json.dump(workflow_config, f, indent=2)\n",
    "print(f\"Saved GitHub Actions workflow configuration to {workflow_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd68b7b",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "This notebook has analyzed the processing status of NYC landmark records in the Pinecone vector database. Here's a summary of the findings:\n",
    "\n",
    "1. **Total Landmarks**: We found a total of [TOTAL_COUNT] landmarks in the CoreDataStore API.\n",
    "2. **Processing Status**:\n",
    "   - [PROCESSED_COUNT] landmarks have already been processed ([PROCESSED_PERCENTAGE]%)\n",
    "   - [UNPROCESSED_COUNT] landmarks still need processing ([UNPROCESSED_PERCENTAGE]%)\n",
    "3. **Exported Files**:\n",
    "   - Full processing report: [REPORT_FILE]\n",
    "   - Unprocessed landmarks list: [UNPROCESSED_FILE]\n",
    "   - GitHub Actions workflow configuration: [WORKFLOW_FILE]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To process all remaining landmarks:\n",
    "\n",
    "1. Run the GitHub Actions workflow using the generated configuration:\n",
    "   - Upload the workflow config file to the GitHub repository\n",
    "   - Trigger the workflow with the appropriate parameters\n",
    "\n",
    "2. Monitor the processing progress:\n",
    "   - Track GitHub Actions execution\n",
    "   - Run this notebook again after processing to verify completion\n",
    "\n",
    "3. Verify results:\n",
    "   - Run verification tests to confirm all landmarks were processed correctly\n",
    "   - Check metadata consistency across the database\n",
    "   - Verify vector counts match expected totals\n",
    "\n",
    "## Troubleshooting Notes\n",
    "\n",
    "If you encounter any issues with the DataFrame creation in the analysis section, make sure:\n",
    "\n",
    "1. The landmarks data is properly converted from sets to lists before processing\n",
    "2. Avoid using direct list comprehension methods that might trigger NumPy type conversion issues\n",
    "3. Consider creating the DataFrame in steps if you encounter type conversion errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-landmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
