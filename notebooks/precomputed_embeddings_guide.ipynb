{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pre-Computed Embeddings with Pinecone in NYC Landmarks Vector Database\n",
    "\n",
    "This notebook provides a comprehensive guide to working with pre-computed embeddings in the NYC Landmarks Vector Database project, focusing on how Pinecone stores and retrieves these vectors.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Pinecone is a vector database that stores pre-computed embeddings, making it ideal for similarity search applications. In this notebook, we'll explore:\n",
    "\n",
    "1. How Pinecone stores pre-computed vectors\n",
    "2. Strategies for working with pre-computed embeddings\n",
    "3. Optimizing query performance using cached embeddings\n",
    "4. Best practices for vector management\n",
    "\n",
    "By the end of this notebook, you'll understand how to effectively leverage pre-computed embeddings for faster and more efficient queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add project directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "# Import project modules\n",
    "from nyc_landmarks.config.settings import settings\n",
    "from nyc_landmarks.embeddings.generator import EmbeddingGenerator\n",
    "from nyc_landmarks.vectordb.pinecone_db import PineconeDB\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=settings.LOG_LEVEL.value,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Understanding Pre-computed Embeddings\n",
    "\n",
    "### What are Pre-computed Embeddings?\n",
    "\n",
    "Pre-computed embeddings are vector representations of text (or other data) that have been generated in advance and stored for later use. In the context of our NYC Landmarks project:\n",
    "\n",
    "- **Text → Vector**: Each piece of text (description of a landmark, historical information, etc.) is converted into a vector (list of floating-point numbers) using an embedding model like OpenAI's text-embedding-ada-002.\n",
    "- **Storage**: These vectors are stored in a vector database (Pinecone) for efficient similarity search.\n",
    "- **Retrieval**: When a query is made, we find the most similar vectors to the query's vector.\n",
    "\n",
    "### Why Use Pre-computed Embeddings?\n",
    "\n",
    "1. **Performance**: Generating embeddings is computationally expensive and time-consuming. Pre-computing them allows for faster query responses.\n",
    "2. **Cost Efficiency**: API calls to embedding models (like OpenAI) cost money. Generating embeddings once and reusing them reduces costs.\n",
    "3. **Consistency**: Ensures consistent vector representations across multiple queries.\n",
    "4. **Scalability**: Enables handling large datasets with minimal query-time overhead.\n",
    "\n",
    "Let's see how we generate embeddings in our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding generator\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "\n",
    "# Example text to embed\n",
    "sample_text = \"The Empire State Building is a 102-story Art Deco skyscraper in Midtown Manhattan, New York City.\"\n",
    "\n",
    "# Measure the time taken to generate an embedding\n",
    "start_time = time.time()\n",
    "embedding = embedding_generator.generate_embedding(sample_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Generated embedding with {len(embedding)} dimensions\")\n",
    "print(f\"Time taken: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "# Show a small portion of the embedding vector\n",
    "print(\"\\nEmbedding vector (first 10 dimensions):\")\n",
    "print(embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. How Pinecone Stores Vectors\n",
    "\n",
    "Pinecone is a vector database designed specifically for storing and searching pre-computed vectors. Let's connect to our Pinecone index and explore how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone database client\n",
    "pinecone_db = PineconeDB()\n",
    "\n",
    "# Check if the connection was successful\n",
    "if pinecone_db.index:\n",
    "    print(f\"✅ Successfully connected to Pinecone index: {pinecone_db.index_name}\")\n",
    "    print(f\"Namespace: {pinecone_db.namespace}\")\n",
    "    print(f\"Dimensions: {pinecone_db.dimensions}\")\n",
    "else:\n",
    "    print(\n",
    "        \"❌ Failed to connect to Pinecone. Check your credentials and network connection.\"\n",
    "    )\n",
    "\n",
    "# Get index statistics\n",
    "stats = pinecone_db.get_index_stats()\n",
    "\n",
    "print(\"\\n📊 Index Statistics:\")\n",
    "print(f\"Total Vector Count: {stats.get('total_vector_count', 0):,}\")\n",
    "print(f\"Dimension: {stats.get('dimension')}\")\n",
    "print(f\"Index Fullness: {stats.get('index_fullness')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Pinecone's Vector Storage Architecture\n",
    "\n",
    "Pinecone stores vectors in a way that optimizes for similarity search. Here's how it works:\n",
    "\n",
    "1. **Vector Storage**: When you upload vectors to Pinecone, they are stored in their original form (pre-computed).\n",
    "   - Each vector is associated with a unique ID\n",
    "   - Metadata can be attached to each vector\n",
    "   - The dimensionality of all vectors must be consistent\n",
    "\n",
    "2. **Indexing**: Pinecone builds specialized indexes to enable fast similarity search:\n",
    "   - Vectors are organized into clusters based on similarity\n",
    "   - Various indexing algorithms (e.g., HNSW - Hierarchical Navigable Small World) are used to optimize search\n",
    "\n",
    "3. **Querying**: When you search, Pinecone compares your query vector to stored vectors:\n",
    "   - Similarity is typically measured using cosine similarity, Euclidean distance, or dot product\n",
    "   - Results are returned as vectors and their IDs, sorted by similarity score\n",
    "\n",
    "Let's examine the `query_vectors` method from our PineconeDB class to understand how querying works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation of query_vectors from PineconeDB\n",
    "\n",
    "'''\n",
    "def query_vectors(\n",
    "    self,\n",
    "    query_vector: List[float],\n",
    "    top_k: int = 5,\n",
    "    filter_dict: Optional[Dict[str, Any]] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Query vectors from Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        query_vector: Embedding of the query text\n",
    "        top_k: Number of results to return\n",
    "        filter_dict: Dictionary of metadata filters\n",
    "\n",
    "    Returns:\n",
    "        List of matching vectors with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = self.index.query(\n",
    "            vector=query_vector,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            filter=filter_dict,\n",
    "        )\n",
    "\n",
    "        # Process the response to extract matches\n",
    "        result_list: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Handle response.matches which can be a list or other iterable\n",
    "        # Cast response to Any to handle different return types from Pinecone SDK\n",
    "        from typing import Any as TypeAny\n",
    "        from typing import cast\n",
    "\n",
    "        response_dict = cast(TypeAny, response)\n",
    "\n",
    "        # Access matches safely\n",
    "        matches = getattr(response_dict, \"matches\", [])\n",
    "        for match in matches:\n",
    "            # Handle match objects\n",
    "            match_dict: Dict[str, Any] = {}\n",
    "\n",
    "            # Extract ID if available\n",
    "            if hasattr(match, \"id\"):\n",
    "                match_dict[\"id\"] = match.id\n",
    "\n",
    "            # Extract score if available\n",
    "            if hasattr(match, \"score\"):\n",
    "                match_dict[\"score\"] = match.score\n",
    "\n",
    "            # Extract metadata if available\n",
    "            if hasattr(match, \"metadata\"):\n",
    "                match_dict[\"metadata\"] = match.metadata\n",
    "\n",
    "            result_list.append(match_dict)\n",
    "\n",
    "        return result_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to query vectors: {e}\")\n",
    "        return []\n",
    "'''\n",
    "\n",
    "# Note: This code block is for reference only and won't be executed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Practical Examples with Pre-computed Embeddings\n",
    "\n",
    "Now that we understand how pre-computed embeddings work with Pinecone, let's explore practical examples and strategies for leveraging them effectively.\n",
    "\n",
    "### Example 1: Using Pre-computed Embeddings for Common Queries\n",
    "\n",
    "In many applications, certain queries are common and repeated frequently. Pre-computing and caching embeddings for these queries can significantly improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some common queries about NYC landmarks\n",
    "common_queries = [\n",
    "    \"What is the Empire State Building?\",\n",
    "    \"Tell me about the Brooklyn Bridge\",\n",
    "    \"What are the historic districts in Manhattan?\",\n",
    "    \"What is the architectural style of Grand Central Terminal?\",\n",
    "    \"When was the Statue of Liberty designated as a landmark?\",\n",
    "]\n",
    "\n",
    "# Create an embedding cache\n",
    "embedding_cache = {}\n",
    "\n",
    "print(\"Generating and caching embeddings for common queries...\")\n",
    "for query in common_queries:\n",
    "    # Measure embedding generation time\n",
    "    start_time = time.time()\n",
    "    embedding_cache[query] = embedding_generator.generate_embedding(query)\n",
    "    end_time = time.time()\n",
    "    print(f\"  - '{query}': {end_time - start_time:.3f}s\")\n",
    "\n",
    "print(f\"\\nCache contains {len(embedding_cache)} pre-computed embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a query using a cached embedding if available\n",
    "\n",
    "\n",
    "def execute_query_with_cache(\n",
    "    query_text: str,\n",
    "    embedding_cache: Dict[str, List[float]],\n",
    "    top_k: int = 5,\n",
    "    filter_dict: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute a vector search query using cached embeddings when available.\"\"\"\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check if query is in cache\n",
    "    cache_hit = query_text in embedding_cache\n",
    "\n",
    "    # Get embedding (from cache or generate new)\n",
    "    embedding_start = time.time()\n",
    "    if cache_hit:\n",
    "        query_embedding = embedding_cache[query_text]\n",
    "        embedding_source = \"cache\"\n",
    "    else:\n",
    "        query_embedding = embedding_generator.generate_embedding(query_text)\n",
    "        embedding_source = \"generated\"\n",
    "    embedding_time = time.time() - embedding_start\n",
    "\n",
    "    # Execute the query\n",
    "    query_start = time.time()\n",
    "    results = pinecone_db.query_vectors(\n",
    "        query_vector=query_embedding, top_k=top_k, filter_dict=filter_dict\n",
    "    )\n",
    "    query_time = time.time() - query_start\n",
    "\n",
    "    # Calculate total time\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        \"query\": query_text,\n",
    "        \"embedding\": query_embedding,\n",
    "        \"results\": results,\n",
    "        \"metrics\": {\n",
    "            \"embedding_time\": embedding_time,\n",
    "            \"query_time\": query_time,\n",
    "            \"total_time\": total_time,\n",
    "            \"result_count\": len(results),\n",
    "            \"cache_hit\": cache_hit,\n",
    "            \"embedding_source\": embedding_source,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a cached query\n",
    "cached_query = \"Tell me about the Brooklyn Bridge\"\n",
    "print(f\"Testing cached query: '{cached_query}'\")\n",
    "cached_result = execute_query_with_cache(cached_query, embedding_cache)\n",
    "\n",
    "# Test with a non-cached query\n",
    "new_query = \"What is the history of the Flatiron Building?\"\n",
    "print(f\"\\nTesting new query: '{new_query}'\")\n",
    "new_result = execute_query_with_cache(new_query, embedding_cache)\n",
    "\n",
    "# Compare performance\n",
    "print(\"\\n🔍 Performance Comparison:\\n\")\n",
    "print(f\"Cached Query:\")\n",
    "print(\n",
    "    f\"  - Embedding Time: {cached_result['metrics']['embedding_time']:.6f}s (from {cached_result['metrics']['embedding_source']})\"\n",
    ")\n",
    "print(f\"  - Query Time: {cached_result['metrics']['query_time']:.3f}s\")\n",
    "print(f\"  - Total Time: {cached_result['metrics']['total_time']:.3f}s\")\n",
    "print(f\"  - Results: {cached_result['metrics']['result_count']}\")\n",
    "\n",
    "print(f\"\\nNew Query:\")\n",
    "print(\n",
    "    f\"  - Embedding Time: {new_result['metrics']['embedding_time']:.3f}s (from {new_result['metrics']['embedding_source']})\"\n",
    ")\n",
    "print(f\"  - Query Time: {new_result['metrics']['query_time']:.3f}s\")\n",
    "print(f\"  - Total Time: {new_result['metrics']['total_time']:.3f}s\")\n",
    "print(f\"  - Results: {new_result['metrics']['result_count']}\")\n",
    "\n",
    "# Calculate the speedup\n",
    "if new_result[\"metrics\"][\"embedding_time\"] > 0:\n",
    "    speedup = (\n",
    "        new_result[\"metrics\"][\"embedding_time\"]\n",
    "        / cached_result[\"metrics\"][\"embedding_time\"]\n",
    "    )\n",
    "    print(f\"\\n⚡ Embedding generation speedup with cache: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Visualizing the Results\n",
    "\n",
    "Let's create a function to display query results in a readable format and visualize the similarity scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_results(query_result, max_results=None, show_metadata=True):\n",
    "    \"\"\"Display query results in a readable format.\n",
    "\n",
    "    Args:\n",
    "        query_result: The result dictionary returned by execute_query_with_cache\n",
    "        max_results: Maximum number of results to display (default: all)\n",
    "        show_metadata: Whether to display metadata fields (default: True)\n",
    "    \"\"\"\n",
    "    results = query_result[\"results\"]\n",
    "    if max_results:\n",
    "        results = results[:max_results]\n",
    "\n",
    "    print(f\"Query: '{query_result['query']}'\\n\")\n",
    "    print(\n",
    "        f\"🔍 Found {query_result['metrics']['result_count']} results (showing {len(results)})\"\n",
    "    )\n",
    "    print(f\"⏱️ Total query time: {query_result['metrics']['total_time']:.3f} seconds\")\n",
    "\n",
    "    # Show cache information if available\n",
    "    if \"cache_hit\" in query_result[\"metrics\"]:\n",
    "        cache_status = \"HIT ✓\" if query_result[\"metrics\"][\"cache_hit\"] else \"MISS ✗\"\n",
    "        print(\n",
    "            f\"💾 Cache: {cache_status} ({query_result['metrics']['embedding_source']})\"\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, match in enumerate(results):\n",
    "        print(f\"\\n📌 Result #{i+1} - Score: {match['score']:.4f}\")\n",
    "        print(f\"ID: {match['id']}\")\n",
    "\n",
    "        if show_metadata and \"metadata\" in match:\n",
    "            metadata = match[\"metadata\"]\n",
    "            print(\"\\nMetadata:\")\n",
    "\n",
    "            # Print important fields first\n",
    "            priority_fields = [\n",
    "                \"name\",\n",
    "                \"borough\",\n",
    "                \"landmark_type\",\n",
    "                \"designation_date\",\n",
    "                \"neighborhood\",\n",
    "            ]\n",
    "            for field in priority_fields:\n",
    "                if field in metadata and metadata[field]:\n",
    "                    print(f\"  {field.capitalize()}: {metadata[field]}\")\n",
    "\n",
    "            # Print content fields if available\n",
    "            content_fields = [\"text_chunk\", \"description\", \"text\"]\n",
    "            for field in content_fields:\n",
    "                if field in metadata and metadata[field]:\n",
    "                    content = metadata[field]\n",
    "                    snippet = content[:250] + \"...\" if len(content) > 250 else content\n",
    "                    print(f\"\\n  Content ({field}): {snippet}\")\n",
    "                    break\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # Create visualization of similarity scores\n",
    "    if results:\n",
    "        scores = [match[\"score\"] for match in results]\n",
    "        labels = [\n",
    "            match.get(\"metadata\", {}).get(\"name\", match[\"id\"][:20]) for match in results\n",
    "        ]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.barh(labels, scores, color=\"skyblue\")\n",
    "        plt.xlabel(\"Similarity Score\")\n",
    "        plt.title(f'Similarity Scores for \"{query_result[\"query\"]}\"')\n",
    "        plt.xlim(0, 1.0)\n",
    "\n",
    "        # Add the values to the end of each bar\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(\n",
    "                width + 0.01,\n",
    "                bar.get_y() + bar.get_height() / 2,\n",
    "                f\"{width:.4f}\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the visualization with our cached query result\n",
    "display_query_results(cached_result, max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 5. Advanced Techniques with Pre-computed Embeddings\n",
    "\n",
    "Beyond simple caching, there are several advanced techniques we can use with pre-computed embeddings:\n",
    "\n",
    "### 5.1 Vector Manipulation\n",
    "\n",
    "One advantage of working with pre-computed embeddings is that we can manipulate the vectors directly to perform operations like:\n",
    "\n",
    "- Combining multiple embeddings\n",
    "- Emphasizing certain aspects of a query\n",
    "- Creating \"negative\" queries (find things that are different)\n",
    "\n",
    "Let's see an example of combining multiple embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings(embeddings, weights=None):\n",
    "    \"\"\"Combine multiple embeddings into a single embedding.\n",
    "\n",
    "    Args:\n",
    "        embeddings: List of embeddings to combine\n",
    "        weights: Optional weights for each embedding (must sum to 1)\n",
    "\n",
    "    Returns:\n",
    "        Combined embedding vector\n",
    "    \"\"\"\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"No embeddings provided\")\n",
    "\n",
    "    # Default to equal weights if not specified\n",
    "    if weights is None:\n",
    "        weights = [1 / len(embeddings)] * len(embeddings)\n",
    "\n",
    "    # Ensure weights sum to 1\n",
    "    if abs(sum(weights) - 1.0) > 1e-6:\n",
    "        weights = [w / sum(weights) for w in weights]\n",
    "\n",
    "    # Check dimensions match\n",
    "    dim = len(embeddings[0])\n",
    "    if not all(len(emb) == dim for emb in embeddings):\n",
    "        raise ValueError(\"All embeddings must have the same dimensions\")\n",
    "\n",
    "    # Combine embeddings\n",
    "    combined = [0.0] * dim\n",
    "    for i in range(dim):\n",
    "        for j, embedding in enumerate(embeddings):\n",
    "            combined[i] += embedding[i] * weights[j]\n",
    "\n",
    "    # Normalize the combined vector\n",
    "    magnitude = np.sqrt(sum(x * x for x in combined))\n",
    "    combined = [x / magnitude for x in combined]\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Generate embeddings for two related concepts\n",
    "print(\"Generating embeddings for two concepts...\")\n",
    "embedding1 = embedding_generator.generate_embedding(\"Historic landmark in Brooklyn\")\n",
    "embedding2 = embedding_generator.generate_embedding(\"Famous bridge in New York\")\n",
    "\n",
    "# Combine the embeddings with different weights\n",
    "combined_embedding = combine_embeddings([embedding1, embedding2], weights=[0.3, 0.7])\n",
    "\n",
    "print(f\"Embedding 1 dimensions: {len(embedding1)}\")\n",
    "print(f\"Embedding 2 dimensions: {len(embedding2)}\")\n",
    "print(f\"Combined embedding dimensions: {len(combined_embedding)}\")\n",
    "\n",
    "# Query using the combined embedding\n",
    "print(\"\\nQuerying with combined embedding...\")\n",
    "results = pinecone_db.query_vectors(query_vector=combined_embedding, top_k=5)\n",
    "print(f\"Found {len(results)} results\")\n",
    "\n",
    "# Show the top result\n",
    "if results:\n",
    "    top_result = results[0]\n",
    "    print(f\"\\nTop result:\")\n",
    "    print(f\"  ID: {top_result['id']}\")\n",
    "    print(f\"  Score: {top_result['score']:.4f}\")\n",
    "\n",
    "    if \"metadata\" in top_result and \"name\" in top_result[\"metadata\"]:\n",
    "        print(f\"  Name: {top_result['metadata']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 5.2 Persistent Storage of Pre-computed Embeddings\n",
    "\n",
    "For production applications, you'll want to store your pre-computed embeddings persistently. Here are some options:\n",
    "\n",
    "1. **Local File Storage**: Save embeddings to disk with pickle or JSON\n",
    "2. **Database Storage**: Store embeddings in a relational or NoSQL database\n",
    "3. **Cloud Storage**: Store embeddings in S3, Azure Blob Storage, etc.\n",
    "4. **Redis**: Use Redis with vector support for fast retrieval\n",
    "\n",
    "Let's demonstrate how to store and load embeddings using pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_to_file(embeddings_dict, filepath):\n",
    "    \"\"\"Save embeddings dictionary to a file using pickle.\n",
    "\n",
    "    Args:\n",
    "        embeddings_dict: Dictionary of embeddings\n",
    "        filepath: Path to save the file\n",
    "    \"\"\"\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(embeddings_dict, f)\n",
    "    print(f\"Saved {len(embeddings_dict)} embeddings to {filepath}\")\n",
    "\n",
    "\n",
    "def load_embeddings_from_file(filepath):\n",
    "    \"\"\"Load embeddings dictionary from a file.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the file\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of embeddings\n",
    "    \"\"\"\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        embeddings_dict = pickle.load(f)\n",
    "    print(f\"Loaded {len(embeddings_dict)} embeddings from {filepath}\")\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "# Save our embedding cache to a file\n",
    "embeddings_file = \"../data/cached_embeddings.pkl\"\n",
    "\n",
    "# Create a small metadata structure to save along with embeddings\n",
    "cache_metadata = {\n",
    "    \"model\": settings.OPENAI_EMBEDDING_MODEL,\n",
    "    \"dimensions\": settings.OPENAI_EMBEDDING_DIMENSIONS,\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"queries\": list(embedding_cache.keys()),\n",
    "}\n",
    "\n",
    "# Create a dictionary with both embeddings and metadata\n",
    "save_data = {\"embeddings\": embedding_cache, \"metadata\": cache_metadata}\n",
    "\n",
    "# Uncomment the following lines to actually save and load the embeddings\n",
    "# save_embeddings_to_file(save_data, embeddings_file)\n",
    "# loaded_data = load_embeddings_from_file(embeddings_file)\n",
    "# loaded_embeddings = loaded_data[\"embeddings\"]\n",
    "# loaded_metadata = loaded_data[\"metadata\"]\n",
    "# print(f\"Loaded embeddings for {len(loaded_embeddings)} queries\")\n",
    "# print(f\"Model used: {loaded_metadata['model']}\")\n",
    "# print(f\"Created at: {loaded_metadata['created_at']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 6. How Pinecone Handles Similarity Search\n",
    "\n",
    "Pinecone uses Approximate Nearest Neighbors (ANN) algorithms to efficiently search through large vector spaces. Let's understand this process:\n",
    "\n",
    "### 6.1 Approximate Nearest Neighbors (ANN)\n",
    "\n",
    "Exact nearest neighbor search becomes prohibitively expensive as the number of vectors grows. ANN algorithms trade perfect accuracy for massive performance gains:\n",
    "\n",
    "1. **Indexing Structures**: Vectors are organized into specialized data structures\n",
    "2. **Pruning**: Large portions of the vector space can be eliminated from search\n",
    "3. **Approximation**: Results are very close to the exact nearest neighbors, but found much faster\n",
    "\n",
    "### 6.2 Pinecone's Search Process\n",
    "\n",
    "When you query Pinecone with a vector:\n",
    "\n",
    "1. The query vector is compared to a subset of the stored vectors\n",
    "2. The most similar vectors are identified\n",
    "3. Results are returned sorted by similarity score\n",
    "\n",
    "Importantly, Pinecone only compares your query vector to the pre-computed vectors it stores. It doesn't modify or re-compute these vectors during the search process.\n",
    "\n",
    "### 6.3 Trade-offs in Vector Search\n",
    "\n",
    "- **Accuracy vs. Speed**: Higher accuracy requires more comparisons, which takes more time\n",
    "- **Index Size vs. Query Speed**: More complex indexes are faster for queries but use more memory\n",
    "- **Batch Size vs. Latency**: Larger batches are more efficient but increase latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 7. Vector Similarity Metrics\n",
    "\n",
    "Pinecone supports several similarity metrics to compare vectors:\n",
    "\n",
    "1. **Cosine Similarity**: Measures the cosine of the angle between vectors (1.0 is perfect similarity)\n",
    "2. **Euclidean Distance**: Measures the straight-line distance between vectors (lower is more similar)\n",
    "3. **Dot Product**: Simple multiplication of vector elements (higher is more similar)\n",
    "\n",
    "In our NYC Landmarks project, we use cosine similarity as it's generally most effective for text embeddings.\n",
    "\n",
    "Let's implement a simple function to calculate cosine similarity between two embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vec1: First vector\n",
    "        vec2: Second vector\n",
    "\n",
    "    Returns:\n",
    "        Cosine similarity score (between -1 and 1, higher is more similar)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for efficient computation\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Calculate magnitudes\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    if magnitude1 * magnitude2 == 0:\n",
    "        return 0.0  # Handle zero vectors\n",
    "    else:\n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "\n",
    "# Calculate similarity between our embeddings\n",
    "similarity = cosine_similarity(embedding1, embedding2)\n",
    "print(\n",
    "    f\"Cosine similarity between 'Historic landmark in Brooklyn' and 'Famous bridge in New York': {similarity:.4f}\"\n",
    ")\n",
    "\n",
    "# Calculate similarity with the combined embedding\n",
    "sim1 = cosine_similarity(combined_embedding, embedding1)\n",
    "sim2 = cosine_similarity(combined_embedding, embedding2)\n",
    "\n",
    "print(f\"\\nSimilarity of combined embedding with:\")\n",
    "print(f\"  - 'Historic landmark in Brooklyn': {sim1:.4f}\")\n",
    "print(f\"  - 'Famous bridge in New York': {sim2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 8. Performance Optimization Strategies\n",
    "\n",
    "Based on our understanding of pre-computed embeddings, here are strategies to optimize query performance in production:\n",
    "\n",
    "### 8.1 Client-Side Embedding Cache\n",
    "\n",
    "Implement a client-side cache for frequently used queries:\n",
    "\n",
    "```python\n",
    "# Pseudocode for a simple LRU cache for embeddings\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_embedding(query_text):\n",
    "    return embedding_generator.generate_embedding(query_text)\n",
    "```\n",
    "\n",
    "### 8.2 Query Classification\n",
    "\n",
    "Classify incoming queries and map them to pre-computed queries when appropriate:\n",
    "\n",
    "```python\n",
    "# Pseudocode for query classification\n",
    "def classify_query(query_text):\n",
    "    if \"empire state building\" in query_text.lower():\n",
    "        return \"empire_state_building\"\n",
    "    elif \"brooklyn bridge\" in query_text.lower():\n",
    "        return \"brooklyn_bridge\"\n",
    "    # ... more classifications\n",
    "    else:\n",
    "        return None\n",
    "```\n",
    "\n",
    "### 8.3 Batch Processing\n",
    "\n",
    "For applications that process multiple queries, batch them to reduce overhead:\n",
    "\n",
    "```python\n",
    "# Pseudocode for batch embedding generation\n",
    "def generate_embeddings_batch(texts):\n",
    "    return embedding_generator.generate_embeddings(texts)  # Assuming this method exists\n",
    "```\n",
    "\n",
    "### 8.4 Vector Quantization\n",
    "\n",
    "For very large embedding caches, consider vector quantization to reduce memory usage:\n",
    "\n",
    "```python\n",
    "# Pseudocode for vector quantization\n",
    "def quantize_vector(vector, bits=8):\n",
    "    # Implement quantization logic here\n",
    "    return quantized_vector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 9. Frequently Asked Questions\n",
    "\n",
    "### 9.1 Does Pinecone store pre-computed vectors?\n",
    "\n",
    "**Yes**, Pinecone only stores pre-computed vectors. It does not generate embeddings itself.\n",
    "\n",
    "- **What Pinecone stores**: Pre-computed vector embeddings, IDs, and metadata\n",
    "- **What Pinecone does not do**: Generate embeddings from raw text\n",
    "\n",
    "### 9.2 How are vectors stored in Pinecone?\n",
    "\n",
    "Vectors are stored in their original form (as floating-point arrays) along with:\n",
    "- A unique ID for each vector\n",
    "- Optional metadata (key-value pairs)\n",
    "\n",
    "Pinecone builds specialized indexes on top of these vectors to enable efficient similarity search.\n",
    "\n",
    "### 9.3 What happens when I query Pinecone?\n",
    "\n",
    "When you query Pinecone:\n",
    "\n",
    "1. You provide a query vector (not raw text)\n",
    "2. Pinecone compares this vector to the stored vectors using the specified similarity metric\n",
    "3. Pinecone returns the most similar vectors, their IDs, and metadata\n",
    "\n",
    "### 9.4 Do I need to generate embeddings for every query?\n",
    "\n",
    "Not necessarily. You can:\n",
    "\n",
    "1. Cache embeddings for common queries\n",
    "2. Pre-compute embeddings for expected queries\n",
    "3. Use vector manipulation to handle variations of queries\n",
    "\n",
    "### 9.5 How can I optimize my embedding usage?\n",
    "\n",
    "- **Batch processing**: Generate embeddings in batches\n",
    "- **Caching**: Store embeddings for common queries\n",
    "- **Pruning**: Reduce embedding dimensions if needed (though this may affect quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 10. Best Practices for Working with Pre-computed Embeddings\n",
    "\n",
    "### 10.1 Embedding Generation\n",
    "\n",
    "- **Consistency**: Use the same model and parameters for all embeddings\n",
    "- **Preprocessing**: Normalize text inputs (e.g., lowercase, remove extra whitespace)\n",
    "- **Batch Processing**: Generate embeddings in batches to improve throughput\n",
    "- **Version Control**: Keep track of the model and parameters used to generate embeddings\n",
    "\n",
    "### 10.2 Storage and Retrieval\n",
    "\n",
    "- **Metadata**: Store relevant metadata with each vector for filtering and context\n",
    "- **ID Strategy**: Use meaningful IDs that can be easily mapped back to source data\n",
    "- **Compression**: Consider compression techniques for large embedding collections\n",
    "- **Backup**: Regularly backup your embeddings, especially if generation is expensive\n",
    "\n",
    "### 10.3 Query Optimization\n",
    "\n",
    "- **Query Planning**: Analyze common query patterns and pre-compute where possible\n",
    "- **Caching**: Implement multi-level caching (in-memory, disk, etc.)\n",
    "- **Monitoring**: Track query performance and cache hit rates\n",
    "- **Filtering**: Use metadata filters to reduce the search space\n",
    "\n",
    "### 10.4 Vector Maintenance\n",
    "\n",
    "- **Updating**: Have a strategy for updating embeddings when models or data change\n",
    "- **Pruning**: Remove outdated or irrelevant vectors\n",
    "- **Reindexing**: Plan for periodic reindexing to maintain performance\n",
    "- **Testing**: Verify search quality periodically with test queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've explored how Pinecone stores and works with pre-computed embeddings in the context of the NYC Landmarks Vector Database project. Key takeaways include:\n",
    "\n",
    "1. **Pinecone stores pre-computed vectors** - it doesn't generate embeddings itself\n",
    "2. **Pre-computing embeddings offers significant performance benefits** - especially for common queries\n",
    "3. **Vector manipulation allows for advanced search strategies** - like combining concepts or emphasizing certain aspects\n",
    "4. **Proper embedding management is crucial** - including caching, storage, and maintenance\n",
    "\n",
    "By understanding these concepts, you can build efficient and effective vector search applications with the NYC Landmarks Vector Database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
